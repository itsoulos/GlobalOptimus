%% LyX 2.4.3 created this file.  For more info, see https://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[journal,article,submit,pdftex,moreauthors]{Definitions/mdpi}
\usepackage{textcomp}
\usepackage[utf8]{inputenc}
\usepackage{float}
\usepackage{url}
\usepackage{bm}
\usepackage{varwidth}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{rotfloat}

\makeatletter

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% LyX specific LaTeX commands.

\Title{A novel method that is based on Differential Evolution suitable for
large scale optimization problems}

\TitleCitation{A novel method that is based on Differential Evolution suitable for
large scale optimization problems}

\Author{Glykeria Kyrou$^{1,*}$, Vasileios Charilogis$^{2}$ and Ioannis G.
Tsoulos$^{3}$}

\AuthorNames{Kyrou, G.; Charilogis, V.; Tsoulos, I.G. }

\AuthorNames{Glykeria Kyrou, Vasileios Charilogis and Ioannis G. Tsoulos }


\address{$^{1}$\quad{}Department of Informatics and Telecommunications,
University of Ioannina, 47150 Kostaki Artas, Greece; g.kyrou@uoi.gr\\
$^{2}$\quad{}Department of Informatics and Telecommunications, University
of Ioannina, Greece; v.charilog@uoi.gr\\
$^{3}\quad$Department of Informatics and Telecommunications, University
of Ioannina, 47150 Kostaki Artas, Greece;itsoulos@uoi.gr}


\corres{Correspondence: g.kyrou@uoi.gr}


\abstract{Global optimization represents a fundamental challenge in computer
science and engineering, as it aims to identify high-quality solutions
to problems spanning from moderate to extremely high dimensionality.
The DE algorithm is a population-based algorithm like genetic algorithms
and uses similar operators such as: crossover, mutation and selection.
The proposed method introduces a set of methodological enhancements
designed to increase both the robustness and the computational efficiency
of the classical DE framework. Specifically, an adaptive termination
criterion is incorporated, enabling early stopping based on statistical
measures of convergence and population stagnation. Furthermore, a
population sampling strategy based on k-means clustering is employed
to enhance exploration and improve the redistribution of individuals
in high-dimensional search spaces. This mechanism enables structured
population renewal and effectively mitigates premature convergence.
The enhanced algorithm was evaluated on standard large-scale numerical
optimization benchmarks and compared with established global optimization
methods. The experimental results indicate substantial improvements
in convergence speed, scalability, and solution stability.}


\keyword{Optimization; Differential Evolution; Evolutionary techniques; Stochastic
methods; Large-scale problems}

\DeclareTextSymbolDefault{\textquotedbl}{T1}
%% Because html converters don't know tabularnewline
\providecommand{\tabularnewline}{\\}
%% Variable width box for table cells
\newenvironment{cellvarwidth}[1][t]
    {\begin{varwidth}[#1]{\linewidth}}
    {\@finalstrut\@arstrutbox\end{varwidth}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
%  LaTeX support: latex@mdpi.com 
%  For support, please attach all files needed for compiling as well as the log file, and specify your operating system, LaTeX version, and LaTeX editor.

%=================================================================
%\documentclass[preprints,article,submit,pdftex,moreauthors]{Definitions/mdpi} 
% For posting an early version of this manuscript as a preprint, you may use "preprints" as the journal. Changing "submit" to "accept" before posting will remove line numbers.

% Below journals will use APA reference format:
% admsci, aichem, behavsci, businesses, econometrics, economies, education, ejihpe, famsci, games, humans, ijcs, ijfs, journalmedia, jrfm, languages, psycholint, publications, tourismhosp, youth

% Below journals will use Chicago reference format:
% arts, genealogy, histories, humanities, jintelligence, laws, literature, religions, risks, socsci

%--------------------
% Class Options:
%--------------------
%----------
% journal
%----------
% Choose between the following MDPI journals:
% accountaudit, acoustics, actuators, addictions, adhesives, admsci, adolescents, aerobiology, aerospace, agriculture, agriengineering, agrochemicals, agronomy, ai, air, algorithms, allergies, alloys, amh, analytica, analytics, anatomia, anesthres, animals, antibiotics, antibodies, antioxidants, applbiosci, appliedchem, appliedmath, appliedphys, applmech, applmicrobiol, applnano, applsci, aquacj, architecture, arm, arthropoda, arts, asc, asi, astronomy, atmosphere, atoms, audiolres, automation, axioms, bacteria, batteries, bdcc, behavsci, beverages, biochem, bioengineering, biologics, biology, biomass, biomechanics, biomed, biomedicines, biomedinformatics, biomimetics, biomolecules, biophysica, biosensors, biosphere, biotech, birds, blockchains, bloods, blsf, brainsci, breath, buildings, businesses, cancers, carbon, cardiogenetics, catalysts, cells, ceramics, challenges, chemengineering, chemistry, chemosensors, chemproc, children, chips, cimb, civileng, cleantechnol, climate, clinbioenerg, clinpract, clockssleep, cmd, cmtr, coasts, coatings, colloids, colorants, commodities, complications, compounds, computation, computers, condensedmatter, conservation, constrmater, cosmetics, covid, crops, cryo, cryptography, crystals, csmf, ctn, curroncol, cyber, dairy, data, ddc, dentistry, dermato, dermatopathology, designs, devices, diabetology, diagnostics, dietetics, digital, disabilities, diseases, diversity, dna, drones, dynamics, earth, ebj, ecm, ecologies, econometrics, economies, education, eesp, ejihpe, electricity, electrochem, electronicmat, electronics, encyclopedia, endocrines, energies, eng, engproc, ent, entomology, entropy, environments, epidemiologia, epigenomes, esa, est, famsci, fermentation, fibers, fintech, fire, fishes, fluids, foods, forecasting, forensicsci, forests, fossstud, foundations, fractalfract, fuels, future, futureinternet, futureparasites, futurepharmacol, futurephys, futuretransp, galaxies, games, gases, gastroent, gastrointestdisord, gastronomy, gels, genealogy, genes, geographies, geohazards, geomatics, geometry, geosciences, geotechnics, geriatrics, glacies, grasses, greenhealth, gucdd, hardware, hazardousmatters, healthcare, hearts, hemato, hematolrep, heritage, higheredu, highthroughput, histories, horticulturae, hospitals, humanities, humans, hydrobiology, hydrogen, hydrology, hygiene, idr, iic, ijerph, ijfs, ijgi, ijmd, ijms, ijns, ijpb, ijt, ijtm, ijtpp, ime, immuno, informatics, information, infrastructures, inorganics, insects, instruments, inventions, iot, j, jal, jcdd, jcm, jcp, jcs, jcto, jdad, jdb, jeta, jfb, jfmk, jimaging, jintelligence, jlpea, jmahp, jmmp, jmms, jmp, jmse, jne, jnt, jof, joitmc, joma, jop, jor, journalmedia, jox, jpbi, jpm, jrfm, jsan, jtaer, jvd, jzbg, kidney, kidneydial, kinasesphosphatases, knowledge, labmed, laboratories, land, languages, laws, life, lights, limnolrev, lipidology, liquids, literature, livers, logics, logistics, lubricants, lymphatics, machines, macromol, magnetism, magnetochemistry, make, marinedrugs, materials, materproc, mathematics, mca, measurements, medicina, medicines, medsci, membranes, merits, metabolites, metals, meteorology, methane, metrics, metrology, micro, microarrays, microbiolres, microelectronics, micromachines, microorganisms, microplastics, microwave, minerals, mining, mmphys, modelling, molbank, molecules, mps, msf, mti, multimedia, muscles, nanoenergyadv, nanomanufacturing, nanomaterials, ncrna, ndt, network, neuroglia, neurolint, neurosci, nitrogen, notspecified, nursrep, nutraceuticals, nutrients, obesities, oceans, ohbm, onco, oncopathology, optics, oral, organics, organoids, osteology, oxygen, parasites, parasitologia, particles, pathogens, pathophysiology, pediatrrep, pets, pharmaceuticals, pharmaceutics, pharmacoepidemiology, pharmacy, philosophies, photochem, photonics, phycology, physchem, physics, physiologia, plants, plasma, platforms, pollutants, polymers, polysaccharides, populations, poultry, powders, preprints, proceedings, processes, prosthesis, proteomes, psf, psych, psychiatryint, psychoactives, psycholint, publications, purification, quantumrep, quaternary, qubs, radiation, reactions, realestate, receptors, recycling, regeneration, religions, remotesensing, reports, reprodmed, resources, rheumato, risks, robotics, rsee, ruminants, safety, sci, scipharm, sclerosis, seeds, sensors, separations, sexes, signals, sinusitis, siuj, skins, smartcities, sna, societies, socsci, software, soilsystems, solar, solids, spectroscj, sports, standards, stats, std, stresses, surfaces, surgeries, suschem, sustainability, symmetry, synbio, systems, tae, targets, taxonomy, technologies, telecom, test, textiles, thalassrep, therapeutics, thermo, timespace, tomography, tourismhosp, toxics, toxins, transplantology, transportation, traumacare, traumas, tropicalmed, universe, urbansci, uro, vaccines, vehicles, venereology, vetsci, vibration, virtualworlds, viruses, vision, waste, water, wem, wevj, wild, wind, women, world, youth, zoonoticdis

%---------
% article
%---------
% The default type of manuscript is "article", but can be replaced by: 
% abstract, addendum, article, book, bookreview, briefreport, casereport, comment, commentary, communication, conferenceproceedings, correction, conferencereport, entry, expressionofconcern, extendedabstract, datadescriptor, editorial, essay, erratum, hypothesis, interestingimage, obituary, opinion, projectreport, reply, retraction, review, perspective, protocol, shortnote, studyprotocol, systematicreview, supfile, technicalnote, viewpoint, guidelines, registeredreport, tutorial
% supfile = supplementary materials

%----------
% submit
%----------
% The class option "submit" will be changed to "accept" by the Editorial Office when the paper is accepted. This will only make changes to the frontpage (e.g., the logo of the journal will get visible), the headings, and the copyright information. Also, line numbering will be removed. Journal info and pagination for accepted papers will also be assigned by the Editorial Office.

%------------------
% moreauthors
%------------------
% If there is only one author the class option oneauthor should be used. Otherwise use the class option moreauthors.

%---------
% pdftex
%---------
% The option pdftex is for use with pdfLaTeX. If eps figures are used, remove the option pdftex and use LaTeX and dvi2pdf.

%=================================================================
% MDPI internal commands - do not modify
\firstpage{1} 
\setcounter{page}{\@firstpage}
\pubvolume{1}
\issuenum{1}
\articlenumber{0}
\pubyear{2025}
\copyrightyear{2025}
%\externaleditor{Firstname Lastname} % More than 1 editor, please add `` and '' before the last editor name
\datereceived{}
\daterevised{ } % Comment out if no revised date
\dateaccepted{}
\datepublished{}
%\datecorrected{} % For corrected papers include a "Corrected: XXX" date in the original paper.
%\dateretracted{} % For retracted papers include a "RETRACTED: XXX" date in the original paper.
\hreflink{https://doi.org/} % If needed use \linebreak
%\doinum{}
%\pdfoutput=1 % Uncommented for upload to arXiv.org
%\CorrStatement{yes}  % For updates
%\longauthorlist{yes} % For many authors that exceed the left citation part

%=================================================================
% Add packages and commands here. The following packages are loaded in our class file: fontenc, inputenc, calc, indentfirst, fancyhdr, graphicx, epstopdf, lastpage, ifthen, lineno, float, amsmath, setspace, enumitem, mathpazo, booktabs, titlesec, etoolbox, tabto, xcolor, soul, multirow, microtype, tikz, totcount, changepage, attrib, upgreek, cleveref, amsthm, hyphenat, natbib, hyperref, footmisc, url, geometry, newfloat, caption

%=================================================================
%% Please use the following mathematics environments: Theorem, Lemma, Corollary, Proposition, Characterization, Property, Problem, Example, ExamplesandDefinitions, Hypothesis, Remark, Definition, Notation, Assumption
%% For proofs, please use the proof environment (the amsthm package is loaded by the MDPI class).

%=================================================================
% The fields PACS, MSC, and JEL may be left empty or commented out if not applicable
%\PACS{J0101}
%\MSC{}
%\JEL{}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Only for the journal Diversity
%\LSID{\url{http://}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Only for the journal Applied Sciences:
%\featuredapplication{Authors are encouraged to provide a concise description of the specific application or a potential application of the work. This section is not mandatory.}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Only for the journal Data:
%\dataset{DOI number or link to the deposited data set in cases where the data set is published or set to be published separately. If the data set is submitted and will be published as a supplement to this paper in the journal Data, this field will be filled by the editors of the journal. In this case, please make sure to submit the data set as a supplement when entering your manuscript into our manuscript editorial system.}

%\datasetlicense{license under which the data set is made available (CC0, CC-BY, CC-BY-SA, CC-BY-NC, etc.)}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Only for the journal BioTech, Fishes, Neuroimaging and Toxins
%\keycontribution{The breakthroughs or highlights of the manuscript. Authors can write one or two sentences to describe the most important part of the paper.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Only for the journal Encyclopedia
%\encyclopediadef{Instead of the abstract}
%\entrylink{The Link to this entry published on the encyclopedia platform.}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Different journals have different requirements. Please check the specific journal guidelines in the "Instructions for Authors" on the journal's official website.
 
%\addhighlights{yes}
%\renewcommand{\addhighlights}{%
%
%\noindent The goal is to increase the discoverability and readability of the article via search engines and other scholars. Highlights should not be a copy of the abstract, but a simple text allowing the reader to quickly and simplified find out what the article is about and what can be cited from it. Each of these parts should be devoted up to 2 bullet points.\vspace{3pt}\\
%\textbf{What are the main findings?}
% \begin{itemize}[labelsep=2.5mm,topsep=-3pt]
% \item First bullet.
% \item Second bullet.
% \end{itemize}\vspace{3pt}
%\textbf{What is the implication of the main finding?}
% \begin{itemize}[labelsep=2.5mm,topsep=-3pt]
% \item First bullet.
% \item Second bullet.
% \end{itemize}
%}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\makeatother

\begin{document}
\maketitle

\section{Introduction}

\textbf{The }basic goal of global optimization is to find the global
minimum of a continuous multidimensional function and is defined as:
\begin{equation}
x^{*}=\mbox{arg}\min_{x\in S}f(x)\label{eq:eq1}
\end{equation}
with $S$: 
\[
S=\left[a_{1},b_{1}\right]\times\left[a_{2},b_{2}\right]\times\ldots\left[a_{n},b_{n}\right]
\]

with, $a_{i}$ and $b_{i}$ representing lower and upper bounds for
each variable $x_{i}$.

In recent years many researchers have published important reviews
on global optimization. Such methods find application in a wide range
of scientific fields, such as mathematics \citep{go_math1,go_math3},
physics \citep{go_physics2,go_physics3}, chemistry \citep{go_chem1,go_chem2},
biology \citep{go_bio1,go_bio2}, medicine \citep{go_med2,medicine},
agriculture \citep{go_agri1,go_agri2} and economics \citep{go_econ1,go_econ2}.\textbf{
}A particular challenge is the large-scale global optimization (LSGO)
problem, where the complexity increases significantly with increasing
problem dimensions. Finding efficient and computationally feasible
solutions has become particularly difficult, which has led the research
community to focus on the development of innovative algorithms. LSGO
problems are encountered in a wide range of applications, while their
importance is also reflected in the organization of the first global
large-scale optimization competition within the framework of the CEC
in 2008. Other competitions followed in 2010 \citep{key-23}, 2013
\citep{key-24} and 2015 \citep{key-25}, attracting the intense interest
of the academic community.

To address these challenges, various heuristic and meta-heuristic
approaches have been developed. Evolutionary Algorithms (EA) \citep{key-6,key-7}are
one of the most effective categories, as they mimic natural selection
and genetic evolution to search for best solutions. Due to their adaptability
and robustness, EAs can solve difficult optimization problems. Some
of the most well-known EAs are Differential Evolution (DE) \citep{diffe1,diffe2},
Genetic Algorithms \citep{genetic2,genetic3}, Evolutionary Strategies
\citep{key-8,key-11}, Evolutionary Programming\citep{key-12,key-13}
, Multimodal Optimization Algorithms \citep{key-14,key-15}. Also,
methods inspired by Swarm Intelligence \citep{key-27,key-28} such
as: Particle Swarm Optimization (PSO) \citep{pso_major,pso1}, Ant
Colony Optimization (ACO) \citep{aco1,aco2}, Artificial Bee Colony
Optimization (ABC) \citep{key-18,key-17}, Firefly Algorithm (FA)
\citep{key-19,key-20} , Bat Algorithm \citep{key-21,key-22} are
strong alternatives.

Differential Evolution (DE) is one of the most widely used optimization
techniques, as it offers high robustness, simplicity and fast convergence.
DE is a highly efficient evolutionary algorithm that has gained significant
recognition since the late 1990s. DE, originally introduced in 1995
by Storn and Price \citep{key-1,key-2-1}, has proven to be a versatile
optimization tool that can be applied in various scientific and engineering
fields. It is particularly effective for symmetric optimization problems,
as well as for dealing with discontinuous, noisy, and dynamic challenges.
In physics, it has been used in energy-related problems, including
wind power optimization. In chemistry, it has contributed to advances
in atmospheric chemistry \citep{key-1-1}and the development of high-performance
chemical reactors. \citep{key-2} DE has also had significant implications
in health-related areas, such as breast cancer research \citep{key-3}
and medical diagnostics. \citep{key-4}Despite its effectiveness,
DE has some limitations, such as the difficulty of adapting its parameters
to different problems and its reduced performance in high-dimensional
environments. Furthermore, while it has a powerful exploration mechanism,
it often lags in exploiting the identified solutions, which can slow
down the convergence process. To address these challenges, several
improved versions of DE have been proposed. The goal of these improvements
is to achieve a better balance between exploration and exploitation,
parameter adaptability, and more effectively handle large-scale problems.
The continuous research activity in this field demonstrates the importance
of LSGO and the need to develop increasingly efficient algorithms
to deal with it.

The current work introduces a number of modifications to the DE algorithm
in order to speed up the process and increase the efficiency of the
algorithm, especially for large - scale problems. The modifications
introduced in DE aim to improve performance on large-scale problems.
In more detail: 
\begin{itemize}
\item Sampling Method: The use of k-means for sampling contributes to improving
the quality of the initial solutions and reducing the dimensional
complexity, leading to faster convergence.
\item Termination Technique for Differential Evolution: The proposed technique
allows for early termination of the process when no significant improvement
is observed, thus reducing the number of unnecessary evaluations of
the objective function.
\item Different mechanisms for the differential weighted parameter: The
integration of Number, Random and Migrant approaches allows greater
adaptability of the algorithm to the requirements of each problem.
\item Periodic Local Optimization Refinement: The use of local methods,
such as BFGS, contributes to further improving the solutions, increasing
the accuracy of the final results.
\end{itemize}
The combined use of these techniques enhances the efficiency and reliability
of DE in large-scale problems, offering a more efficient way of searching
for solutions.

Recent studies have proposed various strategies to address large-scale
optimization challenges, including cooperative coevolution \citep{large_co},
Particle Swarm Optimization \citep{large_pso}, a memetic DE approach
\citep{large_memetic}, a self-adaptive Fast Fireworks Algorithm\citep{key-40},\textbf{
}swarm-based methods with learning mechanisms\citep{key-41},\citep{key-42}
and advanced decomposition techniques such as dual Differential Grouping
\citep{key-43}.

The remains of this paper are divided as follows: in section \ref{sec:Materials-and-Methods}
the original DE algorithm, the proposed method as well as the flowchart
with detailed description are presented, in section \ref{sec:Results}
of the test functions used in the experiments as well as the related
experiments are presented. In the \ref{sec:Discussion} section, there
is a brief discussion of the results obtained from the experiments.
In section \ref{sec:Conclusions} some conclusions and directions
for future improvements are discussed.

\section{Materials and Methods\label{sec:Materials-and-Methods}}

\subsubsection*{2.1 The original Differential Evolution method}
\begin{enumerate}
\item \textbf{Set }the population size $NP~\ge~4,$usually $NP~=~10n$,
where n is the dimension of the input problem.
\item \textbf{Create} randomly from a distribution $NP$ agents $x_{i},\ i=1,\ldots$$NP$
\item \textbf{Set }the crossover probability $CR\in[0,1].$ A typical value
for this parameter is 0.9.
\item \textbf{Set }the differential weight $F\in[0,2].$ A typical value
for this parameter is 0.8.
\item \textbf{Initialize }all members of the population in the search space.
The members of the population are called agents.
\item \textbf{Until }some stopping criterion is met, repeat:
\end{enumerate}
(a) \textbf{For $i=1...NP$} \textbf{do}.
\begin{itemize}
\item \textbf{Set $x_{i}$} as the agent $i$.
\item \textbf{Pick }randomly three agents $a$, $b$, $c$.
\item \textbf{Pick }a random index $R~\in{1,...,n}$.
\item \textbf{Compute }the trial vector $y=[y_{1},y_{2},...,y_{n}]$ as
follows.
\item \textbf{For }$j=1,...,n$ \textbf{do}:

A. \textbf{Set }$r_{i}~\in~[0,1]$ a random number.

B. \textbf{If }$r_{j}~<~CR~or\ j=R$ then $y_{j}=a_{j}+F\times b_{j}-c_{j}~else~y_{j}=x_{ij}$.
\item \textbf{If} $f(y)~\le~f(x_{i})$ then $x_{i}=y$.
\item \textbf{EndFor.}
\end{itemize}
(b) \textbf{EndFor.}

6. \textbf{Return }the agent $x_{\mbox{best}}$ in the population
with the lower function value $x_{best}$.\medskip{}

The DE process begins by defining the population size $NP$, typically
set as $NP=10n$, where n is the problem's dimensionality. We define
the crossover probability $CR$ with a default value of 0.9 and the
differential weight $F$ with a default value of 0.8. We randomly
initialize all members of the population, referred to as agents, within
the search space. The method iterates until a termination criterion
is met. In each iteration, for every agent $x_{i}$ in the population,
we randomly select three distinct agents $a$, $b$, and \textbf{c}.
We then choose a random index $R$ from 1 to $n$. Next, we construct
a trial vector y by computing, for each component j, the value $y_{j}=a_{j}+F\times\left(b_{j}-c_{j}\right)$
if a random number $r_{j}$ is less than $CR$ or if $j=R$; otherwise,
we keep $y_{j}=x_{j}$. If the objective function value $f(y)$ is
better than or equal to $f(x_{i})$, we replace $x_{i}$ with $y$.
At the end of the process, we return the best-performing agent $x_{best}$,
which has the optimal objective function value. The method combines
stochastic search with directional variations derived from differences
between population agents, ensuring an efficient exploration-exploitation
balance in the search space.

\subsubsection*{2.2 The proposed Differential Evolution method}

The proposed algorithm incorporates a series of modifications to the
Original DE method, which makes finding the global minimum in high
- dimensional problems more efficient. The main steps of the proposed
method are listed subsequently.
\begin{enumerate}
\item \textbf{Initialization step}.
\begin{enumerate}
\item \textbf{Set} as $NP$ the population size of the method (number of
agents).
\item \textbf{Create} randomly from a distribution $NP$ agents $x_{i},\ i=1,\ldots$$NP$
\item \textbf{Compute} the fitness value $f_{i}$ of each agent $x_{i}$
using the objective function as $f_{i}=f\left(x_{i}\right)$.
\item \textbf{Set} as $p_{l}$ the local search rate.
\item \textbf{Set} the integer parameter $N_{t}$ as the tournament size.
\item \textbf{Set} as $N_{g}$ the maximum number of iterations allowed.
\item \textbf{Set} as $N_{I}$ the number of iterations used in the stopping
rule.
\item \textbf{Set} $k=0$, the iteration counter.
\item \textbf{Set} the parameter $CR$, which represents the crossover probability
with $\mbox{\ensuremath{CR}~\ensuremath{\le~}1}$.
\item \textbf{Select} the differential weight method, which is represented
by the parameter $F$. In the proposed method three distinct methods
were incorporated:
\begin{enumerate}
\item \textbf{Number}. In this case the parameter $F$ is chosen as a constant
value. 
\item \textbf{Random}. The random method represents the differential weight
mechanism proposed by Charilogis et al. \citep{de_char}, where it
is defined as:
\begin{equation}
F=-0.5+2r
\end{equation}
where $r$ is a random number with $r\in[0,1]$.
\item \textbf{Migrant}. In this case the differential weight mechanism proposed
in \citep{de_migrant} was used.
\end{enumerate}
\end{enumerate}
\item \textbf{For} $i=1,\ldots,NP$ \textbf{do\label{enu:For--do}}
\begin{enumerate}
\item \textbf{Select} the agent $x_{i}$
\item \textbf{Select} randomly three distinct agents $x_{a},x_{b},x_{c}$.
The selection of these agents could be performed randomly or with
the application of the tournament selection procedure. During tournament
selection, a subset of $N_{t}$ agents are selected from the current
population and the one with the lowest fitness value is selected.
\item \textbf{Choose} a random integer $R\in[1,n],$where $n$ is the dimension
of the objective problem.
\item \textbf{Create} a trial point $x_{t}$.
\item \textbf{For} $j=1,\ldots.n$ \textbf{do}
\begin{enumerate}
\item \textbf{Select} a random number $r\in[0,1]$.
\item \textbf{If} $r\le CR$ \textbf{or} $i=R$ \textbf{then} $x_{t,j}=x_{a,j}+F\times\left(x_{b,j}-x_{c,j}\right)$
\textbf{else} $x_{t,j}=x_{i,j}$
\end{enumerate}
\item \textbf{End For}
\item \textbf{Set} $y_{t}=f\left(x_{t}\right)$
\item \textbf{If} $y_{t}\le f(x_{i})$ \textbf{then} $x_{i}=x_{t},\ f(x_{i})=y_{t}$.
\item \textbf{Select} a random number $r\in[0,1]$. If $r\le p_{l}$ then
$x_{i}=\mbox{LS\ensuremath{\left(x_{i}\right)}},$where LS defines
a local search procedure. In the proposed method the BFGS variant
of Powell \citep{powell} was used.
\end{enumerate}
\item \textbf{End For}
\item \textbf{Check for termination}.
\begin{enumerate}
\item \textbf{Set} $k=k+1$
\item \textbf{If} $k\ge N_{g}$ then terminate.
\item \textbf{Check} the termination rule specified in the work of Charilogis
et al \citep{de_char}. In this work the quantity 
\begin{equation}
\delta^{(k)}=\left|\sum_{i=1}^{\mbox{NP}}\left|f_{i}^{(k)}\right|-\sum_{i=1}^{\mbox{NP}}\left|f_{i}^{(k-1)}\right|\right|\label{eq:eq1-1}
\end{equation}
is calculated. The term $f_{i}^{(k)}$ stands for the fitness value
of agent $i$ at iteration $k$. If $\delta^{(k)}\le\epsilon$ for
a number of $N_{I}$ iterations, then terminate the algorithm else
goto step \ref{enu:For--do}.
\end{enumerate}
\end{enumerate}
\medskip{}
The modified Differential Evolution method begins with the initialization
step. We define the population size NP as the number of agents. We
randomly create $NP$ agents $x_{i}$, $i=1$ to $NP$, from some
distribution and calculate the fitness value $f_{i}=f(x_{i})$ for
each agent using the objective function. We define the local search
rate $p_{l}$, the tournament size $N_{t}$, the maximum number of
iterations $N_{g}$, the number of iterations $N_{I}$ for the termination
rule, and set the iteration counter $k=0$. We define the crossover
probability $CR$ and select the method for the differential weight
$F$, which can be constant, random ($F$ = -0.5 + 2$r$), where r
is a random number in {[}0,1{]}), or based on the Migrant method.

Next, for each agent$x_{i}$, $i=1$ to $NP$, we randomly select
three distinct agents $x_{a}$, $x_{b}$, $x_{c}$, either through
random selection or through a tournament procedure where we select
a subset of $N_{t}$ agents and choose the one with the lowest fitness
value. We select a random integer $R$ in {[}1,n{]}, where n is the
problem's dimensionality, and create a trial point $x_{t}$. For each
dimension $j=1$ to $n$, we select a random number $r$ in {[}0,1{]}.
If $r\le CR$ or$j=R$, then $x_{t,j}=x_{a,j}+F\times\left(x_{b,j}-x_{c,j}\right)$,
otherwise$x_{t},j=x_{i},j$. We calculate the fitness value$y_{t}=f(x_{t})$
and if$y_{t}\le f(x_{i})$, we replace $x_{i}$ with $x_{t}$ and
$f(x_{i})_{i}$ with $y_{t}$. We select a random number r in {[}0,1{]},
and if $r\le p_{l}$, we apply a local search $LS(x_{i})$ to $x_{i}$
using Powell's BFGS method.

After completing the iteration, we update the counter $k=k+1$. If
$k\ge N_{g}$, we terminate the algorithm. Otherwise, we calculate
the quantity \ref{eq:eq1-1}, where $f_{i}^{(k)}$ is the fitness
value of agent i at iteration k. If $\delta^{(k)}\le\varepsilon$
for $N_{I}$ iterations, we terminate the algorithm otherwise, we
continue with the next iteration. This method incorporates random
variations, local search, and different mechanisms for the differential
weight $F$, aiming to improve search efficiency.

\subsubsection*{2.3 The Flowchart of the proposed Differential Evolution method}

The steps of the proposed DE algorithm can be described as follows:

The process begins by initializing the basic parameters, including
population size, differential weight values, and crossover probabilities.
The initial population is then generated, either uniformly distributed
over the search space or clustered using the k-means algorithm. Once
the initial population is determined, the fitness of each individual
is evaluated and stored in a fitness table. At this stage, a differential
weight calculation method is selected, with Stable, Random or Migrant
options. Likewise, the sampling procedure for generating new solutions
is specified, which may include tournament selection or random sampling.
To further refine the candidate solutions, a local search procedure
is applied, improving the quality of the trial solutions. The algorithm
then checks the termination criteria to decide whether to proceed
or terminate. If the criteria are still not met, the process returns
to update the fitness table and continues to repeat. If the termination
criteria are met, the algorithm outputs the best solution found.

\begin{figure}
\centering{}\includegraphics[scale=0.5]{flowchart}\caption{The steps of the proposed DE algorithm.}
\end{figure}


\section{Experiments\label{sec:Results}}

This section begins with a description of the functions that will
be used in the experiments and then presents in detail the experiments
that were performed, in which the parameters available in the proposed
algorithm were studied, in order to study their reliability and adequacy. 

\subsection{Test Functions}

A variety of test functions were used in the conducted experiments.
These functions are used in a series of research papers \citep{testfunc1,testfunc2,testfunc2-1,testfunc4}.
In the present research work, these functions were used with a varying
number of dimensions from 25 to 150. The description of each used
test function is provided below. In all cases, the constant n defines
the dimension of the objective function.
\begin{center}
{\tiny{}
\begin{table}
\centering{}{\tiny{}%
\begin{tabular}{|c|c|c|c|}
\hline 
{\tiny\textbf{NAME}} & {\tiny\textbf{FORMULA}} & {\tiny\textbf{DIM}} & {\tiny$G_{min}$}\tabularnewline
\hline 
\hline 
\begin{cellvarwidth}[t]
\centering
{\tiny ATTRACTIVE}{\tiny\par}

{\tiny SECTOR}
\end{cellvarwidth} & {\tiny$f(x)=\left(\sum_{i=1}^{n}(s_{i}x_{i})^{2}\right)^{0.9}$} & {\tiny 2} & {\tiny 0}\tabularnewline
\hline 
{\tiny BUCHE RASTRIGIN} & {\tiny$f(\mathbf{x})=\sum_{i=1}^{n}\left[z_{i}\cdot\left(1+0.1\cdot\sin(10\pi z_{i})\right)\right]$} & {\tiny n} & {\tiny 0}\tabularnewline
\hline 
{\tiny DIFFERENT POWERS} & {\tiny$f(\mathbf{x})=\sqrt{\sum_{i=1}^{n}|x_{i}|^{2+4\frac{i-1}{n-1}}}$} & {\tiny n} & {\tiny 0}\tabularnewline
\hline 
{\tiny DISCUS} & {\tiny$f(x)=10^{6}x_{1}^{2}+\sum_{i=2}^{n}x_{i}^{2}$} & {\tiny n} & {\tiny 0}\tabularnewline
\hline 
{\tiny ELLIPSOIDAL} & {\tiny$f(x)=\sum_{i=1}^{n}\left(10^{6}\right)^{\frac{i-1}{n-1}}x_{i}^{2}$} & {\tiny n} & {\tiny 0}\tabularnewline
\hline 
{\tiny GALLAGHER101} & {\tiny$f(\mathbf{x})=\max_{i=1}^{101}\left[h_{i}-w_{i}\sqrt{\sum_{j=1}^{n}(x_{j}-c_{ij})^{2}}\right]$$\ min:100+1$} & {\tiny n} & {\tiny 0}\tabularnewline
\hline 
{\tiny GALLAGHER21} & {\tiny$f(\mathbf{x})=\max_{i=1}^{21}\left[h_{i}-w_{i}\sqrt{\sum_{j=1}^{n}(x_{j}-c_{ij})^{2}}\right]$$\ min:10+10+1$} & {\tiny n} & {\tiny 0}\tabularnewline
\hline 
\begin{cellvarwidth}[t]
\centering
{\tiny GRIEWANK}{\tiny\par}

{\tiny ROSENBROCK}
\end{cellvarwidth} & {\tiny$f(\mathbf{x})=\underbrace{\left(\frac{\|\mathbf{x}\|^{2}}{4000}-\prod_{i=1}^{n}\cos\left(\frac{x_{i}}{\sqrt{i}}\right)+1\right)}_{\text{Griewank}}\cdot\underbrace{\left(\frac{1}{10}\sum_{i=1}^{n-1}\left[100(x_{i+1}-x_{i}^{2})^{2}+(1-x_{i})^{2}\right]\right)}_{\text{Rosenbrock}}$} & {\tiny n} & {\tiny 0}\tabularnewline
\hline 
{\tiny GRIEWANK} & {\tiny$f(x)=1+\frac{1}{200}\sum_{i=1}^{n}x_{i}^{2}-\prod_{i=1}^{n}\frac{\cos(x_{i})}{\sqrt{(i)}}$} & {\tiny n} & {\tiny 0}\tabularnewline
\hline 
{\tiny RARSTIGIN} & {\tiny$f(\mathbf{x})=An+\sum_{i=1}^{n}\left[x_{i}^{2}-A\cos(2\pi x_{i})\right]$$\ A=10$} & {\tiny n} & {\tiny 0}\tabularnewline
\hline 
{\tiny ROSENBROCK} & {\tiny$f(x)=\sum_{i=1}^{n-1}\left(100\left(x_{i+1}-x_{i}^{2}\right)^{2}+\left(x_{i}-1\right)^{2}\right),\quad-30\le x_{i}\le30$} & {\tiny n} & {\tiny 0}\tabularnewline
\hline 
{\tiny SHARP RIDGE} & {\tiny$f(\mathbf{x})=x_{1}^{2}+\alpha\sum_{i=2}^{n}x_{i}^{2},\ a>1$} & {\tiny n} & {\tiny 0}\tabularnewline
\hline 
{\tiny SPHERE} & {\tiny$f(\mathbf{x})=\sum_{i=1}^{n}x_{i}^{2}$} & {\tiny n} & {\tiny 0}\tabularnewline
\hline 
{\tiny STEP ELLIPSOIDAL} & {\tiny$f(\mathbf{x})=\sum_{i=1}^{n}\left\lfloor x_{i}+0.5\right\rfloor ^{2}+\alpha\sum_{i=1}^{n}\left(10^{6}\cdot\frac{i-1}{n-1}\right)x_{i}^{2},\ a=1$} & {\tiny n} & {\tiny 0}\tabularnewline
\hline 
{\tiny ZAKHAROV} & {\tiny$f(\mathbf{x})=\sum_{i=1}^{n}x_{i}^{2}+\left(\sum_{i=1}^{n}\frac{i}{2}x_{i}\right)^{2}+\left(\sum_{i=1}^{n}\frac{i}{2}x_{i}\right)^{4}$} & {\tiny n} & {\tiny 0}\tabularnewline
\hline 
\end{tabular}}
\end{table}
}{\tiny\par}
\par\end{center}

\subsection{Experimental results }

A series of experiments were carried out for the previously mentioned
functions and these experiments were executed on an AMD RYZEN 5950X
with 128GB RAM. The operating system of the running machine was Debian
Linux. Each experiment was conducted 30 times, with different random
numbers each time, and the averages were recorded. The software used
in the experiments was coded in ANSI C++ using the freely available
optimization environment of OPTIMUS\citep{OPTIMUS}, which can be
downloaded from\textbf{ }\url{https://github.com/itsoulos/OPTIMUS}.
The values for the experimental parameters used in the proposed method
are outlined in the Table \ref{tab:expSettings}

\begin{table}[H]
\caption{The values of the parameters of the proposed method.\label{tab:expSettings}}

\centering{}%
\begin{tabular}{|c|c|c|}
\hline 
PARAMETER & MEANING & VALUE\tabularnewline
\hline 
\hline 
$NP$ & Number of agents for all methods & 200\tabularnewline
\hline 
$n$ & Maximum number of allowed iterations for all methods & 200\tabularnewline
\hline 
$p_{l}$ & Local search rate, & 0.05\tabularnewline
\hline 
$F$ & Differential weight for classic DE & $F\in[0,1.0]$\tabularnewline
\hline 
$F$ & Differential weight for PROPOSED & 0.8\tabularnewline
\hline 
$CR$ & Crossover probability & 0.9\tabularnewline
\hline 
$N_{I}$ & Number of iterations used in the termination rule & 8\tabularnewline
\hline 
- & Mutation rate for GA & 0.05 (5\%)\tabularnewline
\hline 
- & Selection Rate for GA & 0.05 (5\%)\tabularnewline
\hline 
- & Selection method fo GA & Roulette\tabularnewline
\hline 
\end{tabular}
\end{table}
In the following tables that depict the experimental results, the
numbers in cells stand for the average function calls, as measured
on 30 independent runs. The numbers in parentheses denote the fraction
of the executions where the method successfully discovered the global
minimum. If this number is not present, then the method managed to
locate the global minimum in every run (100\% success).

\subsection{The proposed method in comparison with others}

The Table \ref{tab:Experimental-results-using} presents the results
of a comparative analysis of various optimization methods (BICCA\citep{key-32},
MLSHADESPA\citep{key-30}, SHADE\_ILS\citep{key-31}, Differential
Evolution (DE)\citep{diffe1,diffe2}, Genetic Algorithm (GA)\citep{genetic2,genetic3},
Whale Optimization Algorithm(WOA) \citep{WOA,WOA1}, Particle Swarm
Optimization (IPSO)\citep{pso_major,pso1}, PROPOSED) across a wide
range of test functions with dimensions of 25, 50, 100, and 150. Each
row corresponds to a test function, while the columns represent the
methods. The numerical values in each cell indicate the number of
objective function calls required to find the minimum, while the values
in parentheses show the success rate of each method in each case.
In the last row (TOTAL), the total sum of function calls for each
method is displayed, along with the average success rate. The best
methods should simultaneously exhibit a low number of function calls
(efficiency) and a high success rate (reliability). The analysis shows
that the Proposed method delivers strong and consistent performance.
Its overall success rate (0.85) is comparable to those of GA, MLSHADESPA,
SHADE\_ILS, IPSO, DE and WOA (around 0.90), and distinctly higher
than the BICCA method (0.73). This indicates that the Proposed method
remains dependable in locating the global minimum even when faced
with complex or high-dimensional search spaces. In terms of computational
cost, the Proposed method requires a total of 387,335 objective function
evaluations, which is substantially lower than most competing techniques.
This advantage appears consistently across the majority of tested
functions. For example, in the DifferentPowers function, the Proposed
method significantly outperforms GA across all dimensionalities: at
25 dimensions it uses 6,478 evaluations compared to 14,495 for GA
at 100 dimensions, 16,225 versus 28,413 and at 150 dimensions, 21,495
versus 33,569. Similar observations are made for the GriewankRosenbrock
function, where the Proposed method demonstrates clear efficiency
benefits: at 100 dimensions it requires 6,465 evaluations whereas
BICCA needs 20,462, and at 150 dimensions the gap widens further with
7,272 evaluations compared to 30,604 for BICCA. These differences
illustrate the methodâ€™s robustness and its ability to maintain low
computational demands in highly nonlinear and difficult optimization
landscapes.

In summary, the findings suggest that the Proposed method offers a
strong balance between reliability and computational efficiency. It
competes effectively with and in many cases surpasses widely used
optimization algorithms, while maintaining a consistently lower number
of objective function evaluations. Its stability across different
functions and dimensions confirms its applicability to a broad range
of optimization scenarios, making it a promising and efficient alternative
within the field of evolutionary and metaheuristic optimization.

{\scriptsize{}
\begin{sidewaystable}[H]
{\scriptsize\caption{Experimental results using different optimization methods. Numbers
in cells represent sum function calls.\label{tab:Experimental-results-using}}
}{\scriptsize\par}

{\tiny{}%
\begin{tabular}{|c|c|c|c|c|c|c|c|c|}
\hline 
{\tiny\textbf{FUNCTION}} & {\tiny\textbf{BICCA}} & {\tiny\textbf{MLSHADESPA}} & {\tiny\textbf{SHADE\_ILS}} & {\tiny\textbf{DE}} & {\tiny\textbf{GA}} & {\tiny\textbf{WOA}} & {\tiny\textbf{IPSO}} & {\tiny\textbf{PROPOSED}}\tabularnewline
\hline 
\hline 
\begin{cellvarwidth}[t]
\centering
{\tiny ATTRACTIVE}{\tiny\par}

{\tiny SECTOR\_25}
\end{cellvarwidth} & 5130 & 950 & 452 & 4439 & 2208 & 2641 & 2120 & 1697\tabularnewline
\hline 
\begin{cellvarwidth}[t]
\centering
{\tiny ATTRACTIVE}{\tiny\par}

{\tiny SECTOR\_50}
\end{cellvarwidth} & 10097 & 994 & 558 & 18104 & 2230 & 5700 & 2167 & 1761\tabularnewline
\hline 
\begin{cellvarwidth}[t]
\centering
{\tiny ATTRACTIVE}{\tiny\par}

{\tiny SECTOR\_100}
\end{cellvarwidth} & 20178 & 989 & 748 & 15246 & 2231 & 5785 & 2179 & 1832\tabularnewline
\hline 
\begin{cellvarwidth}[t]
\centering
{\tiny ATTRACTIVE}{\tiny\par}

{\tiny SECTOR\_150}
\end{cellvarwidth} & 30259 & 1047 & 959 & 6646 & 2232 & 9248 & 2196 & 1867\tabularnewline
\hline 
{\tiny BUCHE RASTRIGIN\_25} & 5144(0.33) & 9420(0.90) & 2093(0.90) & 1466(0.90) & 12979(0.90) & 15048(0.93) & 12115(0.90) & 5893(0.90)\tabularnewline
\hline 
{\tiny BUCHE RASTRIGIN\_50} & 10345(0.03) & 18003(0.50) & 3440(0.50) & 1894(0.50) & 20711(0.50) & 58557(0.77) & 30866(0.50) & 12585(0.50)\tabularnewline
\hline 
{\tiny BUCHE RASTRIGIN\_100} & 20676(0.03) & 30652(0.53) & 5428(0.53) & 2020(0.53) & 29121(0.53) & 43001(0.97) & 39680(0.53) & 16490(0.53)\tabularnewline
\hline 
{\tiny BUCHE RASTRIGIN\_150} & 30894(0.03) & 47160(0.27) & 7663(0.27) & 2511(0.27) & 37696(0.27) & 54641 & 53060(0.27) & 23466(0.27)\tabularnewline
\hline 
{\tiny DISCUS\_25} & 5125 & 1365 & 536 & 4255 & 2656 & 3006 & 2452 & 1992\tabularnewline
\hline 
{\tiny DISCUS\_50} & 10101 & 1425 & 642 & 10297 & 2663 & 6310 & 2498 & 2060\tabularnewline
\hline 
{\tiny DISCUS\_100} & 20189 & 1402 & 826 & 8284 & 2631 & 5835 & 2523 & 2104\tabularnewline
\hline 
{\tiny DISCUS\_150} & 30265 & 1487 & 1042 & 8548 & 2620 & 8227 & 2548 & 2144\tabularnewline
\hline 
{\tiny DIFFERENTPOWERS\_25} & 5144 & 13007 & 2644 & 4786 & 14495 & 14921 & 13313 & 6478\tabularnewline
\hline 
{\tiny DIFFERENTPOWERS\_50} & 10389 & 20029 & 3860 & 14391 & 20539 & 35828 & 19839 & 11183\tabularnewline
\hline 
{\tiny DIFFERENTPOWERS\_100} & 20644 & 27859 & 5450 & 7355 & 28413 & 52081 & 28379 & 16225\tabularnewline
\hline 
{\tiny DIFFERENTPOWERS\_150} & 30877 & 36894 & 7059 & 6266 & 33569 & 93074 & 36287 & 21495\tabularnewline
\hline 
{\tiny ELLIPSOIDAL\_25} & 5139(0.87) & 4227 & 1117 & 4161 & 5955 & 7299 & 6375 & 3590\tabularnewline
\hline 
{\tiny ELLIPSOIDAL\_50} & 10247 & 9146 & 2178 & 16624 & 10892 & 19281 & 11641 & 6424\tabularnewline
\hline 
{\tiny ELLIPSOIDAL\_100} & 20492 & 18062 & 3966 & 12708 & 20202 & 38501 & 20736 & 11549\tabularnewline
\hline 
{\tiny ELLIPSOIDAL\_150} & 30708 & 26835 & 5993 & 21936 & 36236 & 63093 & 29414 & 16930\tabularnewline
\hline 
{\tiny GALLAGHER21\_25} & 5122(0.46) & 1304(0.90) & 503(0.90) & 4180(0.90) & 3346(0.90) & 9210(0.90) & 3605(0.90) & 2261(0.90)\tabularnewline
\hline 
{\tiny GALLAGHER21\_50} & 10119(0.03) & 1757(0.50) & 701(0.50) & 7938(0.50) & 3192(0.50) & 35580(0.50) & 8866(0.50) & 4503(0.50)\tabularnewline
\hline 
{\tiny GALLAGHER21\_100} & 20167 & 392(0.53) & 637(0.53) & 1323(0.53) & 1593(0.53) & 1950(0.53) & 5363(0.53) & 1756(0.53)\tabularnewline
\hline 
{\tiny GALLAGHER21\_150} & 30248 & 385(0.27) & 825(0.27) & 1313(0.27) & 1582(0.27) & 1738(0.27) & 2050(0.27) & 1662(0.27)\tabularnewline
\hline 
{\tiny GALLAGHER101\_25} & 5117(0.07) & 1270 & 501(0.90) & 3625(0.90) & 3340(0.90) & 7664(0.90) & 3473(0.90) & 2769(0.90)\tabularnewline
\hline 
{\tiny GALLAGHER101\_50} & 10114(0.03) & 1396 & 634(0.50) & 18470(0.50) & 7134(0.50) & 38817(0.50) & 8796(0.50) & 4890(0.50)\tabularnewline
\hline 
{\tiny GALLAGHER101\_100} & 20193(0.03) & 1868 & 901(0.53) & 14700(0.53) & 5794(0.53) & 39700(0.53) & 9257(0.53) & 5886(0.53)\tabularnewline
\hline 
{\tiny GALLAGHER101\_150} & 30269(0.03) & 1922 & 1127(0.27) & 24214(0.27) & 7210(0.27) & 36525(0.27) & 14076(0.27) & 8646(0.27)\tabularnewline
\hline 
{\tiny GRIEWANK \_25} & 5173(0.70) & 7828 & 1811 & 4123(0.97) & 9733 & 10166 & 9454 & 4084\tabularnewline
\hline 
{\tiny GRIEWANK \_50} & 10138 & 3434 & 1061 & 17524(0.93) & 5410 & 18966 & 9827 & 5039\tabularnewline
\hline 
{\tiny GRIEWANK \_100} & 20208 & 2825 & 1124 & 14809 & 4982 & 19318 & 10369 & 6460\tabularnewline
\hline 
{\tiny GRIEWANK \_150} & 30290 & 3035 & 1391 & 6335(0.97) & 5221 & 28823 & 10741 & 6542\tabularnewline
\hline 
{\tiny GRIEWANK\_ROSENBROCK\_25} & 5180 & 14086 & 3132 & 3238 & 17038 & 10630 & 9698 & 4466\tabularnewline
\hline 
{\tiny GRIEWANK\_ROSENBROCK\_50} & 10362 & 20021 & 4319 & 16379 & 23217 & 22912 & 11610 & 5325\tabularnewline
\hline 
{\tiny GRIEWANK\_ROSENBROCK\_100} & 20462 & 23913 & 4925 & 11375 & 31195 & 24543 & 13409 & 6465\tabularnewline
\hline 
{\tiny GRIEWANK\_ROSENBROCK\_150} & 30604 & 29813 & 6080 & 4446 & 37364 & 33948 & 15075 & 7272\tabularnewline
\hline 
{\tiny ROSENBROCK\_25} & 5163 & 12518 & 2793 & 3543 & 15493 & 13642 & 13642 & 5950\tabularnewline
\hline 
{\tiny ROSENBROCK\_50} & 10451 & 21195 & 4555 & 12085 & 24602 & 33038 & 22317 & 8963\tabularnewline
\hline 
{\tiny ROSENBROCK\_100} & 20785 & 35136 & 7151 & 6038 & 39496 & 48451 & 36400 & 15930\tabularnewline
\hline 
{\tiny ROSENBROCK\_150} & 31103 & 50850 & 10669 & 4203 & 53211 & 75425 & 50281 & 22135\tabularnewline
\hline 
{\tiny RARSTIGIN\_25} & 5139(0.36) & 7826(0.90) & 1767(0.90) & 1574(0.90) & 9581(0.90) & 15530(0.90) & 9826(0.90) & 4577(0.90)\tabularnewline
\hline 
{\tiny RARSTIGIN\_50} & 10208(0.03) & 10741(0.50) & 2091(0.50) & 1895(0.50) & 12272(0.50) & 41187(0.73) & 17354(0.50) & 7746(0.50)\tabularnewline
\hline 
{\tiny RARSTIGIN\_100} & 20358(0.03) & 11464(0.53) & 2338(0.53) & 1869(0.53) & 2134(0.53) & 27383(0.90) & 19347(0.53) & 9147(0.53)\tabularnewline
\hline 
{\tiny RARSTIGIN\_150} & 30561(0.03) & 14002(0.27) & 2942(0.27) & 2122(0.27) & 13990(0.27) & 32297(0.93) & 27682(0.27) & 11620(0.27)\tabularnewline
\hline 
{\tiny SPHERE\_25} & 5134 & 482 & 358 & 4131 & 1689 & 2206 & 1611 & 1481\tabularnewline
\hline 
{\tiny SPHERE\_50} & 10088 & 500 & 459 & 18098 & 1700 & 5111 & 1633 & 1509\tabularnewline
\hline 
{\tiny SPHERE\_100} & 20169 & 498 & 655 & 15241 & 1699 & 5107 & 1639 & 1524\tabularnewline
\hline 
{\tiny SPHERE\_150} & 30250 & 523 & 858 & 6639 & 1700 & 7347 & 1645 & 1535\tabularnewline
\hline 
{\tiny STEP ELLIPSOIDAL\_25} & 5114(0.70) & 375(0.90) & 313(0.90) & 1857(0.93) & 2069(0.90) & 1812(0.97) & 1846(0.90) & 1625(0.90)\tabularnewline
\hline 
{\tiny STEP ELLIPSOIDAL\_50} & 10086(0.03) & 375(0.50) & 391(0.50) & 6493(0.67) & 2469(0.50) & 2541(0.50) & 3993(0.50) & 2300(0.50)\tabularnewline
\hline 
{\tiny STEP ELLIPSOIDAL\_100} & 20167(0.03) & 377(0.53) & 541(0.53) & 5658(0.53) & 1681(0.53) & 2405(0.53) & 3946(0.53) & 2465(0.53)\tabularnewline
\hline 
{\tiny STEP ELLIPSOIDAL\_150} & 30248(0.03) & 383(0.27) & 695(0.27) & 5588(0.27) & 1673(0.27) & 2854(0.27) & 5065(0.27) & 3143(0.27)\tabularnewline
\hline 
{\tiny SHARP RIDGE\_25} & 5125 & 9281 & 2193 & 5153 & 11536 & 11398 & 11371 & 5104\tabularnewline
\hline 
{\tiny SHARP RIDGE\_50} & 10261 & 9843 & 2284 & 18677 & 11818 & 19405 & 12550 & 5226\tabularnewline
\hline 
{\tiny SHARP RIDGE\_100} & 20366 & 10190 & 2403 & 15159 & 11659 & 20507 & 13017 & 5995\tabularnewline
\hline 
{\tiny SHARP RIDGE\_150} & 30458 & 11205 & 2885 & 7476(0.97) & 11866 & 25983 & 13776 & 6481\tabularnewline
\hline 
{\tiny ZAKHAROV\_25} & 5120 & 4383 & 1177 & 1735 & 5756 & 9556 & 3449 & 2185\tabularnewline
\hline 
{\tiny ZAKHAROV\_50} & 10118 & 18043 & 3584 & 2371 & 15522 & 23884 & 6469 & 3027\tabularnewline
\hline 
{\tiny ZAKHAROV\_100} & 20211 & 45770 & 8470 & 2216 & 38359 & 29581 & 16562 & 5572\tabularnewline
\hline 
{\tiny ZAKHAROV\_150} & 30293 & 46497 & 9315 & 2503 & 36399 & 32379 & 21273 & 6304\tabularnewline
\hline 
 & 992785(0.73) & 708659(0.85) & 157213(0.85) & 478253(0.85) & 786004(0.85) & 1371596(0.90) & 782751(0.85) & 387335(0.85)\tabularnewline
\hline 
\end{tabular}}{\tiny\par}
\end{sidewaystable}
}{\scriptsize\par}

Figure 2 illustrates the distribution of function evaluations for
the PROPOSED optimizer compared with all baseline algorithms. A Kruskal--Wallis
test confirms strong overall differences across methods (p \textless{}
2.2e-16), indicating that the optimizers do not come from the same
underlying distribution. The PROPOSED method consistently exhibits
substantially lower function-call counts, with both its median and
dispersion markedly smaller than those of all competing algorithms.
The results, combined with the consistently lower function-call requirements
displayed in the figure, indicate that the superiority of the PROPOSED
optimizer is highly unlikely to be attributable to random variation.
Instead, it reflects a systematic and robust performance advantage
over all baseline methods.

\begin{figure}
\centering{}\includegraphics[scale=0.5]{methods}\caption{A statistical comparison of the proposed with other optimization methods.}
\end{figure}


\subsection{The effect of differential weight mechanism}

Table \ref{tab:Experiments-using-different} presents the impact of
the three differential weight strategies NUMBER(T), RANDOM(T), and
MIGRANT(T) on the performance of the algorithm across a broad set
of benchmark functions, where (T) denotes tournament--based selection.
The results clearly show that MIGRANT(T) is by far the most efficient
method.. The MIGRANT(T) strategy consistently achieves the best outcomes,
requiring the fewest objective function evaluations overall (387,335)
compared with NUMBER(T) (527,444) and RANDOM(T) (543,201). This improvement
is particularly noteworthy given that all three strategies achieve
the same overall success rate (0.85), indicating that the performance
advantage arises purely from efficiency rather than reliability differences.
This trend is visible across multiple test functions. In the Attractive
Sector family (25--150 dimensions), MIGRANT(T) demonstrates uniformly
superior performance. For example, in Attractive Sector\_25, it requires
1697 calls, compared with 1743 for NUMBER(T) and 1756 for RANDOM(T).
As dimensionality increases, this advantage becomes even more pronounced.
The performance gap becomes even more substantial in multimodal landscapes
such as Buche--Rastrigin. In Buche Rastrigin\_25, MIGRANT(T) needs
5893 function calls (success rate 0.90), significantly fewer than
NUMBER(T) (12,243) and RANDOM(T) (12,035). The difference becomes
overwhelming in the highest-dimensional case (Buche Rastrigin\_150):
MIGRANT(T) completes the optimization with 23,466 calls, while NUMBER(T)
requires 40,240, and RANDOM(T) needs 39,263. These results underline
MIGRANT(T)â€™s superior adaptability in sharply multimodal and high-variance
landscapes. For example, Ellipsoidal\_150 is solved in 16,930 calls
by MIGRANT(T), compared with 19,311 for NUMBER(T) and 19,940 for RANDOM(T).
This difference becomes particularly important for large-scale smooth
problems, where maintaining efficiency is critical.

Overall, the evidence strongly indicates that MIGRANT(T) is the most
effective differential weight mechanism among the tested variants.
It consistently reduces the number of function evaluations across
a wide variety of functions both unimodal and multimodal while preserving
identical success rates. This combination of efficiency, robustness,
and stability makes MIGRANT(T) a particularly advantageous choice
for enhancing the performance of Differential Evolution in high-dimensional
and challenging optimization scenarios.

{\scriptsize{}
\begin{sidewaystable}[H]
{\scriptsize\caption{Experiments using different weight selection for the proposed method.\label{tab:Experiments-using-different}}
}{\scriptsize\par}

{\tiny{}%
\begin{tabular}{|c|c|c|c|}
\hline 
{\tiny\textbf{FUNCTION}} & {\tiny\textbf{MIGRANT (T)}} & {\tiny\textbf{NUMBER (T)}} & {\tiny\textbf{RANDOM (T)}}\tabularnewline
\hline 
\hline 
\begin{cellvarwidth}[t]
\centering
{\tiny ATTRACTIVE}{\tiny\par}

{\tiny SECTOR\_25}
\end{cellvarwidth} & 1697 & 1743 & 1756\tabularnewline
\hline 
\begin{cellvarwidth}[t]
\centering
{\tiny ATTRACTIVE}{\tiny\par}

{\tiny SECTOR\_50}
\end{cellvarwidth} & 1761 & 1823 & 1828\tabularnewline
\hline 
\begin{cellvarwidth}[t]
\centering
{\tiny ATTRACTIVE}{\tiny\par}

{\tiny SECTOR\_100}
\end{cellvarwidth} & 1832 & 1879 & 1880\tabularnewline
\hline 
\begin{cellvarwidth}[t]
\centering
{\tiny ATTRACTIVE}{\tiny\par}

{\tiny SECTOR\_150}
\end{cellvarwidth} & 1867 & 1900 & 1920\tabularnewline
\hline 
{\tiny BUCHE RASTRIGIN\_25} & 5893(0.90) & 12243(0.90) & 12035(0.90)\tabularnewline
\hline 
{\tiny BUCHE RASTRIGIN\_50} & 12585(0.50) & 19529(0.50) & 20457(0.50)\tabularnewline
\hline 
{\tiny BUCHE RASTRIGIN\_100} & 16490(0.53) & 30055(0.53) & 31465(0.53)\tabularnewline
\hline 
{\tiny BUCHE RASTRIGIN\_150} & 23466(0.27) & 40240(0.27) & 39263(0.27)\tabularnewline
\hline 
{\tiny DISCUS\_25} & 1992 & 1857 & 1896\tabularnewline
\hline 
{\tiny DISCUS\_50} & 2060 & 1926 & 1971\tabularnewline
\hline 
{\tiny DISCUS\_100} & 2104 & 1978 & 1989\tabularnewline
\hline 
{\tiny DISCUS\_150} & 2144 & 2006 & 2040\tabularnewline
\hline 
{\tiny DIFFERENTPOWERS\_25} & 6478 & 11422 & 11629\tabularnewline
\hline 
{\tiny DIFFERENTPOWERS\_50} & 11183 & 15258 & 15179\tabularnewline
\hline 
{\tiny DIFFERENTPOWERS\_100} & 16225 & 21451 & 20659\tabularnewline
\hline 
{\tiny DIFFERENTPOWERS\_150} & 21495 & 24429 & 24670\tabularnewline
\hline 
{\tiny ELLIPSOIDAL\_25} & 3590 & 3751 & 3958\tabularnewline
\hline 
{\tiny ELLIPSOIDAL\_50} & 6424 & 6864 & 7184\tabularnewline
\hline 
{\tiny ELLIPSOIDAL\_100} & 11549 & 13756 & 13890\tabularnewline
\hline 
{\tiny ELLIPSOIDAL\_150} & 16930 & 19311 & 19940\tabularnewline
\hline 
{\tiny GALLAGHER21\_25} & 2261(0.90) & 3815(0.90) & 6364(0.90)\tabularnewline
\hline 
{\tiny GALLAGHER21\_50} & 4503(0.50) & 5277(0.50) & 9643(0.50)\tabularnewline
\hline 
{\tiny GALLAGHER21\_100} & 1756(0.53) & 1523(0.53) & 1521(0.53)\tabularnewline
\hline 
{\tiny GALLAGHER21\_150} & 1662(0.27) & 1521(0.27) & 1526(0.27)\tabularnewline
\hline 
{\tiny GALLAGHER101\_25} & 2769(0.90) & 3472(0.90) & 5657(0.90)\tabularnewline
\hline 
{\tiny GALLAGHER101\_50} & 4890(0.50) & 6950(0.50) & 7454(0.50)\tabularnewline
\hline 
{\tiny GALLAGHER101\_100} & 5886(0.53) & 6846(0.53) & 9505(0.53)\tabularnewline
\hline 
{\tiny GALLAGHER101\_150} & 8646(0.27) & 7701(0.27) & 12352(0.27)\tabularnewline
\hline 
{\tiny GRIEWANK \_25} & 4084 & 5276 & 5145\tabularnewline
\hline 
{\tiny GRIEWANK \_50} & 5039 & 5138 & 5729\tabularnewline
\hline 
{\tiny GRIEWANK \_100} & 6460 & 5726 & 6002\tabularnewline
\hline 
{\tiny GRIEWANK \_150} & 6542 & 5870 & 6164\tabularnewline
\hline 
{\tiny GRIEWANK\_ROSENBROCK\_25} & 4466 & 7458 & 6939\tabularnewline
\hline 
{\tiny GRIEWANK\_ROSENBROCK\_50} & 5325 & 9766 & 9255\tabularnewline
\hline 
{\tiny GRIEWANK\_ROSENBROCK\_100} & 6465 & 11776 & 11001\tabularnewline
\hline 
{\tiny GRIEWANK\_ROSENBROCK\_150} & 7272 & 13482 & 12543\tabularnewline
\hline 
{\tiny ROSENBROCK\_25} & 5950 & 7824 & 7955\tabularnewline
\hline 
{\tiny ROSENBROCK\_50} & 8963 & 13970 & 13057\tabularnewline
\hline 
{\tiny ROSENBROCK\_100} & 15930 & 23402 & 22348\tabularnewline
\hline 
{\tiny ROSENBROCK\_150} & 22135 & 32850 & 31562\tabularnewline
\hline 
{\tiny RARSTIGIN\_25} & 4577(0.90) & 9691(0.90) & 10242(0.90)\tabularnewline
\hline 
{\tiny RARSTIGIN\_50} & 7746(0.50) & 13134(0.50) & 12740(0.50)\tabularnewline
\hline 
{\tiny RARSTIGIN\_100} & 9147(0.53) & 13128(0.53) & 13184(0.53)\tabularnewline
\hline 
{\tiny RARSTIGIN\_150} & 11620(0.27) & 15105(0.27) & 15602(0.27)\tabularnewline
\hline 
{\tiny SPHERE\_25} & 1481 & 1507 & 1512\tabularnewline
\hline 
{\tiny SPHERE\_50} & 1509 & 1534 & 1539\tabularnewline
\hline 
{\tiny SPHERE\_100} & 1524 & 1555 & 1556\tabularnewline
\hline 
{\tiny SPHERE\_150} & 1535 & 1568 & 1567\tabularnewline
\hline 
{\tiny STEP ELLIPSOIDAL\_25} & 1625(0.90) & 1642(0.90) & 2090(0.90)\tabularnewline
\hline 
{\tiny STEP ELLIPSOIDAL\_50} & 2300(0.50) & 2774(0.50) & 4021(0.50)\tabularnewline
\hline 
{\tiny STEP ELLIPSOIDAL\_100} & 2465(0.53) & 1598(0.53) & 1571(0.53)\tabularnewline
\hline 
{\tiny STEP ELLIPSOIDAL\_150} & 3143(0.27) & 1531(0.27) & 1521(0.27)\tabularnewline
\hline 
{\tiny SHARP RIDGE\_25} & 5104 & 6215 & 6026\tabularnewline
\hline 
{\tiny SHARP RIDGE\_50} & 5226 & 6850 & 7123\tabularnewline
\hline 
{\tiny SHARP RIDGE\_100} & 5995 & 7782 & 7649\tabularnewline
\hline 
{\tiny SHARP RIDGE\_150} & 6481 & 8112 & 8237\tabularnewline
\hline 
{\tiny ZAKHAROV\_25} & 2185 & 2752 & 2639\tabularnewline
\hline 
{\tiny ZAKHAROV\_50} & 3027 & 4063 & 3864\tabularnewline
\hline 
{\tiny ZAKHAROV\_100} & 5572 & 6265 & 5634\tabularnewline
\hline 
{\tiny ZAKHAROV\_150} & 6304 & 7574 & 7553\tabularnewline
\hline 
 & 387335(0.85) & 527444(0.85) & 543201(0.85)\tabularnewline
\hline 
\end{tabular}}{\tiny\par}
\end{sidewaystable}
}{\scriptsize\par}

Figure 3 presents the pairwise statistical analysis of the three migration
strategies MIGRANT (T), NUMBER (T), and RANDOM (T) based on their
required function evaluations. According to the Kruskal--Wallis test
(p = 0.3), no statistically significant overall difference is detected
among the three strategies, indicating that their distributions of
function calls are broadly comparable. Pairwise t-tests further support
this conclusion: none of the comparisons reach conventional significance
thresholds (ns: p \textgreater{} 0.05), demonstrating that the observed
differences in median and variability across the strategies are not
statistically meaningful. Although the MIGRANT (T) strategy tends
to exhibit slightly lower function-call counts on average, these differences
fall within the range of natural stochastic variation. Collectively,
these findings suggest that the three strategies perform similarly
with respect to computational cost. 

\begin{figure}
\centering{}\includegraphics[scale=0.5]{tournament_selection}\caption{A statistical comparison of the proposed with different weight selection.}
\end{figure}


\subsection{The effect of selection mechanism}

Table \ref{tab:Effect-of-Random}compares the four selection strategies
RANDOM(R), RANDOM(T), MIGRANT(R), and MIGRANT(T), where (R) denotes
purely random selection and (T) denotes tournament--based selection.
The results clearly show that MIGRANT(T) is by far the most efficient
method. It achieves the lowest total number of objective function
evaluations (387,335), significantly outperforming MIGRANT(R) (962,599),
RANDOM(T) (543,201), and RANDOM(R) (767,225). Since all methods achieve
the same success rate (0.85), the performance differences are due
solely to efficiency, demonstrating the importance of the selection
mechanism. This advantage becomes evident across nearly all tested
functions. For the Attractive Sector family (25--150 dimensions),
MIGRANT(T) consistently requires the fewest evaluations. For example,
in Attractive Sector\_25, it needs only 1697 calls, compared to 2174
MIGRANT(R), 1756 for RANDOM(T) and as many as 2162 for RANDOM(R).
The differences are even more striking for multimodal benchmarks such
as Buche--Rastrigin. In the 25-dimensional case, MIGRANT(T) achieves
5893 calls (0.90 success), while MIGRANT(R) needs 15,894, RANDOM(T)
needs 12,035, and RANDOM(R) 11,921. At the 150-dimensional level,
the gap widens dramatically: MIGRANT(T) requires 23,466 calls, whereas
MIGRANT(R) rises to 77,590, RANDOM(T) 39,263 and RANDOM(R) to 54,663.
These results highlight the strong stabilizing effect that tournament
selection has on the MIGRANT mechanism. The superiority of MIGRANT(T)
is even more pronounced in the Sharp Ridge functions. In Sharp Ridge\_150,
MIGRANT(T) completes the optimization with 6481 calls, while MIGRANT(R)
requires 12,053, RANDOM(T) 8237, and RANDOM(R) 12,395. Finally, in
the Zakharov functions, MIGRANT(T) again shows consistently superior
performance. In Zakharov\_150, MIGRANT(T) needs just 6304 calls, compared
to 25,370 for MIGRANT(R), 7553 RANDOM(T) and 16,240 for RANDOM(R).
Even in the easier 25-dimensional case, MIGRANT(T) requires 2185 calls,
whereas RANDOM(R) requires over twice as many (4605).

Overall, these results highlight the strong interaction between the
MIGRANT mechanism and tournament selection. Tournament selection dramatically
enhances the performance of MIGRANT, reducing the computational cost
by large margins across all functions while preserving identical success
rates. As a result, MIGRANT(T) emerges as the most balanced, stable,
and efficient strategy, making it highly suitable for optimization
scenarios where minimizing objective function evaluations is essential.

{\scriptsize{}
\begin{sidewaystable}[H]
{\scriptsize\caption{Effect of Random and Tournament Selection Strategies on Optimization
Performance\label{tab:Effect-of-Random}}
}{\scriptsize\par}

{\tiny{}%
\begin{tabular}{|c|c|c|c|c|}
\hline 
{\tiny\textbf{FUNCTION}} & {\tiny\textbf{MIGRANT (T)}} & {\tiny\textbf{MIGRANT (R)}} & {\tiny\textbf{RANDOM (T)}} & {\tiny\textbf{RANDOM (R)}}\tabularnewline
\hline 
\hline 
\begin{cellvarwidth}[t]
\centering
{\tiny ATTRACTIVE}{\tiny\par}

{\tiny SECTOR\_25}
\end{cellvarwidth} & 1697 & 2174 & 1756 & 2162\tabularnewline
\hline 
\begin{cellvarwidth}[t]
\centering
{\tiny ATTRACTIVE}{\tiny\par}

{\tiny SECTOR\_50}
\end{cellvarwidth} & 1761 & 2212 & 1828 & 2162\tabularnewline
\hline 
\begin{cellvarwidth}[t]
\centering
{\tiny ATTRACTIVE}{\tiny\par}

{\tiny SECTOR\_100}
\end{cellvarwidth} & 1832 & 2177 & 1880 & 2192\tabularnewline
\hline 
\begin{cellvarwidth}[t]
\centering
{\tiny ATTRACTIVE}{\tiny\par}

{\tiny SECTOR\_150}
\end{cellvarwidth} & 1867 & 2206 & 1920 & 2174\tabularnewline
\hline 
{\tiny BUCHE RASTRIGIN\_25} & 5893(0.90) & 15894(0.90) & 12035(0.90) & 11921(0.90)\tabularnewline
\hline 
{\tiny BUCHE RASTRIGIN\_50} & 12585(0.50) & 50438(0.50) & 20457(0.50) & 20542(0.50)\tabularnewline
\hline 
{\tiny BUCHE RASTRIGIN\_100} & 16490(0.53) & 59214(0.53) & 31465(0.53) & 34570(0.53)\tabularnewline
\hline 
{\tiny BUCHE RASTRIGIN\_150} & 23466(0.27) & 77590(0.27) & 39263(0.27) & 54663(0.27)\tabularnewline
\hline 
{\tiny DISCUS\_25} & 1992 & 2588 & 1896 & 2542\tabularnewline
\hline 
{\tiny DISCUS\_50} & 2060 & 2601 & 1971 & 2552\tabularnewline
\hline 
{\tiny DISCUS\_100} & 2104 & 2553 & 1989 & 2617\tabularnewline
\hline 
{\tiny DISCUS\_150} & 2144 & 2608 & 2040 & 2591\tabularnewline
\hline 
{\tiny DIFFERENTPOWERS\_25} & 6478 & 13918 & 11629 & 14477\tabularnewline
\hline 
{\tiny DIFFERENTPOWERS\_50} & 11183 & 20100 & 15179 & 20064\tabularnewline
\hline 
{\tiny DIFFERENTPOWERS\_100} & 16225 & 27396 & 20659 & 29408\tabularnewline
\hline 
{\tiny DIFFERENTPOWERS\_150} & 21495 & 35710 & 24670 & 35070\tabularnewline
\hline 
{\tiny ELLIPSOIDAL\_25} & 3590 & 6424 & 3958 & 5932\tabularnewline
\hline 
{\tiny ELLIPSOIDAL\_50} & 6424 & 11704 & 7184 & 10585\tabularnewline
\hline 
{\tiny ELLIPSOIDAL\_100} & 11549 & 20736 & 13890 & 20887\tabularnewline
\hline 
{\tiny ELLIPSOIDAL\_150} & 16930 & 29835 & 19940 & 28265\tabularnewline
\hline 
{\tiny GALLAGHER21\_25} & 2261(0.90) & 5412(0.90) & 6364(0.90) & 2891(0.90)\tabularnewline
\hline 
{\tiny GALLAGHER21\_50} & 4503(0.50) & 11988(0.50) & 9643(0.50) & 3311(0.50)\tabularnewline
\hline 
{\tiny GALLAGHER21\_100} & 1756(0.53) & 1565(0.53) & 1521(0.53) & 1524(0.53)\tabularnewline
\hline 
{\tiny GALLAGHER21\_150} & 1662(0.27) & 1490(0.27) & 1526(0.27) & 1520(0.27)\tabularnewline
\hline 
{\tiny GALLAGHER101\_25} & 2769(0.90) & 5180(0.90) & 5657(0.90) & 3016(0.90)\tabularnewline
\hline 
{\tiny GALLAGHER101\_50} & 4890(0.50) & 21179(0.50) & 7454(0.50) & 4878(0.50)\tabularnewline
\hline 
{\tiny GALLAGHER101\_100} & 5886(0.53) & 26739(0.53) & 9505(0.53) & 4507(0.53)\tabularnewline
\hline 
{\tiny GALLAGHER101\_150} & 8646(0.27) & 46866(0.27) & 12352(0.27) & 3400(0.27)\tabularnewline
\hline 
{\tiny GRIEWANK \_25} & 4084 & 8148 & 5145 & 9902\tabularnewline
\hline 
{\tiny GRIEWANK \_50} & 5039 & 7894 & 5729 & 5203\tabularnewline
\hline 
{\tiny GRIEWANK \_100} & 6460 & 9083 & 6002 & 4145\tabularnewline
\hline 
{\tiny GRIEWANK \_150} & 6542 & 9154 & 6164 & 4075\tabularnewline
\hline 
{\tiny GRIEWANK\_ROSENBROCK\_25} & 4466 & 11510 & 6939 & 17429\tabularnewline
\hline 
{\tiny GRIEWANK\_ROSENBROCK\_50} & 5325 & 14658 & 9255 & 24666\tabularnewline
\hline 
{\tiny GRIEWANK\_ROSENBROCK\_100} & 6465 & 15890 & 11001 & 34019\tabularnewline
\hline 
{\tiny GRIEWANK\_ROSENBROCK\_150} & 7272 & 17910 & 12543 & 39208\tabularnewline
\hline 
{\tiny ROSENBROCK\_25} & 5950 & 13718 & 7955 & 15591\tabularnewline
\hline 
{\tiny ROSENBROCK\_50} & 8963 & 21827 & 13057 & 23980\tabularnewline
\hline 
{\tiny ROSENBROCK\_100} & 15930 & 34948 & 22348 & 40245\tabularnewline
\hline 
{\tiny ROSENBROCK\_150} & 22135 & 49061 & 31562 & 53073\tabularnewline
\hline 
{\tiny RARSTIGIN\_25} & 4577(0.90) & 11276(0.90) & 10242(0.90) & 9910(0.90)\tabularnewline
\hline 
{\tiny RARSTIGIN\_50} & 7746(0.50) & 26967(0.50) & 12740(0.50) & 14234(0.50)\tabularnewline
\hline 
{\tiny RARSTIGIN\_100} & 9147(0.53) & 27639(0.53) & 13184(0.53) & 16666(0.53)\tabularnewline
\hline 
{\tiny RARSTIGIN\_150} & 11620(0.27) & 34865(0.27) & 15602(0.27) & 19135(0.27)\tabularnewline
\hline 
{\tiny SPHERE\_25} & 1481 & 1620 & 1512 & 1627\tabularnewline
\hline 
{\tiny SPHERE\_50} & 1509 & 1641 & 1539 & 1634\tabularnewline
\hline 
{\tiny SPHERE\_100} & 1524 & 1635 & 1556 & 1644\tabularnewline
\hline 
{\tiny SPHERE\_150} & 1535 & 1644 & 1567 & 1639\tabularnewline
\hline 
{\tiny STEP ELLIPSOIDAL\_25} & 1625(0.90) & 2073(0.90) & 2090(0.90) & 1750(0.90)\tabularnewline
\hline 
{\tiny STEP ELLIPSOIDAL\_50} & 2300(0.50) & 5937(0.50) & 4021(0.50) & 1664(0.50)\tabularnewline
\hline 
{\tiny STEP ELLIPSOIDAL\_100} & 2465(0.53) & 6546(0.53) & 1571(0.53) & 1523(0.53)\tabularnewline
\hline 
{\tiny STEP ELLIPSOIDAL\_150} & 3143(0.27) & 11487(0.27) & 1521(0.27) & 1520(0.27)\tabularnewline
\hline 
{\tiny SHARP RIDGE\_25} & 5104 & 10153 & 6026 & 11776\tabularnewline
\hline 
{\tiny SHARP RIDGE\_50} & 5226 & 11108 & 7123 & 12123\tabularnewline
\hline 
{\tiny SHARP RIDGE\_100} & 5995 & 11592 & 7649 & 12704\tabularnewline
\hline 
{\tiny SHARP RIDGE\_150} & 6481 & 12053 & 8237 & 12395\tabularnewline
\hline 
{\tiny ZAKHAROV\_25} & 2185 & 3941 & 2639 & 4605\tabularnewline
\hline 
{\tiny ZAKHAROV\_50} & 3027 & 8972 & 3864 & 7963\tabularnewline
\hline 
{\tiny ZAKHAROV\_100} & 5572 & 23782 & 5634 & 14514\tabularnewline
\hline 
{\tiny ZAKHAROV\_150} & 6304 & 25370 & 7553 & 16240\tabularnewline
\hline 
 & 387335(0.85) & 962599(0.85) & 543201(0.85) & 767225(0.85)\tabularnewline
\hline 
\end{tabular}}{\tiny\par}
\end{sidewaystable}
}{\scriptsize\par}

Figure 4 presents the pairwise statistical analysis between the four
strategies MIGRANT(T), MIGRANT(R), RANDOM(T), and RANDOM(R) based
on their function-call distributions. A Kruskal--Wallis test reports
a statistically significant overall difference among the groups (p
= 0.0019), indicating that at least one strategy differs meaningfully
from the others. To further investigate these differences, pairwise
t-tests were conducted, and the resulting p-values were annotated
using the standard significance notation (ns: p \textgreater{} 0.05,
: p \textless{} 0.05, {*}: p \textless{} 0.01, {*}{*}{*}: p \textless{}
0.001, {*}{*}{*}{*}: p \textless{} 0.0001). The results show that
MIGRANT(T) performs significantly better than both MIGRANT(R) and
RANDOM(R), achieving {*}{*}level differences against the highest-cost
configuration. Additionally, MIGRANT(R) and RANDOM(T) exhibit intermediate
performance, with several comparisons reaching or {*}{*}{*} significance
levels, indicating meaningful but less pronounced differences. Comparisons
classified as \textquotedbl ns\textquotedbl{} suggest that some pairs
have statistically indistinguishable behavior. Overall, the distribution
of function calls shows that the MIGRANT-based strategies, particularly
MIGRANT(T), tend to require fewer evaluations, demonstrating stronger
computational efficiency relative to the RANDOM variants.

\begin{figure}
\centering{}\includegraphics[scale=0.5]{comparison_tournament_random_selection}\caption{Statistical Comparison of Random and Tournament Selection Strategies
on Optimization Performance.}
\end{figure}


\subsection{The effect of sampling method}

In Table \ref{tab:Experiments-on-the} tournament selection is used
to choose the samples that participate in the core operator of Differential
Evolution. Four strategies for computing the differential weight are
evaluated: random weight with uniform sampling (Random(U)), random
weight with k-means sampling (Random(K)), MIGRANT weight with uniform
sampling (Migrant(U)), and MIGRANT weight with k-means sampling (Migrant(K)).
The k-means method, originally proposed by MacQueen\citep{MacQueen}
and used extensively in later work {[}65, 66{]}, is employed not only
to determine cluster centers but also as a structured sampling mechanism.
Across all test functions, MIGRANT(K) consistently achieves the lowest
total number of function calls (387,335) with a success rate of 0.85,
outperforming all other sampling strategies. This advantage becomes
clear when examining individual benchmarks. For the Attractive Sector
family (dimensions 25--150), MIGRANT(K) systematically requires fewer
evaluations than MIGRANT(U) and both Random methods. For example,
in Attractive Sector\_25, MIGRANT(K) uses only 1697 evaluations compared
to 1738for MIGRANT(U), while Random(K) and Random(U) require 1756
and 1792 respectively. This pattern holds across all dimensionalities,
showing the benefit of structured sampling in unimodal landscapes.
The effect becomes far more pronounced in multimodal functions such
as Buche--Rastrigin. In Buche Rastrigin\_25, MIGRANT(K) needs 5893
function calls with a success rate of 0.90, in contrast to MIGRANT(U)'s
12,818 calls (0.03). Random(K) performs similarly to MIGRANT(K) in
success rate but requires more evaluations (12,035), while Random(U)
is by far the least efficient (28,865 calls). The difference becomes
dramatic in higher dimensions: in Buche Rastrigin\_150, MIGRANT(K)
performs the task in 23,466 calls (0.27), whereas Random(U) escalates
to 90,211, showing the instability of uniform sampling in complex
landscapes. Although differences here are smaller due to the problemâ€™s
structure, MIGRANT(K) preserves its advantage in stability. The superiority
of MIGRANT(K) is again evident in the Step Ellipsoidal group. In Step
Ellipsoidal\_25, MIGRANT(K) requires only 5104 calls, remarkably lower
than Random(U) (6699), Random (K) (6026) and still better than MIGRANT(U)
(5014, but with lower success). Finally, for the Zakharov functions,
MIGRANT(K) again shows the best balance between evaluation cost and
success rate. In Zakharov\_25, MIGRANT(K) requires only 2185 evaluations,
beating Migrant(U) (2283), Random(K) (2639) and Random(U) (2797). 

Overall, integrating k-means sampling into the MIGRANT strategy leads
to substantial improvements in both efficiency and reliability. MIGRANT(K)
not only requires the fewest total function evaluations but also maintains
high success rates across diverse problem categories, making it the
most effective approach for the benchmark set. In contrast, Random(U)
repeatedly demonstrates the lowest efficiency, highlighting the advantage
of structured sampling over uniform dispersion in high-dimensional
optimization.

{\scriptsize{}
\begin{sidewaystable}[H]
{\scriptsize\caption{Experiments on the performance of differential evolution using sampling
methods\label{tab:Experiments-on-the}}
}{\scriptsize\par}

{\tiny{}%
\begin{tabular}{|c|c|c|c|c|}
\hline 
{\tiny\textbf{FUNCTION}} & {\tiny\textbf{MIGRANT (K)}} & {\tiny\textbf{MIGRANT (U)}} & {\tiny\textbf{RANDOM (K)}} & {\tiny\textbf{RANDOM (U)}}\tabularnewline
\hline 
\hline 
\begin{cellvarwidth}[t]
\centering
{\tiny ATTRACTIVE}{\tiny\par}

{\tiny SECTOR\_25}
\end{cellvarwidth} & 1697 & 1738 & 1756 & 1792\tabularnewline
\hline 
\begin{cellvarwidth}[t]
\centering
{\tiny ATTRACTIVE}{\tiny\par}

{\tiny SECTOR\_50}
\end{cellvarwidth} & 1761 & 1792 & 1828 & 1832\tabularnewline
\hline 
\begin{cellvarwidth}[t]
\centering
{\tiny ATTRACTIVE}{\tiny\par}

{\tiny SECTOR\_100}
\end{cellvarwidth} & 1832 & 1866 & 1880 & 1891\tabularnewline
\hline 
\begin{cellvarwidth}[t]
\centering
{\tiny ATTRACTIVE}{\tiny\par}

{\tiny SECTOR\_150}
\end{cellvarwidth} & 1867 & 1890 & 1920 & 1920\tabularnewline
\hline 
{\tiny BUCHE RASTRIGIN\_25} & 5893(0.90) & 12818(0.03) & 12035(0.90) & 28865(0.03)\tabularnewline
\hline 
{\tiny BUCHE RASTRIGIN\_50} & 12585(0.50) & 23622(0.03) & 20457(0.50) & 34379(0.03)\tabularnewline
\hline 
{\tiny BUCHE RASTRIGIN\_100} & 16490(0.53) & 41526(0.03) & 31465(0.53) & 70319(0.03)\tabularnewline
\hline 
{\tiny BUCHE RASTRIGIN\_150} & 23466(0.27) & 55612(0.03) & 39263(0.27) & 90211(0.03)\tabularnewline
\hline 
{\tiny DIFFERENT POWERS\_25} & 1992 & 2016 & 1896 & 1936\tabularnewline
\hline 
{\tiny DIFFERENT POWERS\_50} & 2060 & 2077 & 1971 & 1989\tabularnewline
\hline 
{\tiny DIFFERENT POWERS\_100} & 2104 & 2114 & 1989 & 2026\tabularnewline
\hline 
{\tiny DIFFERENT POWERS\_150} & 2144 & 2150 & 2040 & 2058\tabularnewline
\hline 
{\tiny DISCUS\_25} & 6478 & 7368 & 11629 & 11484\tabularnewline
\hline 
{\tiny DISCUS\_50} & 11183 & 11666 & 15179 & 15789\tabularnewline
\hline 
{\tiny DISCUS\_100} & 16225 & 17566 & 20659 & 21459\tabularnewline
\hline 
{\tiny DISCUS\_150} & 21495 & 22526 & 24670 & 24485\tabularnewline
\hline 
{\tiny ELLIPSOIDAL\_25} & 3590 & 3640 & 3958 & 3873\tabularnewline
\hline 
{\tiny ELLIPSOIDAL\_50} & 6424 & 6399 & 7184 & 7022\tabularnewline
\hline 
{\tiny ELLIPSOIDAL\_100} & 11549 & 12161 & 13890 & 13610\tabularnewline
\hline 
{\tiny ELLIPSOIDAL\_150} & 16930 & 17905 & 19940 & 19576\tabularnewline
\hline 
{\tiny GALLAGHER21\_25} & 2261(0.90) & 6920(0.03) & 6364(0.90) & 34112(0.03)\tabularnewline
\hline 
{\tiny GALLAGHER21\_50} & 4503(0.50) & 7904(0.03) & 9643(0.50) & 17404(0.03)\tabularnewline
\hline 
{\tiny GALLAGHER21\_100} & 1756(0.53) & 1463 & 1521(0.53) & 1524(0.53)\tabularnewline
\hline 
{\tiny GALLAGHER21\_150} & 1662(0.27) & 1463 & 1526(0.27) & 1522(0.27)\tabularnewline
\hline 
{\tiny GALLAGHER101\_25} & 2769(0.90) & 6395(0.03) & 5657(0.90) & 27324(0.03)\tabularnewline
\hline 
{\tiny GALLAGHER101\_50} & 4890(0.50) & 8204(0.03) & 7454(0.50) & 17075(0.03)\tabularnewline
\hline 
{\tiny GALLAGHER101\_100} & 5886(0.53) & 10816(0.03) & 9505(0.53) & 18232(0.03)\tabularnewline
\hline 
{\tiny GALLAGHER101\_150} & 8646(0.27) & 12129(0.03) & 12352(0.27) & 17231(0.03)\tabularnewline
\hline 
\begin{cellvarwidth}[t]
\centering
{\tiny GRIEWANK}{\tiny\par}

{\tiny ROSENBROCK\_25}
\end{cellvarwidth} & 4084 & 4353 & 5145 & 5434\tabularnewline
\hline 
\begin{cellvarwidth}[t]
\centering
{\tiny GRIEWANK}{\tiny\par}

{\tiny ROSENBROCK\_50}
\end{cellvarwidth} & 5039 & 5290 & 5729 & 5631\tabularnewline
\hline 
\begin{cellvarwidth}[t]
\centering
{\tiny GRIEWANK}{\tiny\par}

{\tiny ROSENBROCK\_100}
\end{cellvarwidth} & 6460 & 6211 & 6002 & 5916\tabularnewline
\hline 
\begin{cellvarwidth}[t]
\centering
{\tiny GRIEWANK}{\tiny\par}

{\tiny ROSENBROCK\_150}
\end{cellvarwidth} & 6542 & 6895 & 6164 & 6113\tabularnewline
\hline 
{\tiny GRIEWANK\_25} & 4466 & 4818 & 6939 & 7697\tabularnewline
\hline 
{\tiny GRIEWANK\_50} & 5325 & 7163 & 9255 & 11056\tabularnewline
\hline 
{\tiny GRIEWANK\_100} & 6465 & 9992 & 11001 & 15311\tabularnewline
\hline 
{\tiny GRIEWANK\_150} & 7272 & 12350 & 12543 & 19125\tabularnewline
\hline 
{\tiny RARSTIGIN\_25} & 5950 & 5909(0.03) & 7955 & 8447(0.03)\tabularnewline
\hline 
{\tiny RARSTIGIN\_50} & 8963 & 10112(0.03) & 13057 & 13669(0.03)\tabularnewline
\hline 
{\tiny RARSTIGIN\_100} & 15930 & 16541(0.03) & 22348 & 23760(0.03)\tabularnewline
\hline 
{\tiny RARSTIGIN\_150} & 22135 & 23181(0.03) & 31562 & 33005(0.03)\tabularnewline
\hline 
{\tiny ROSENBROCK\_25} & 4577(0.90) & 9432 & 10242(0.90) & 18663\tabularnewline
\hline 
{\tiny ROSENBROCK\_50} & 7746(0.50) & 11863 & 12740(0.50) & 27806\tabularnewline
\hline 
{\tiny ROSENBROCK\_100} & 9147(0.53) & 15307 & 13184(0.53) & 28064\tabularnewline
\hline 
{\tiny ROSENBROCK\_150} & 11620(0.27) & 18904 & 15602(0.27) & 43292\tabularnewline
\hline 
{\tiny SHARP RIDGE\_25} & 1481 & 1498 & 1512 & 1528\tabularnewline
\hline 
{\tiny SHARP RIDGE\_50} & 1509 & 1516 & 1539 & 1548\tabularnewline
\hline 
{\tiny SHARP RIDGE\_100} & 1524 & 1531 & 1556 & 1559\tabularnewline
\hline 
{\tiny SHARP RIDGE\_150} & 1535 & 1548 & 1567 & 1565\tabularnewline
\hline 
{\tiny SPHERE\_25} & 1625(0.90) & 2733 & 2090(0.90) & 7103\tabularnewline
\hline 
{\tiny SPHERE\_50} & 2300(0.50) & 3173 & 4021(0.50) & 6384\tabularnewline
\hline 
{\tiny SPHERE\_100} & 2465(0.53) & 3654 & 1571(0.53) & 5873\tabularnewline
\hline 
{\tiny SPHERE\_150} & 3143(0.27) & 4073 & 1521(0.27) & 5149\tabularnewline
\hline 
{\tiny STEP ELLIPSOIDAL\_25} & 5104 & 5014(0.03) & 6026 & 6699(0.03)\tabularnewline
\hline 
{\tiny STEP ELLIPSOIDAL\_50} & 5226 & 5581(0.03) & 7123 & 7205(0.03)\tabularnewline
\hline 
{\tiny STEP ELLIPSOIDAL\_100} & 5995 & 6091(0.03) & 7649 & 7893(0.03)\tabularnewline
\hline 
{\tiny STEP ELLIPSOIDAL\_150} & 6481 & 5996(0.03) & 8237 & 8037(0.03)\tabularnewline
\hline 
{\tiny ZAKHAROV\_25} & 2185 & 2283 & 2639 & 2797\tabularnewline
\hline 
{\tiny ZAKHAROV\_50} & 3027 & 2901 & 3864 & 3743\tabularnewline
\hline 
{\tiny ZAKHAROV\_100} & 5572 & 4122 & 5634 & 5936\tabularnewline
\hline 
{\tiny ZAKHAROV\_150} & 6304 & 5282 & 7553 & 7460\tabularnewline
\hline 
 & 387335(0.85) & 529063(0.71) & 543201(0.85) & 844408(0.69)\tabularnewline
\hline 
\end{tabular}}{\tiny\par}
\end{sidewaystable}
}{\scriptsize\par}

Figure 5 presents the pairwise t-tests statistical comparison of the
four strategies (MIGRANT(K), RANDOM(K), MIGRANT(U), and RANDOM(U))
based on their function-call distributions. A Kruskal--Wallis test
indicates a statistically significant overall difference among the
groups (p = 0.034), suggesting that at least one strategy exhibits
a distinct performance profile. Pairwise t-tests, corrected using
the conventional significance notation (ns: p \textgreater{} 0.05,
: p \textless{} 0.05, {*}: p \textless{} 0.01, {*}: p \textless{}
0.001, : p \textless{} 0.0001), reveal that MIGRANT(K) differs very
significantly from RANDOM(K) and significantly from MIGRANT(U). In
contrast, the difference between MIGRANT(U) and RANDOM(U) is not statistically
significant (ns). Taken together, these results show that strategies
based on migration with K-type selection (MIGRANT(K)) demonstrate
consistently lower function-call requirements, while the U-type variants
display comparable behavior to the RANDOM(U) baseline. Overall, the
statistical evidence suggests that the K-based migration mechanism
yields a measurable performance advantage relative to the other strategies.

\begin{figure}
\centering{}\includegraphics[scale=0.5]{sample}\caption{Statistical Comparison of Different Sampling Method Combinations in
Differential Evolution Performance.}
\end{figure}


\subsection{The effect of local search rate }

From Table \ref{tab:Experiments-on-the-1} we observe the influence
of periodic local optimization on the performance of the MIGRANT method,
considering four different local search rates: 0.005, 0.01, 0.03,
and 0.05. Among all settings, the 0.005 rate achieves the lowest total
number of function calls (148,027) while maintaining a high success
rate of 0.85, thus providing the best balance between computational
efficiency and optimization reliability. This advantage is consistently
reflected across the benchmark functions. In the Attractive Sector
functions (25--150 dimensions), the 0.005 rate clearly outperforms
the higher-rate configurations. For instance, in Attractive Sector\_25,
it requires only 1441 calls, compared to 1603 for the 0.03 rate and
1697 for the 0.05 rate. The improvement persists as the dimensionality
increases: Attractive Sector\_150 is solved with 1536 calls at the
0.005 rate, while the 0.05 rate requires 1867 calls. The improvement
becomes dramatically more pronounced in the Buche--Rastrigin family,
where the complexity and multimodality amplify the benefit of lower
local search frequency. For Buche Rastrigin\_25, the 0.005 method
requires 2035 function calls (0.90 success), whereas the 0.05 rate
jumps to 5893 calls nearly triple. In the high-dimensional case Buche
Rastrigin\_150, the difference is even more striking: 5900 calls at
the 0.005 rate versus 23,466 for the 0.05 rate. A similar trend can
be seen in the Discus, Sharp Ridge, and Step Ellipsoidal functions.
In Step Ellipsoidal\_50, the 0.005 rate achieves 2136 calls (0.50),
far below the 0.05 rate (2300). For Step Ellipsoidal\_150, the 0.005
variant uses 2914 calls (0.27), while the 0.05 rate needs 3143 calls.
Even in unimodal functions like Discus, the 0.005 method consistently
leads to lower evaluation costs e.g., Discus\_25 requires 1525 calls
vs. 1992 for the 0.05 rate. The Sharp Ridge functions highlight this
behavior even more strongly. For Sharp Ridge\_25, the 0.005 rate requires
only 1934 calls, in contrast to 5104 calls for the 0.05 rate more
than a 2.5\texttimes{} increase. Similar improvements appear in Sharp
Ridge\_150, where the function calls rise from 2350 at 0.005 to 6481
at 0.05. 

In summary, using a lower local search rate specifically the 0.005
setting results in the most efficient optimization behavior across
all tested functions. This variant provides the lowest objective function
calls without compromising success rate, making it the optimal choice
when both efficiency and reliability are essential in high-dimensional
optimization tasks.

{\scriptsize{}
\begin{sidewaystable}[H]
{\scriptsize\caption{Experiments on the Effect of Local Search Rate on Optimization Performance
in Differential Evolution\textbf{.\label{tab:Experiments-on-the-1}}}
}{\scriptsize\par}

{\tiny{}%
\begin{tabular}{|c|c|c|c|c|}
\hline 
{\tiny\textbf{FUNCTION}} & {\footnotesize MIGRANT(0.005)} & {\footnotesize MIGRANT(0.01)} & {\footnotesize MIGRANT(0.03)} & {\footnotesize MIGRANT(0.05)}\tabularnewline
\hline 
\hline 
\begin{cellvarwidth}[t]
\centering
{\tiny ATTRACTIVE}{\tiny\par}

{\tiny SECTOR\_25}
\end{cellvarwidth} & 1441 & 1472 & 1603 & 1697\tabularnewline
\hline 
\begin{cellvarwidth}[t]
\centering
{\tiny ATTRACTIVE}{\tiny\par}

{\tiny SECTOR\_50}
\end{cellvarwidth} & 1582 & 1522 & 1674 & 1761\tabularnewline
\hline 
\begin{cellvarwidth}[t]
\centering
{\tiny ATTRACTIVE}{\tiny\par}

{\tiny SECTOR\_100}
\end{cellvarwidth} & 1516 & 1552 & 1699 & 1832\tabularnewline
\hline 
\begin{cellvarwidth}[t]
\centering
{\tiny ATTRACTIVE}{\tiny\par}

{\tiny SECTOR\_150}
\end{cellvarwidth} & 1536 & 1560 & 1726 & 1867\tabularnewline
\hline 
{\tiny BUCHE RASTRIGIN\_25} & 2035(0.90) & 2502(0.90) & 4323(0.90) & 5893(0.90)\tabularnewline
\hline 
{\tiny BUCHE RASTRIGIN\_50} & 3468(0.50) & 4319(0.50) & 8496(0.50) & 12585(0.50)\tabularnewline
\hline 
{\tiny BUCHE RASTRIGIN\_100} & 4179(0.53) & 5700(0.53) & 10756(0.53) & 16490(0.53)\tabularnewline
\hline 
{\tiny BUCHE RASTRIGIN\_150} & 5900(0.27) & 7794(0.27) & 14818(0.27) & 23466(0.27)\tabularnewline
\hline 
{\tiny DISCUS\_25} & 1525 & 1616 & 1841 & 1992\tabularnewline
\hline 
{\tiny DISCUS\_50} & 1615 & 1658 & 1919 & 2060\tabularnewline
\hline 
{\tiny DISCUS\_100} & 1578 & 1655 & 1979 & 2104\tabularnewline
\hline 
{\tiny DISCUS\_150} & 1590 & 1663 & 1987 & 2144\tabularnewline
\hline 
{\tiny DIFFERENTPOWERS\_25} & 2296 & 2855 & 4661 & 6478\tabularnewline
\hline 
{\tiny DIFFERENTPOWERS\_50} & 3011 & 3807 & 7580 & 11183\tabularnewline
\hline 
{\tiny DIFFERENTPOWERS\_100} & 3827 & 5693 & 11214 & 16225\tabularnewline
\hline 
{\tiny DIFFERENTPOWERS\_150} & 4736 & 7158 & 15238 & 21495\tabularnewline
\hline 
{\tiny ELLIPSOIDAL\_25} & 1765 & 2011 & 2940 & 3590\tabularnewline
\hline 
{\tiny ELLIPSOIDAL\_50} & 2235 & 2844 & 4854 & 6424\tabularnewline
\hline 
{\tiny ELLIPSOIDAL\_100} & 3234 & 4557 & 9215 & 11549\tabularnewline
\hline 
{\tiny ELLIPSOIDAL\_150} & 4581 & 6620 & 12510 & 16930\tabularnewline
\hline 
{\tiny GALLAGHER21\_25} & 1751(0.90) & 1804(0.90) & 2049(0.90) & 2261(0.90)\tabularnewline
\hline 
{\tiny GALLAGHER21\_50} & 2842(0.50) & 3012(0.50) & 3765(0.50) & 4503(0.50)\tabularnewline
\hline 
{\tiny GALLAGHER21\_100} & 1432(0.53) & 1470(0.53) & 1609(0.53) & 1756(0.53)\tabularnewline
\hline 
{\tiny GALLAGHER21\_150} & 1434(0.27) & 1455(0.27) & 1554(0.27) & 1662(0.27)\tabularnewline
\hline 
{\tiny GALLAGHER101\_25} & 1778(0.90) & 1896(0.90) & 2359(0.90) & 2769(0.90)\tabularnewline
\hline 
{\tiny GALLAGHER101\_50} & 3287(0.50) & 3470(0.50) & 4186(0.50) & 4890(0.50)\tabularnewline
\hline 
{\tiny GALLAGHER101\_100} & 3550(0.53) & 3804(0.53) & 4851(0.53) & 5886(0.53)\tabularnewline
\hline 
{\tiny GALLAGHER101\_150} & 4726(0.27) & 5208(0.27) & 6959(0.27) & 8646(0.27)\tabularnewline
\hline 
{\tiny GRIEWANK \_25} & 1858 & 2102 & 3137 & 4084\tabularnewline
\hline 
{\tiny GRIEWANK \_50} & 2154 & 2407 & 3859 & 5039\tabularnewline
\hline 
{\tiny GRIEWANK \_100} & 2135 & 2688 & 4542 & 6460\tabularnewline
\hline 
{\tiny GRIEWANK \_150} & 2298 & 2937 & 4919 & 6542\tabularnewline
\hline 
{\tiny GRIEWANK\_ROSENBROCK\_25} & 1840 & 2116 & 3292 & 4466\tabularnewline
\hline 
{\tiny GRIEWANK\_ROSENBROCK\_50} & 2091 & 2661 & 4199 & 5325\tabularnewline
\hline 
{\tiny GRIEWANK\_ROSENBROCK\_100} & 2343 & 2969 & 4868 & 6465\tabularnewline
\hline 
{\tiny GRIEWANK\_ROSENBROCK\_150} & 2512 & 3295 & 5496 & 7272\tabularnewline
\hline 
{\tiny ROSENBROCK\_25} & 1964 & 2450 & 4091 & 5950\tabularnewline
\hline 
{\tiny ROSENBROCK\_50} & 2675 & 3619 & 6578 & 8963\tabularnewline
\hline 
{\tiny ROSENBROCK\_100} & 3616 & 5278 & 10570 & 15930\tabularnewline
\hline 
{\tiny ROSENBROCK\_150} & 5326 & 7819 & 15572 & 22135\tabularnewline
\hline 
{\tiny RARSTIGIN\_25} & 1831(0.90) & 2135(0.90) & 3346(0.90) & 4577(0.90)\tabularnewline
\hline 
{\tiny RARSTIGIN\_50} & 2858(0.50) & 3334(0.50) & 5664(0.50) & 7746(0.50)\tabularnewline
\hline 
{\tiny RARSTIGIN\_100} & 2941(0.53) & 3697(0.53) & 6336(0.53) & 9147(0.53)\tabularnewline
\hline 
{\tiny RARSTIGIN\_150} & 3997(0.27) & 4826(0.27) & 8275(0.27) & 11620(0.27)\tabularnewline
\hline 
{\tiny SPHERE\_25} & 1402 & 1411 & 1455 & 1481\tabularnewline
\hline 
{\tiny SPHERE\_50} & 1537 & 1444 & 1475 & 1509\tabularnewline
\hline 
{\tiny SPHERE\_100} & 1463 & 1454 & 1489 & 1524\tabularnewline
\hline 
{\tiny SPHERE\_150} & 1481 & 1469 & 1494 & 1535\tabularnewline
\hline 
{\tiny STEP ELLIPSOIDAL\_25} & 1513(0.90) & 1526(0.90) & 1576(0.90) & 1625(0.90)\tabularnewline
\hline 
{\tiny STEP ELLIPSOIDAL\_50} & 2136(0.50) & 2155(0.50) & 2229(0.50) & 2300(0.50)\tabularnewline
\hline 
{\tiny STEP ELLIPSOIDAL\_100} & 2286(0.53) & 2308(0.53) & 2389(0.53) & 2465(0.53)\tabularnewline
\hline 
{\tiny STEP ELLIPSOIDAL\_150} & 2914(0.27) & 2938(0.27) & 3040(0.27) & 3143(0.27)\tabularnewline
\hline 
{\tiny SHARP RIDGE\_25} & 1934 & 2269 & 3453 & 5104\tabularnewline
\hline 
{\tiny SHARP RIDGE\_50} & 2130 & 2680 & 4042 & 5226\tabularnewline
\hline 
{\tiny SHARP RIDGE\_100} & 2190 & 2718 & 4489 & 5995\tabularnewline
\hline 
{\tiny SHARP RIDGE\_150} & 2350 & 3107 & 5131 & 6481\tabularnewline
\hline 
{\tiny ZAKHAROV\_25} & 1570 & 1635 & 1912 & 2185\tabularnewline
\hline 
{\tiny ZAKHAROV\_50} & 1714 & 1884 & 2505 & 3027\tabularnewline
\hline 
{\tiny ZAKHAROV\_100} & 2428 & 2677 & 4597 & 5572\tabularnewline
\hline 
{\tiny ZAKHAROV\_150} & 2090 & 2314 & 4721 & 6304\tabularnewline
\hline 
 & 148027(0.85) & 178999(0.85) & 289106(0.85) & 387335(0.85)\tabularnewline
\hline 
\end{tabular}}{\tiny\par}
\end{sidewaystable}
}{\scriptsize\par}

Figure 6 presents the pairwise statistical comparisons among the four
parameter configurations (0.001, 0.01, 0.03, 0.05) using Pairwise
t-tests. The global Kruskal--Wallis test indicates a statistically
significant difference across the groups (p = 2.9e-08), suggesting
that the parameter choice has a measurable impact on the number of
function evaluations.

All pairwise comparisons were evaluated using independent t-tests,
and the resulting p-values were interpreted using the conventional
significance notation (ns: p \textgreater{} 0.05, : p \textless{}
0.05, {*}: p \textless{} 0.01, {*}{*}{*}: p \textless{} 0.001, {*}{*}:
p \textless{} 0.0001). As illustrated in the figure, most pairwise
differences reach {*} or {*}{*}{*}{*} levels of significance, highlighting
strong and highly consistent differences between the examined parameter
settings. Only a limited number of comparisons fall into the non-significant
range, indicating that in the majority of cases each parameter configuration
leads to distinguishable performance in terms of function calls. These
results confirm that the tested parameter values influence the optimizerâ€™s
efficiency in a systematic and statistically robust manner. The differences
detected are unlikely to be attributed to random variation and instead
reflect meaningful performance changes caused by the selected parameter
settings.

\begin{figure}
\centering{}\includegraphics[scale=0.5]{lrate}\caption{Statistical comparison for the proposed method and different values
of parameter $p_{l}$.}
\end{figure}


\subsection{Practical problems}

To further examine the practical efficiency and scalability of the
proposed optimization algorithm, two real-world engineering design
problems were investigated: the GasCycle\citep{key-41} and the Tandem
Queueing System\citep{key-42}. These problems were selected because
they differ significantly in mathematical formulation and computational
complexity, providing a comprehensive framework for evaluating the
algorithmâ€™s performance under diverse and realistic conditions.

Each problem was tested across multiple dimensional configurations,
ranging from 25 to 500 variables, in order to assess how the algorithm
behaves as the search space becomes more complex. For every configuration,
the execution time in seconds was recorded as the main performance
indicator. This experimental setup enables a direct comparison of
how computational efficiency changes with increasing dimensionality.
\begin{itemize}
\item \textbf{GasCycle Thermal Cycle}

Vars: $\bm{x}=[T_{1},T_{3},P_{1},P_{3}]^{\top}$. \quad{}$r=P_{3}/P_{1},\ \gamma=1.4.$
\[
\eta(\bm{x})=1-r^{-(\gamma-1)/\gamma}\,\frac{T_{1}}{T_{3}},\qquad\min_{\bm{x}}f(\bm{x})=-\eta(\bm{x}).
\]
Bounds: $300\!\le\!T_{1}\!\le\!1500,\;1200\!\le\!T_{3}\!\le\!2000,\;1\!\le\!P_{1},P_{3}\!\le\!20.$

Penalty: infeasible $\Rightarrow f=10^{20}$.

The experimental evaluation of the three algorithms demonstrates a
predictable increase in execution time as the dimensionality of the
problem grows, which aligns with the expected computational complexity
of population-based optimization methods. In lower and medium dimensions,
the algorithms exhibit highly comparable behavior, with only minor
fluctuations that reflect inherent differences in their internal search
mechanisms. As the dimensionality increases, more noticeable distinctions
begin to emerge. In certain higher-dimensional cases, the proposed
algorithm requires a longer execution time compared to the other two
approaches. This outcome can be attributed to the structural characteristics
and computational demands of its individual components, without implying
any deficiency in its design. On the contrary, the overall performance
profile of the proposed method remains consistent and closely aligned
with that of the Genetic and IPSO algorithms, indicating that it scales
reliably even as the dimensionality becomes substantially larger.
Overall, the results show that all methods behave robustly with respect
to execution time, with variations that are reasonable and expected
given their algorithmic properties. The observed differences do not
affect the general conclusion that the proposed algorithm falls well
within the performance range of established techniques and maintains
dependable behavior across the full spectrum of tested dimensions.

\begin{figure}
\begin{centering}
\includegraphics[scale=0.25]{GasCycle}
\par\end{centering}
\caption{Execution Time Comparison of Different Algorithms Across Increasing
Dimensions( GasCycle)}
\end{figure}

\item \textbf{Tandem Space Trajectory} (MGA-1DSM, EVEEJ + 2$\times$Saturn)

Vars\textbf{ ($D{=}18$):} $\bm{x}=[t_{0},T_{1},T_{2},T_{3},T_{4},T_{5A},T_{5B},s_{1},s_{2},s_{3},s_{4},s_{5A},s_{5B},r_{p},k_{A1},k_{A2},k_{B1},k_{B2}]^{\top}$.
\[
\begin{aligned} & 7000\le t_{0}\le10000,\\
 & 30\le T_{1}\le500,\;30\le T_{2}\le600,\;30\le T_{3}\le1200,\\
 & 30\le T_{4}\le1600,\;30\le T_{5A},T_{5B}\le2000,\\
 & 0\le s_{1..4},s_{5A},s_{5B},r_{p},k_{A1},k_{A2},k_{B1},k_{B2}\le1.
\end{aligned}
\]

Objective: 
\[
\min_{\bm{x}}\ \Delta V_{\text{tot}}=\Delta V_{\text{launch}}(T_{1})+\Delta V_{\text{legs}}(T_{1}{:}T_{4})+\Delta V_{A}+\Delta V_{B}+\Delta V_{\text{DSM}}(\bm{s},r_{p})-G_{\text{GA}}-G_{J}+P_{\text{hard}}+P_{\text{soft}},
\]
\medskip{}
 
\[
P_{\text{soft}}=\beta\max\!\Big\{0,\ (T_{1}{+}\cdots{+}T_{4}+\tfrac{1}{2}(T_{5A}{+}T_{5B}))-3500\Big\}.
\]
Notes: $\Delta V_{\text{launch}}$ decreases (log-like) in $T_{1}$
($\ge6$ km/s floor), leg/branch costs decrease with TOF.

\medskip{}

The execution time analysis for the Tandem problem demonstrates that
the Proposed algorithm consistently achieves lower computational time
across all tested dimensions when compared with the Genetic and IPSO
methods. As the dimensionality increases, all algorithms exhibit the
expected upward trend in execution time however, the Proposed approach
maintains a steady advantage throughout the entire range. This consistent
performance suggests that the internal structure and optimization
mechanisms of the Proposed algorithm enable more efficient scaling,
resulting in reduced computational cost even in higher-dimensional
settings. Overall, the comparative results highlight the robustness
and efficiency of the Proposed method, confirming its suitability
for problems with increasing complexity.

\begin{figure}
\centering{}\includegraphics[scale=0.25]{Tandem}\caption{Execution Time Comparison of Different Algorithms Across Increasing
Dimensions( Tandem)}
\end{figure}

\end{itemize}

\section{Discussion\label{sec:Discussion}}

The results across all experiments paint a very clear picture: the
choices we make in sampling, weighting, selection, and local search
frequency strongly influence how the algorithm behaves. What becomes
immediately noticeable is that when these components are designed
in a structured and thoughtful way, the algorithm becomes not only
faster but also far more stable.

\textbf{1. Sampling Techniques}

One of the most striking findings comes from the sampling study. Using
k-means to guide the sampling process consistently leads to better
performance than uniform sampling. The MIGRANT(K) configuration almost
always requires fewer function calls and shows much tighter distributions.
In contrast, uniform sampling often spreads the search in less helpful
directions, leading to wasteful evaluations and higher variance. The
pairwise t-tests confirm this: many comparisons involving MIGRANT(K)
are not just significant, but highly significant. 

\textbf{2. Differential weight selection}

The differential weight mechanism also plays a major role. The MIGRANT(T)
method, which adapts the weight using information gathered during
the search, outperforms both the NUMBER and RANDOM strategies. The
fact that all three mechanisms achieve the same success rate makes
this difference even more meaningful: MIGRANT(T) simply gets the job
done with fewer evaluations. This behavior shows that giving the algorithm
a way to â€œlearnâ€ more effectively from its population leads to smarter,
more economical steps through the search space.

\textbf{3. Selection Mechanisms}

The impact of the selection mechanism is equally unmistakable. Tournament
selection clearly strengthens the performance of the MIGRANT strategy.
MIGRANT(T) is the best-performing approach across almost all functions,
whereas MIGRANT(R) is often among the weakest. This suggests that
random selection, despite its simplicity, can easily disrupt the search
by favoring poor candidates. Tournament selection adds a light but
meaningful pressure toward good solutions, which helps guide the algorithm
in a more reliable direction. 

\textbf{4. Local Search Rate }

The local search experiments reveal something quite intuitive: applying
local search too frequently can actually hurt performance. The lowest
rate, 0.005, consistently delivers the best results. Higher rates
not only increase the cost dramatically but also introduce more noise
into the optimization process. The Kruskal--Wallis p-values back
this up, showing clear statistical differences between the rates.
Essentially, a small and controlled dose of local search works best
anything more becomes an unnecessary overhead.

\textbf{5. Comparison with other Algorithms}

When comparing the proposed approach to well-known methods such as
GA, BICCA, LSHADE-SPA, SHADE-ILS, IPSO, WOA, and classical DE, the
difference is visually and statistically unmistakable. The boxplots
show the proposed method forming a compact, low-cost cluster, while
competing algorithms display larger medians and far more variability.
The extremely small p-values (e.g., \textless{} 2.2e\textminus 16)
confirm that these differences are not random. The proposed method
is simply more efficient and more reliable across the board. Putting
everything together, a common theme emerges: the algorithm performs
best when it combines structured exploration (via k-means sampling
and tournament selection) with adaptive exploitation (through MIGRANT
weighting and a low-rate local search). This blend gives the method
a kind of â€œbalanceâ€ that many classical and modern optimizers struggle
to achieve. It doesnâ€™t rush into local optima, but it also doesnâ€™t
waste evaluations wandering aimlessly. 

Overall, the findings show that the proposed approach is not just
marginally better it is consistently stronger, more efficient, and
more stable across a wide range of benchmark functions. The improvements
come from thoughtful design choices rather than brute-force complexity,
making the method both elegant and practical. In many ways, the results
highlight a simple but powerful idea: intelligent structure beats
randomness. When the algorithm is given meaningful guidance through
sampling, weighting, and selection it becomes a far more capable optimizer.

\section{Conclusions\label{sec:Conclusions}}

This work explored large-scale optimization through a systematically
enhanced version of the Differential Evolution algorithm. The improvements
introduced in this study were designed to address two persistent challenges
in high-dimensional optimization: efficiency and stability. Throughout
the experimental analysis, several key components proved crucial to
achieving these goals. A central contribution is the MIGRANT differential
weight mechanism, which consistently outperformed both the classic
NUMBER and RANDOM schemes. Across a wide variety of benchmark functions,
MIGRANT(T) required significantly fewer objective function evaluations
while maintaining identical success rates. This demonstrates that
an adaptive weight strategy can guide the search more intelligently,
reducing unnecessary evaluations and offering clear performance advantages
in complex landscapes. Equally important was the impact of the sampling
strategy. The results showed that k-means sampling (K) provides a
strong structural advantage compared to uniform sampling. Configurations
using MIGRANT(K) repeatedly achieved the lowest evaluation counts
and exhibited far smaller variance. Pairwise statistical tests confirmed
these differences, with several comparisons reaching high or very
high levels of significance. This indicates that exploiting cluster
information during sampling can greatly improve the quality and diversity
of candidate solutions. The study also highlighted the role of the
selection mechanism. Tournament selection consistently strengthened
the algorithmâ€™s performance, enabling MIGRANT(T) to outperform all
Random-based variants. This confirms that introducing even a light
degree of selective pressure yields more reliable search dynamics,
while fully random selection tends to increase noise and computational
cost. Another important outcome relates to the local search rate.
Although local search can refine promising candidates, the experiments
showed that applying it too frequently becomes counterproductive.
The lowest tested rate (0.005) offered the best trade-off, achieving
lower computational cost and greater stability. In contrast, higher
rates (0.03 and 0.05) significantly increased function evaluations
without improving success rates. This emphasizes the need for careful
calibration of exploitation mechanisms in high-dimensional settings.
Finally, when compared to widely used algorithms such as Genetic Algorithms,
BICCA, LSHADE-SPA, SHADE-ILS, IPSO, WOA, and DE, the proposed method
consistently delivered superior performance. Taken together, these
findings highlight the effectiveness of combining structured sampling,
adaptive weighting, selective pressure, and controlled local search
within Differential Evolution. The synergy of these components results
in an optimizer that is not only faster but also remarkably stable
across different problem types and dimensions.

A promising direction for future research is to explore how the proposed
framework could be integrated with other well-established metaheuristic
algorithms. Such a hybridization could leverage the strengths of different
search strategies and potentially lead to more effective optimization
performance. In addition, another interesting avenue is the incorporation
of learning mechanisms such as reinforcement learning or adaptive
parameter-learning techniques so that the algorithm can dynamically
adjust its strategies and parameters based on the characteristics
of the search landscape. Such a self-adaptive system could further
enhance the stability, robustness, and overall efficiency of the optimization
process.

Overall, this study demonstrates that carefully designed modifications
to Differential Evolution can lead to substantial performance gains,
and it sets the foundation for developing even more powerful and general-purpose
optimization algorithms.

\vspace{6pt}


\authorcontributions{G.K., V.C. and I.G.T. conceived of the idea and the methodology,
and G.K. and V.C. implemented the corresponding software. G.K. conducted
the experiments, employing objective functions as test cases, and
provided the comparative experiments. I.G.T. performed the necessary
statistical tests. All authors have read and agreed to the published
version of the manuscript.}

\funding{This research has been financed by the European Union: Next Generation
EU through the Program Greece 2.0 National Recovery and Resilience
Plan, under the call RESEARCH--CREATE--INNOVATE, project name â€œiCREW:
Intelligent small craft simulator for advanced crew training using
Virtual Reality techniquesâ€ (project code: TAEDK-06195).}

\institutionalreview{Not Applicable.}

\informedconsent{Not Applicable.}

\dataavailability{The original contributions presented in this study are included in
the article. Further inquiries can be directed to the corresponding
author.}

\conflictsofinterest{The authors declare no conflicts of interest.}

\appendixtitles{no}

\begin{adjustwidth}{-\extralength}{0cm}{}


\reftitle{References}
\begin{thebibliography}{999}
\bibitem[Author1(year)]{go_math1} AutCarrizosa, E., Molero-RÃ­o, C.,
\& Romero Morales, D. (2021). Mathematical optimization in classification
and regression trees. Top, 29(1), 5-33.

\bibitem{go_math3}Legat, B., Dowson, O., Garcia, J. D., \& Lubin,
M. (2022). MathOptInterface: a data structure for mathematical optimization
problems. INFORMS Journal on Computing, 34(2), 672-689.

\bibitem{go_physics2}Su, H., Zhao, D., Heidari, A. A., Liu, L., Zhang,
X., Mafarja, M., \& Chen, H. (2023). RIME: A physics-based optimization.
Neurocomputing, 532, 183-214.

\bibitem{go_physics3}Stilck FranÃ§a, D., \& Garcia-Patron, R. (2021).
Limitations of optimization algorithms on noisy quantum devices. Nature
Physics, 17(11), 1221-1227.

\bibitem{go_chem1}Zhang, J., \& Glezakou, V. A. (2021). Global optimization
of chemical cluster structures: Methods, applications, and challenges.
International Journal of Quantum Chemistry, 121(7), e26553.

\bibitem{go_chem2}Hu, Y., Zang, Z., Chen, D., Ma, X., Liang, Y.,
You, W., \& Zhang, Z. (2022). Optimization and evaluation of SO2 emissions
based on WRF-Chem and 3DVAR data assimilation. Remote Sensing, 14(1),
220.

\bibitem{go_med2}Kaur, P., \& Singh, R. K. (2023). A review on optimization
techniques for medical image analysis. Concurrency and Computation:
Practice and Experience, 35(1), e7443.

\bibitem{medicine}Houssein, E. H., Hosney, M. E., Mohamed, W. M.,
Ali, A. A., \& Younis, E. M. (2023). Fuzzy-based hunger games search
algorithm for global optimization and feature selection using medical
data. Neural Computing and Applications, 35(7), 5251-5275.

\bibitem{go_bio1}Wang, L., Cao, Q., Zhang, Z., Mirjalili, S., \&
Zhao, W. (2022). Artificial rabbits optimization: A new bio-inspired
meta-heuristic algorithm for solving engineering optimization problems.
Engineering Applications of Artificial Intelligence, 114, 105082.

\bibitem{go_bio2}Hesami, M., \& Jones, A. M. P. (2020). Application
of artificial intelligence models and optimization algorithms in plant
cell and tissue culture. Applied Microbiology and Biotechnology, 104(22),
9449-9485.

\bibitem{go_agri1}Filip, M., Zoubek, T., Bumbalek, R., Cerny, P.,
Batista, C. E., Olsan, P., ... \& Findura, P. (2020). Advanced computational
methods for agriculture machinery movement optimization with applications
in sugarcane production. Agriculture, 10(10), 434.

\bibitem{go_agri2}Akintuyi, O. B. (2024). Adaptive AI in precision
agriculture: a review: investigating the use of self-learning algorithms
in optimizing farm operations based on real-time data. Research Journal
of Multidisciplinary Studies, 7(02), 016-030.

\bibitem{go_econ1}Wang, Y., Ma, Y., Song, F., Ma, Y., Qi, C., Huang,
F., ... \& Zhang, F. (2020). Economic and efficient multi-objective
operation optimization of integrated energy system considering electro-thermal
demand response. Energy, 205, 118022.

\bibitem{go_econ2}Alirahmi, S. M., Mousavi, S. B., Razmi, A. R.,
\& Ahmadi, P. (2021). A comprehensive techno-economic analysis and
multi-criteria optimization of a compressed air energy storage (CAES)
hybridized with solar and desalination units. Energy Conversion and
Management, 236, 114053.

\bibitem{key-23}Tang, K., Li, X., Suganthan, P. N., Yang, Z., \&
Weise, T. (2007). Benchmark functions for the CECâ€™2010 special session
and competition on large-scale global optimization. Nature inspired
computation and applications laboratory, USTC, China, 24, 1-18.

\bibitem{key-24}Li, X., Tang, K., Omidvar, M. N., Yang, Z., Qin,
K., \& China, H. (2013). Benchmark functions for the CEC 2013 special
session and competition on large-scale global optimization. gene,
7(33), 8.

\bibitem{key-25}Molina, D., \& Herrera, F. (2015, May). Iterative
hybridization of DE with local search for the CEC'2015 special session
on large scale global optimization. In 2015 IEEE congress on evolutionary
computation (CEC) (pp. 1974-1978). IEEE.

\bibitem{key-6}Vikhar, P. A. (2016, December). Evolutionary algorithms:
A critical review and its future prospects. In 2016 International
conference on global trends in signal processing, information computing
and communication (ICGTSPICC) (pp. 261-265). IEEE.

\bibitem{key-7}Bartzâ€Beielstein, T., Branke, J., Mehnen, J., \& Mersmann,
O. (2014). Evolutionary algorithms. Wiley Interdisciplinary Reviews:
Data Mining and Knowledge Discovery, 4(3), 178-195.

\bibitem{diffe1}Deng, W., Shang, S., Cai, X., Zhao, H., Song, Y.,
\& Xu, J. (2021). An improved differential evolution algorithm and
its application in optimization problem. Soft Computing, 25, 5277-5298.

\bibitem{diffe2}Pant, M., Zaheer, H., Garcia-Hernandez, L., \& Abraham,
A. (2020). Differential Evolution: A review of more than two decades
of research. Engineering Applications of Artificial Intelligence,
90, 103479.

\bibitem{genetic2}Sohail, A. (2023). Genetic algorithms in the fields
of artificial intelligence and data sciences. Annals of Data Science,
10(4), 1007-1018.

\bibitem{genetic3}Charilogis, V., Tsoulos, I. G., \& Stavrou, V.
N. (2023). An Intelligent Technique for Initial Distribution of Genetic
Algorithms. Axioms, 12(10), 980.

\bibitem{key-8}van Rijn, S., Wang, H., van Leeuwen, M., \& BÃ¤ck,
T. (2016, December). Evolving the structure of evolution strategies.
In 2016 IEEE Symposium Series on Computational Intelligence (SSCI)
(pp. 1-8). IEEE.

\bibitem{key-11}Mezura-Montes, E., \& Coello, C. A. C. (2008). An
empirical study about the usefulness of evolution strategies to solve
constrained optimization problems. International Journal of General
Systems, 37(4), 443-473.

\bibitem{key-12}Fogel, D. B. (1999). An overview of evolutionary
programming. In Evolutionary algorithms (pp. 89-109). New York, NY:
Springer New York.

\bibitem{key-13}Porto, V. W. (2018). Evolutionary programming. In
Evolutionary Computation 1 (pp. 127-140). CRC Press.

\bibitem{key-14}Wang, H., Moon, I., Yang, S., \& Wang, D. (2012).
A memetic particle swarm optimization algorithm for multimodal optimization
problems. Information Sciences, 197, 38-52.

\bibitem{key-15}Preuss, M. (2015). Multimodal optimization by means
of evolutionary algorithms. Berlin/Heidelberg, Germany: Springer International
Publishing.

\bibitem{key-27}Kennedy, J. (2006). Swarm intelligence. In Handbook
of nature-inspired and innovative computing: integrating classical
models with emerging technologies (pp. 187-219). Boston, MA: Springer
US.

\bibitem{key-28}Parpinelli, R. S., \& Lopes, H. S. (2011). New inspirations
in swarm intelligence: a survey. International Journal of Bio-Inspired
Computation, 3(1), 1-16.

\bibitem{pso_major}Shami, T. M., El-Saleh, A. A., Alswaitti, M.,
Al-Tashi, Q., Summakieh, M. A., \& Mirjalili, S. (2022). Particle
swarm optimization: A comprehensive survey. Ieee Access, 10, 10031-10061.

\bibitem{pso1}Gad, A. G. (2022). Particle swarm optimization algorithm
and its applications: a systematic review. Archives of computational
methods in engineering, 29(5), 2531-2561. 

\bibitem{aco1}Rokbani, N., Kumar, R., Abraham, A., Alimi, A. M.,
Long, H. V., Priyadarshini, I., \& Son, L. H. (2021). Bi-heuristic
ant colony optimization-based approaches for traveling salesman problem.
Soft Computing, 25, 3775-3794.

\bibitem{aco2}Wu, L., Huang, X., Cui, J., Liu, C., \& Xiao, W. (2023).
Modified adaptive ant colony optimization algorithm and its application
for solving path planning of mobile robot. Expert Systems with Applications,
215, 119410.

\bibitem{key-18}Karaboga, D., Gorkemli, B., Ozturk, C., \& Karaboga,
N. (2014). A comprehensive survey: artificial bee colony (ABC) algorithm
and applications. Artificial intelligence review, 42, 21-57.

\bibitem{key-17}Karaboga, D., \& Basturk, B. (2007). A powerful and
efficient algorithm for numerical function optimization: artificial
bee colony (ABC) algorithm. Journal of global optimization, 39, 459-471.

\bibitem{key-19}Johari, N. F., Zain, A. M., Noorfa, M. H., \& Udin,
A. (2013). Firefly algorithm for optimization problem. Applied Mechanics
and Materials, 421, 512-517.

\bibitem{key-20}Yang, X. S., \& Slowik, A. (2020). Firefly algorithm.
In Swarm intelligence algorithms (pp. 163-174). CRC Press.

\bibitem{key-21}Yang, X. S., \& Hossein Gandomi, A. (2012). Bat algorithm:
a novel approach for global engineering optimization. Engineering
computations, 29(5), 464-483.

\bibitem{key-22}Yang, X. S. (2011). Bat algorithm for multi-objective
optimisation. International Journal of Bio-Inspired Computation, 3(5),
267-274.

\bibitem{key-1}Storn, R., \& Price, K. (1995). Differential evolution-a
simple and efficient adaptive scheme for global optimization over
continuous spaces. International computer science institute.

\bibitem{key-2-1}Storn, R., \& Price, K. (1997). Differential evolution--a
simple and efficient heuristic for global optimization over continuous
spaces. Journal of global optimization, 11, 341-359.

\bibitem{key-1-1}Bai, Y., Wu, X., \& Xia, A. (2021). An enhanced
multiâ€objective differential evolution algorithm for dynamic environmental
economic dispatch of power system with wind power. Energy Science
\& Engineering, 9(3), 316-329.

\bibitem{key-2}Penenko, A. V., Konopleva, V. S., \& Penenko, V. V.
(2022, May). Inverse modeling of atmospheric chemistry with a differential
evolution solver: Inverse problem and Data assimilation. In IOP Conference
Series: Earth and Environmental Science (Vol. 1023, No. 1, p. 012015).
IOP Publishing.

\bibitem{key-3}Babanezhad, M., Behroyan, I., Nakhjiri, A. T., Marjani,
A., Rezakazemi, M., \& Shirazian, S. (2020). High-performance hybrid
modeling chemical reactors using differential evolution based fuzzy
inference system. Scientific Reports, 10(1), 21304.

\bibitem{key-4}Liu, L., Zhao, D., Yu, F., Heidari, A. A., Ru, J.,
Chen, H., ... \& Pan, Z. (2021). Performance optimization of differential
evolution with slime mould algorithm for multilevel breast cancer
image segmentation. Computers in Biology and Medicine, 138, 104910

\bibitem{key-5}Balasubramanian, K., \& Ananthamoorthy, N. P. (2021).
Improved adaptive neuro-fuzzy inference system based on modified glowworm
swarm and differential evolution optimization algorithm for medical
diagnosis. Neural Computing and Applications, 33, 7649-7660.

\bibitem{de_symmetry1}Li, Y. H., Wang, J. Q., Wang, X. J., Zhao,
Y. L., Lu, X. H., \& Liu, D. L. (2017). Community detection based
on differential evolution using social spider optimization. Symmetry,
9(9), 183.

\bibitem{large_co}Sofge, D., De Jong, K., \& Schultz, A. (2002, May).
A blended population approach to cooperative coevolution for decomposition
of complex problems. In Proceedings of the 2002 Congress on Evolutionary
Computation. CEC'02 (Cat. No. 02TH8600) (Vol. 1, pp. 413-418). IEEE.

\bibitem{large_pso}Van den Bergh, F., \& Engelbrecht, A. P. (2004).
A cooperative approach to particle swarm optimization. IEEE transactions
on evolutionary computation, 8(3), 225-239.

\bibitem{large_memetic}Gao, Y., \& Wang, Y. J. (2007, August). A
memetic differential evolutionary algorithm for high dimensional functions'
optimization. In Third International Conference on Natural Computation
(ICNC 2007) (Vol. 4, pp. 188-192). IEEE.

\bibitem{key-40}Chen, M., \& Tan, Y. (2023). SF-FWA: A self-adaptive
fast fireworks algorithm for effective large-scale optimization. Swarm
and Evolutionary Computation, 80, 101314.

\bibitem{key-41}Sun, Y., \& Cao, H. (2024). An agent-assisted heterogeneous
learning swarm optimizer for large-scale optimization. Swarm and Evolutionary
Computation, 89, 101627.

\bibitem{key-42}Wang, X., Wang, F., He, Q., \& Guo, Y. (2024). A
multi-swarm optimizer with a reinforcement learning mechanism for
large-scale optimization. Swarm and Evolutionary Computation, 86,
101486.

\bibitem{key-43}Li, J. Y., Zhan, Z. H., Tan, K. C., \& Zhang, J.
(2022). Dual differential grouping: A more general decomposition method
for large-scale optimization. IEEE Transactions on Cybernetics, 53(6),
3624-3638.

\bibitem{key-44}Li, J. Y., Du, K. J., Zhan, Z. H., Wang, H., \& Zhang,
J. (2022). Distributed differential evolution with adaptive resource
allocation. IEEE transactions on cybernetics, 53(5), 2791-2804.

\bibitem[Author1(year)]{de_char} Charilogis, V., Tsoulos, I. G.,
Tzallas, A., \& Karvounis, E. (2022). Modifications for the differential
evolution algorithm. Symmetry, 14(3), 447.

\bibitem{de_migrant}Cheng, J., Zhang, G., \& Neri, F. (2013). Enhancing
distributed differential evolution with multicultural migration for
global numerical optimization. Information Sciences, 247, 72-93.

\bibitem{powell}Powell, M. J. D. (1989). A tolerant algorithm for
linearly constrained optimization calculations. Mathematical Programming,
45, 547-566.

\bibitem{MacQueen}MacQueen, J. (1967, June). Some methods for classification
and analysis of multivariate observations. In Proceedings of the fifth
Berkeley symposium on mathematical statistics and probability (Vol.
1, No. 14, pp. 281-297).

\bibitem{key-1-2}Sasmal, B., Hussien, A. G., Das, A., \& Dhal, K.
G. (2023). A comprehensive survey on aquila optimizer. Archives of
Computational Methods in Engineering, 30(7), 4449-4476.

\bibitem{key-2-2}Abualigah, L., Diabat, A., Mirjalili, S., Abd Elaziz,
M., \& Gandomi, A. H. (2021). The arithmetic optimization algorithm.
Computer methods in applied mechanics and engineering, 376, 113609.

\bibitem{key-4-1}SS, V. C. (2016). Smell detection agent based optimization
algorithm. Journal of The Institution of Engineers (India): Series
B, 97, 431-436.

\bibitem{kmeans1}Li, Y., \& Wu, H. (2012). A clustering method based
on K-means algorithm. Physics Procedia, 25, 1104-1109.

\bibitem{kmeans2}Arora, P., \& Varshney, S. (2016). Analysis of k-means
and k-medoids algorithm for big data. Procedia Computer Science, 78,
507-512.

\bibitem{testfunc1}Ali, M. M., \& Kaelo, P. (2008). Improved particle
swarm algorithms for global optimization. Applied mathematics and
computation, 196(2), 578-593.

\bibitem{testfunc2}Koyuncu, H., \& Ceylan, R. (2019). A PSO based
approach: Scout particle swarm algorithm for continuous global optimization
problems. Journal of Computational Design and Engineering, 6(2), 129-142.

\bibitem{testfunc2-1}Siarry, P., Berthiau, G., Durdin, F., \& Haussy,
J. (1997). Enhanced simulated annealing for globally minimizing functions
of many-continuous variables. ACM Transactions on Mathematical Software
(TOMS), 23(2), 209-228.

\bibitem{testfunc4}LaTorre, A., Molina, D., Osaba, E., Poyatos, J.,
Del Ser, J., \& Herrera, F. (2021). A prescription of methodological
guidelines for comparing bio-inspired optimization algorithms. Swarm
and Evolutionary Computation, 67, 100973.

\bibitem[(2025)]{OPTIMUS}Tsoulos, I.G., Charilogis, V., Kyrou, G.,
Stavrou, V.N. \& Tzallas,A. (2025). OPTIMUS: A Multidimensional Global
Optimization Package. Journal of Open Source Software, 10(108), 7584.
Doi: https://doi.org/10.21105/joss.07584

\bibitem{key-32}Ge, H., Zhao, M., Hou, Y., Kai, Z., Sun, L., Tan,
G., ... \& Chen, C. P. (2020). Bi-space interactive cooperative coevolutionary
algorithm for large scale black-box optimization. Applied Soft Computing,
97, 106798.

\bibitem{key-30}Hadi, A. A., Mohamed, A. W., \& Jambi, K. M. (2019).
LSHADE-SPA memetic framework for solving large-scale optimization
problems. Complex \& Intelligent Systems, 5(1), 25-40.

\bibitem{key-31}Molina, D., LaTorre, A., \& Herrera, F. (2018, July).
SHADE with iterative local search for large-scale global optimization.
In 2018 IEEE congress on evolutionary computation (CEC) (pp. 1-8).
IEEE.

\bibitem[(2023)]{WOA}Nadimi-Shahraki, M. H., Zamani, H., Asghari
Varzaneh, Z., \& Mirjalili, S. (2023). A systematic review of the
whale optimization algorithm: theoretical foundation, improvements,
and hybridizations. Archives of Computational Methods in Engineering,
30(7), 4113-4159.

\bibitem{WOA1}Brodzicki, A., Piekarski, M., \& Jaworek-Korjakowska,
J. (2021). The whale optimization algorithm approach for deep neural
networks. Sensors, 21(23), 8003

\bibitem{key-41}Luo, B., Su, X., Zhang, S., Yan, P., Liu, J., \&
Li, R. (2025). Analysis of a novel gas cycle cooler with large temperature
glide for space cooling.Â Energy,Â 326, 136294.

\bibitem{key-42}Keerthika, R., Niranjan, S. P., \& Komala Durga,
B. (2025). A Survey on the tandem queueing models.Â Scope,Â 14, 134-148.

\end{thebibliography}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% for journal Sci
%\reviewreports{\\
%Reviewer 1 comments and authors' response\\
%Reviewer 2 comments and authors' response\\
%Reviewer 3 comments and authors' response
%}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\PublishersNote{}

\end{adjustwidth}{}
\end{document}
