%% LyX 2.4.3 created this file.  For more info, see https://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[journal,article,submit,pdftex,moreauthors]{Definitions/mdpi}
\usepackage[utf8]{inputenc}
\usepackage{float}
\usepackage{url}
\usepackage{bm}
\usepackage{varwidth}
\usepackage{amsmath}
\usepackage{graphicx}

\makeatletter

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% LyX specific LaTeX commands.

\Title{A novel Magnificent Frigatebird Optimization algorithm with proposed
movement strategies for enhanced Global Search}

\TitleCitation{A Novel Magnificent Frigatebird Optimization algorithm with proposed
movement strategies for enhanced Global Search}

\Author{Glykeria Kyrou$^{1}$, Vasileios Charilogis$^{2}$ and Ioannis G.
Tsoulos$^{3,*}$}

\AuthorNames{Kyrou, G., Charilogis, V., \textbackslash\& Tsoulos, I.G. }

\AuthorNames{Glykeria Kyrou, Vasileios Charilogis and Ioannis G. Tsoulos }

\AuthorCitation{Kyrou, G.; Charilogis, V.; Tsoulos, I.G.}


\address{$^{1}$\quad{}Department of Informatics and Telecommunications,
University of Ioannina, 47150 Kostaki Artas, Greece; g.kyrou@uoi.gr\\
$^{2}$\quad{}Department of Informatics and Telecommunications, University
of Ioannina, Greece; v.charilog@uoi.gr\\
$^{3}\quad$Department of Informatics and Telecommunications, University
of Ioannina, 47150 Kostaki Artas, Greece;itsoulos@uoi.gr}


\corres{Correspondence: itsoulos@uoi.gr}


\abstract{Global optimization is a fundamental tool for addressing complex
and nonlinear problems across scientific and technological domains.
The primary objective of this work is to enhance the efficiency, stability,
and convergence speed of the Magnificent Frigatebird Optimization
(MFO) algorithm by introducing new strategies that strengthen both
global exploration and local exploitation. To this end, we propose
an improved version of MFO that incorporates three novel movement
strategies (aggressive, conservative, and mixed), a BFGS-based local
search procedure for more accurate solution refinement, and a dynamic
termination criterion capable of detecting stagnation and reducing
unnecessary function evaluations. The algorithm is extensively evaluated
on a diverse set of benchmark functions, demonstrating substantially
lower computational cost and higher reliability compared to classical
evolutionary and swarm-based methods. The results confirm the effectiveness
of the proposed modifications and highlight the potential of the enhanced
MFO to be applied to demanding real-world optimization problems.}


\keyword{Global optimization; evolutionary methods; stochastic methods}

%% Because html converters don't know tabularnewline
\providecommand{\tabularnewline}{\\}
%% Variable width box for table cells
\newenvironment{cellvarwidth}[1][t]
    {\begin{varwidth}[#1]{\linewidth}}
    {\@finalstrut\@arstrutbox\end{varwidth}}
%% A simple dot to overcome graphicx limitations
\newcommand{\lyxdot}{.}

\floatstyle{ruled}
\newfloat{algorithm}{tbp}{loa}
\providecommand{\algorithmname}{Algorithm}
\floatname{algorithm}{\protect\algorithmname}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
%  LaTeX support: latex@mdpi.com 
%  For support, please attach all files needed for compiling as well as the log file, and specify your operating system, LaTeX version, and LaTeX editor.

%=================================================================
%\documentclass[preprints,article,submit,pdftex,moreauthors]{Definitions/mdpi} 
% For posting an early version of this manuscript as a preprint, you may use "preprints" as the journal. Changing "submit" to "accept" before posting will remove line numbers.

% Below journals will use APA reference format:
% admsci, aichem, behavsci, businesses, econometrics, economies, education, ejihpe, famsci, games, humans, ijcs, ijfs, journalmedia, jrfm, languages, psycholint, publications, tourismhosp, youth

% Below journals will use Chicago reference format:
% arts, genealogy, histories, humanities, jintelligence, laws, literature, religions, risks, socsci

%--------------------
% Class Options:
%--------------------
%----------
% journal
%----------
% Choose between the following MDPI journals:
% accountaudit, acoustics, actuators, addictions, adhesives, admsci, adolescents, aerobiology, aerospace, agriculture, agriengineering, agrochemicals, agronomy, ai, air, algorithms, allergies, alloys, amh, analytica, analytics, anatomia, anesthres, animals, antibiotics, antibodies, antioxidants, applbiosci, appliedchem, appliedmath, appliedphys, applmech, applmicrobiol, applnano, applsci, aquacj, architecture, arm, arthropoda, arts, asc, asi, astronomy, atmosphere, atoms, audiolres, automation, axioms, bacteria, batteries, bdcc, behavsci, beverages, biochem, bioengineering, biologics, biology, biomass, biomechanics, biomed, biomedicines, biomedinformatics, biomimetics, biomolecules, biophysica, biosensors, biosphere, biotech, birds, blockchains, bloods, blsf, brainsci, breath, buildings, businesses, cancers, carbon, cardiogenetics, catalysts, cells, ceramics, challenges, chemengineering, chemistry, chemosensors, chemproc, children, chips, cimb, civileng, cleantechnol, climate, clinbioenerg, clinpract, clockssleep, cmd, cmtr, coasts, coatings, colloids, colorants, commodities, complications, compounds, computation, computers, condensedmatter, conservation, constrmater, cosmetics, covid, crops, cryo, cryptography, crystals, csmf, ctn, curroncol, cyber, dairy, data, ddc, dentistry, dermato, dermatopathology, designs, devices, diabetology, diagnostics, dietetics, digital, disabilities, diseases, diversity, dna, drones, dynamics, earth, ebj, ecm, ecologies, econometrics, economies, education, eesp, ejihpe, electricity, electrochem, electronicmat, electronics, encyclopedia, endocrines, energies, eng, engproc, ent, entomology, entropy, environments, epidemiologia, epigenomes, esa, est, famsci, fermentation, fibers, fintech, fire, fishes, fluids, foods, forecasting, forensicsci, forests, fossstud, foundations, fractalfract, fuels, future, futureinternet, futureparasites, futurepharmacol, futurephys, futuretransp, galaxies, games, gases, gastroent, gastrointestdisord, gastronomy, gels, genealogy, genes, geographies, geohazards, geomatics, geometry, geosciences, geotechnics, geriatrics, glacies, grasses, greenhealth, gucdd, hardware, hazardousmatters, healthcare, hearts, hemato, hematolrep, heritage, higheredu, highthroughput, histories, horticulturae, hospitals, humanities, humans, hydrobiology, hydrogen, hydrology, hygiene, idr, iic, ijerph, ijfs, ijgi, ijmd, ijms, ijns, ijpb, ijt, ijtm, ijtpp, ime, immuno, informatics, information, infrastructures, inorganics, insects, instruments, inventions, iot, j, jal, jcdd, jcm, jcp, jcs, jcto, jdad, jdb, jeta, jfb, jfmk, jimaging, jintelligence, jlpea, jmahp, jmmp, jmms, jmp, jmse, jne, jnt, jof, joitmc, joma, jop, jor, journalmedia, jox, jpbi, jpm, jrfm, jsan, jtaer, jvd, jzbg, kidney, kidneydial, kinasesphosphatases, knowledge, labmed, laboratories, land, languages, laws, life, lights, limnolrev, lipidology, liquids, literature, livers, logics, logistics, lubricants, lymphatics, machines, macromol, magnetism, magnetochemistry, make, marinedrugs, materials, materproc, mathematics, mca, measurements, medicina, medicines, medsci, membranes, merits, metabolites, metals, meteorology, methane, metrics, metrology, micro, microarrays, microbiolres, microelectronics, micromachines, microorganisms, microplastics, microwave, minerals, mining, mmphys, modelling, molbank, molecules, mps, msf, mti, multimedia, muscles, nanoenergyadv, nanomanufacturing, nanomaterials, ncrna, ndt, network, neuroglia, neurolint, neurosci, nitrogen, notspecified, nursrep, nutraceuticals, nutrients, obesities, oceans, ohbm, onco, oncopathology, optics, oral, organics, organoids, osteology, oxygen, parasites, parasitologia, particles, pathogens, pathophysiology, pediatrrep, pets, pharmaceuticals, pharmaceutics, pharmacoepidemiology, pharmacy, philosophies, photochem, photonics, phycology, physchem, physics, physiologia, plants, plasma, platforms, pollutants, polymers, polysaccharides, populations, poultry, powders, preprints, proceedings, processes, prosthesis, proteomes, psf, psych, psychiatryint, psychoactives, psycholint, publications, purification, quantumrep, quaternary, qubs, radiation, reactions, realestate, receptors, recycling, regeneration, religions, remotesensing, reports, reprodmed, resources, rheumato, risks, robotics, rsee, ruminants, safety, sci, scipharm, sclerosis, seeds, sensors, separations, sexes, signals, sinusitis, siuj, skins, smartcities, sna, societies, socsci, software, soilsystems, solar, solids, spectroscj, sports, standards, stats, std, stresses, surfaces, surgeries, suschem, sustainability, symmetry, synbio, systems, tae, targets, taxonomy, technologies, telecom, test, textiles, thalassrep, therapeutics, thermo, timespace, tomography, tourismhosp, toxics, toxins, transplantology, transportation, traumacare, traumas, tropicalmed, universe, urbansci, uro, vaccines, vehicles, venereology, vetsci, vibration, virtualworlds, viruses, vision, waste, water, wem, wevj, wild, wind, women, world, youth, zoonoticdis

%---------
% article
%---------
% The default type of manuscript is "article", but can be replaced by: 
% abstract, addendum, article, book, bookreview, briefreport, casereport, comment, commentary, communication, conferenceproceedings, correction, conferencereport, entry, expressionofconcern, extendedabstract, datadescriptor, editorial, essay, erratum, hypothesis, interestingimage, obituary, opinion, projectreport, reply, retraction, review, perspective, protocol, shortnote, studyprotocol, systematicreview, supfile, technicalnote, viewpoint, guidelines, registeredreport, tutorial
% supfile = supplementary materials

%----------
% submit
%----------
% The class option "submit" will be changed to "accept" by the Editorial Office when the paper is accepted. This will only make changes to the frontpage (e.g., the logo of the journal will get visible), the headings, and the copyright information. Also, line numbering will be removed. Journal info and pagination for accepted papers will also be assigned by the Editorial Office.

%------------------
% moreauthors
%------------------
% If there is only one author the class option oneauthor should be used. Otherwise use the class option moreauthors.

%---------
% pdftex
%---------
% The option pdftex is for use with pdfLaTeX. If eps figures are used, remove the option pdftex and use LaTeX and dvi2pdf.

%=================================================================
% MDPI internal commands - do not modify
\firstpage{1} 
\setcounter{page}{\@firstpage}
\pubvolume{1}
\issuenum{1}
\articlenumber{0}
\pubyear{2025}
\copyrightyear{2025}
%\externaleditor{Firstname Lastname} % More than 1 editor, please add `` and '' before the last editor name
\datereceived{}
\daterevised{ } % Comment out if no revised date
\dateaccepted{}
\datepublished{}
%\datecorrected{} % For corrected papers include a "Corrected: XXX" date in the original paper.
%\dateretracted{} % For retracted papers include a "RETRACTED: XXX" date in the original paper.
\hreflink{https://doi.org/} % If needed use \linebreak
%\doinum{}
%\pdfoutput=1 % Uncommented for upload to arXiv.org
%\CorrStatement{yes}  % For updates
%\longauthorlist{yes} % For many authors that exceed the left citation part

%=================================================================
% Add packages and commands here. The following packages are loaded in our class file: fontenc, inputenc, calc, indentfirst, fancyhdr, graphicx, epstopdf, lastpage, ifthen, lineno, float, amsmath, setspace, enumitem, mathpazo, booktabs, titlesec, etoolbox, tabto, xcolor, soul, multirow, microtype, tikz, totcount, changepage, attrib, upgreek, cleveref, amsthm, hyphenat, natbib, hyperref, footmisc, url, geometry, newfloat, caption

%=================================================================
%% Please use the following mathematics environments: Theorem, Lemma, Corollary, Proposition, Characterization, Property, Problem, Example, ExamplesandDefinitions, Hypothesis, Remark, Definition, Notation, Assumption
%% For proofs, please use the proof environment (the amsthm package is loaded by the MDPI class).

%=================================================================
% The fields PACS, MSC, and JEL may be left empty or commented out if not applicable
%\PACS{J0101}
%\MSC{}
%\JEL{}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Only for the journal Diversity
%\LSID{\url{http://}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Only for the journal Applied Sciences:
%\featuredapplication{Authors are encouraged to provide a concise description of the specific application or a potential application of the work. This section is not mandatory.}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Only for the journal Data:
%\dataset{DOI number or link to the deposited data set in cases where the data set is published or set to be published separately. If the data set is submitted and will be published as a supplement to this paper in the journal Data, this field will be filled by the editors of the journal. In this case, please make sure to submit the data set as a supplement when entering your manuscript into our manuscript editorial system.}

%\datasetlicense{license under which the data set is made available (CC0, CC-BY, CC-BY-SA, CC-BY-NC, etc.)}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Only for the journal BioTech, Fishes, Neuroimaging and Toxins
%\keycontribution{The breakthroughs or highlights of the manuscript. Authors can write one or two sentences to describe the most important part of the paper.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Only for the journal Encyclopedia
%\encyclopediadef{Instead of the abstract}
%\entrylink{The Link to this entry published on the encyclopedia platform.}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Different journals have different requirements. Please check the specific journal guidelines in the "Instructions for Authors" on the journal's official website.
 
%\addhighlights{yes}
%\renewcommand{\addhighlights}{%
%
%\noindent The goal is to increase the discoverability and readability of the article via search engines and other scholars. Highlights should not be a copy of the abstract, but a simple text allowing the reader to quickly and simplified find out what the article is about and what can be cited from it. Each of these parts should be devoted up to 2 bullet points.\vspace{3pt}\\
%\textbf{What are the main findings?}
% \begin{itemize}[labelsep=2.5mm,topsep=-3pt]
% \item First bullet.
% \item Second bullet.
% \end{itemize}\vspace{3pt}
%\textbf{What is the implication of the main finding?}
% \begin{itemize}[labelsep=2.5mm,topsep=-3pt]
% \item First bullet.
% \item Second bullet.
% \end{itemize}
%}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\makeatother

\begin{document}
\maketitle

\section{Introduction}

The primary objective of global optimization is to locate the global
minimum of a continuous and multidimensional function, in such a way
as to ensure complete exploration of the search space. Global optimization
aims to examine the entire problem domain in order to find the lowest
possible value that is feasible. This procedure is applied to complex
functions which usually include multiple local minima, making it difficult
to identify the global minimum. Global optimization includes techniques
that ensure that local optima are avoided while focusing on maximizing
the accuracy and efficiency of the search process. The objective is
to find the lowest point through systematic exploration of the entire
domain of the function$f:S\rightarrow R,S\subset R^{n}$ and it is
defined as follows:

\begin{equation}
x^{*}=\mbox{arg}\min_{x\in S}f(x)\label{eq:eq1-1}
\end{equation}
where the set $S$ is defined as follows: 
\[
S=\left[a_{1},b_{1}\right]\times\left[a_{2},b_{2}\right]\times\ldots\left[a_{n},b_{n}\right]
\]

Global optimization methods aim to avoid premature convergence to
local minima while maintaining a balance between accurate exploitation
and wide-range exploration of the search space. Owing to their ability
to cope with highly complex energy landscapes, global optimization
techniques have become a rapidly evolving field, with numerous studies
addressing their theoretical foundations and practical applications\citep{go1,go2,go3}.
During the past decade, bio-inspired metaheuristics have emerged as
powerful alternatives to classical optimization algorithms, offering
flexible mechanisms for balancing exploration and exploitation. Among
these approaches, the Magnificent Frigatebird Optimization (MFO) algorithm
is notable for its modeling of the kleptoparasitic foraging behavior
of frigatebirds, which alternate between broad exploratory movements
and targeted dives toward promising prey. This behavioral inspiration
provides a computational framework capable of navigating difficult
search landscapes through adaptive and dynamic movement patterns.
The present work introduces an improved version of the MFO algorithm
with the aim of increasing efficiency, accelerating convergence, and
enhancing stability. 

The proposed enhancements include three new movement strategies: \textbf{aggressive,
conservative, and mixed }designed to reinforce exploration and exploitation
in a controlled manner. Additionally, a \textbf{BFGS-based local search}
mechanism\citep{Powell}is incorporated to refine candidate solutions
and improve convergence precision. Finally, \textbf{a stochastic early-termination
strategy}\citep{stopping rule2} is integrated to detect stagnation
by monitoring the best objective value across iterations; if no improvement
is observed for a predefined number of generations, the algorithm
terminates early to avoid unnecessary evaluations.

The remains of this paper are divided as follows: in section\ref{sec:Literature review}
presented the related literature and positions the proposed method
within the current state of the art in section\ref{sec:Materials-and-Methods}
the proposed MFO algorithm, a flowchart with a detailed description
is presented, in section\ref{sec:Results} of the test functions used
in the experiments as well as the related experiments are presented.
In the \ref{sec:Discussion}section, there is a brief discussion of
the results obtained from the experiments. In section \ref{sec:Conclusions}some
conclusions and directions for future improvements are discussed.

\section{Literature review\label{sec:Literature review}}

Global optimization has been extensively examined in a wide range
of scientific and technological fields including mathematics\citep{go_math1,go_math3},
physics\citep{go_physics2,go_physics3}, chemistry\citep{go_chem1,go_chem2},
medicine\citep{go_med2,medicine}, and economics\citep{key-1,go_econ2}due
to its critical role in solving complex nonlinear problems. Over the
years, two major methodological families have been established: deterministic
and stochastic global optimization methods\citep{key-2,go_determ3}
and stochastic methods\citep{stohastic,stohastic1}. Deterministic
approaches rely on rigorous mathematical principles and provide theoretical
guarantees for identifying the global minimum. Interval analysis techniques
constitute a representative paradigm, progressively subdividing the
search space to isolate promising regions. Although such methods offer
reliable accuracy, their computational cost scales unfavorably with
dimensionality, restricting their applicability primarily to small
or medium scale problems. In contrast, stochastic global optimization
methods do not guarantee convergence to the global optimum; however,
they demonstrate considerable flexibility in highly nonlinear, multimodal,
or noisy environments. Classical examples include Controlled Random
Search \citep{crs1,crs2}, Simulated Annealing \citep{simann_major,simann1},
and Multistart \citep{mult_minfinder,mult_cfo}. Despite their effectiveness,
these approaches may encounter difficulties in problems characterized
by strong nonlinearity, hard constraints, or intricate search landscapes.

A significant subset of stochastic methods is evolutionary computation,
which draws inspiration from principles of biological evolution such
as natural selection, genetic variation, and adaptation. Evolutionary
approaches including Genetic Algorithms \citep{key-3,genetic3}, Evolution
Strategies \citep{key-9,key-10}, Genetic Programming \citep{key-11,key-12},
and Differential Evolution (DE) \citep{diffe1,diffe2} have been widely
adopted for highly complex optimization tasks. Parallel to these methods,
swarm intelligence algorithms capitalize on collective behaviors observed
in nature. Particle Swarm Optimization (PSO) \citep{pso_major,pso1}
and Ant Colony Optimization (ACO)\citep{aco1,aco2}are prominent examples,
enabling efficient exploration. In recent years, a diverse family
of bio-inspired algorithms has emerged, modeling behaviors of various
animal species and ecological systems. Methods such as the Magnificent
Frigatebird Optimization (MFO\textbf{)}\citep{key-16},\textbf{ }the
Grey Wolf Optimizer (GWO)\citep{key-25}, and Whale Optimization Algorithm
(WOA)\citep{key-22} exemplify this trend, each introducing unique
exploration--exploitation dynamics derived from natural processes.
Within this continuously expanding landscape, the MFO algorithm differentiates
itself through the modeling of the kleptoparasitic foraging strategy
of frigatebirds, whose ability to alternate between broad exploratory
flights and targeted dives provides an adaptive and dynamic optimization
framework.

Although many nature-inspired optimization algorithms have been proposed
in the literature, new methodological variations continue to be introduced,
each aiming to refine specific aspects of the search process. Within
this broader context, the present work examines an enhanced version
of the MFO algorithm that integrates adaptive movement strategies,
a BFGS-based local improvement mechanism, and an early-termination
criterion. The combination of these components provides an alternative
formulation of the search dynamics within the MFO framework and offers
a complementary contribution to the ongoing development of bio-inspired
optimization techniques.

\section{Materials and Methods\label{sec:Materials-and-Methods}}

\subsection{The main steps of the algorithm}

The Magnificent Frigatebird Optimization (MFO) algorithm is a biology-inspired
metaheuristic that models the cooperative foraging behavior of magnificent
frigatebirds in marine environments. Each computational agent represents
a frigatebird that explores the search space in pursuit of high-quality
food sources, i.e., optimal solutions to the given problem. The algorithm
begins by uniformly initializing a population within the feasible
domain, thus ensuring adequate diversity. After evaluating the objective
function, the best individual is recorded as the initial global optimum
and serves as a reference throughout the search. At the beginning
of each generation, the population is re-evaluated and a stochastically
selected subset of individuals undergoes local refinement via the
BFGS method. This hybridization endows the algorithm with a dual character,
combining stochastic global exploration with efficient gradient-based
exploitation in smooth regions of the landscape. The updated population
is then rank-ordered according to fitness, enabling structured propagation
of information from high-quality to lower-quality solutions. The core
optimization dynamics are executed in Phase A (Movement Phase). During
this stage, each individual selects a superior peer as a guide and
updates his position using one of three movement strategies: aggressive,
conservative, or mixed. These strategies correspond to distinct update
mechanisms, each imposing different search characteristics. The aggressive
strategy introduces strong stochastic perturbations and large displacements,
promoting global exploration and allowing the algorithm to escape
deceptive or highly multimodal regions. In contrast, the conservative
strategy generates small, directed steps toward promising solutions,
thereby enhancing local exploitation and enabling fine-grained refinement.
The mixed strategy probabilistically alternates between the two modes,
providing an adaptive mechanism that dynamically balances exploration
and exploitation based on the landscape structure. In all strategies,
a newly generated position is accepted only if it improves upon the
current solution. Phase B (Dive Phase) introduces an additional exploitation-oriented
update, wherein each agent performs a movement biased toward the current
global best. This mechanism reinforces the intensification process
and accelerates convergence once the population has entered a promising
basin. The interplay between Phases A and B yields a search dynamic
capable of performing broad exploration in the early generations and
highly targeted exploitation in the later stages. At the end of each
generation, the global best solution is updated, and whenever the
activation criteria are satisfied, an additional BFGS refinement is
applied to increase accuracy. A dynamic termination criterion monitors
the improvement rate of the global optimum over a predefined number
of generations. If the progress falls below a threshold or the maximum
number of generations is reached, the algorithm terminates and returns
the best solution.

Overall, MFO forms an adaptive optimization framework in which the
aggressive and mixed strategies sustain adequate exploration, while
the conservative updates, Dive Phase, and selectively applied BFGS
refinement reinforce exploitation and ensure precise final convergence.
The synergy among these mechanisms renders the enhanced MFO particularly
effective in complex landscapes characterized by ruggedness, high
multimodality, and strong nonlinearity. The steps of the Magnificent
Frigatebird Optimization (MFO) algorithm are presented in the following
Algorithm. \ref{alg:Magnificent-Frigatebird-Optimiza}

\begin{algorithm}
\caption{Magnificent Frigatebird Optimization (MFO) algorithm\label{alg:Magnificent-Frigatebird-Optimiza}}

{\scriptsize\textbf{INPUT}}{\scriptsize\par}

{\scriptsize - $N$: population size. }{\scriptsize\par}

{\scriptsize - $G_{\mbox{max}}$: max generations.}{\scriptsize\par}

{\scriptsize - $T$: inner iterations per generation.}{\scriptsize\par}

{\scriptsize - $\mbox{strategyMode\ensuremath{\in}\ensuremath{\left\{  \mbox{aggressive},\mbox{conservative,\mbox{mixed}}\right\} } }$.}{\scriptsize\par}

{\scriptsize - $p_{l}\in[0,1]$ as the local search rate.}{\scriptsize\par}

{\scriptsize\textbf{OUTPUT}}{\scriptsize\par}

{\scriptsize -$\left(x_{\mbox{best}},f_{\mbox{best}}\right)$}{\scriptsize\par}

{\scriptsize\textbf{INITIALIZATION}}{\scriptsize\par}

{\scriptsize - }{\scriptsize\textbf{Sample}}{\scriptsize{} initial population
$P$ of size $N$ uniformly in $S$ and evaluate $f$}{\scriptsize\par}

{\scriptsize - }{\scriptsize\textbf{Obtain}}{\scriptsize{} the current
best $\left(x_{\mbox{best}},f_{\mbox{best}}\right)$ from $P$}{\scriptsize\par}

{\scriptsize - }{\scriptsize\textbf{Set}}{\scriptsize{} $k=0$, the generation
counter.}{\scriptsize\par}

{\scriptsize\textbf{MFO main pseudocode }}{\scriptsize\par}

{\scriptsize 01}{\scriptsize\textbf{ while}}{\scriptsize{} $k<G_{\mbox{max}}$
and the termination rule described in subsection \ref{subsec:The-used-termination}
does not hold }{\scriptsize\textbf{do}}{\scriptsize\par}

{\scriptsize 02 \hspace{0.5cm}}{\scriptsize\textbf{Set}}{\scriptsize{}
$k=k+1$}{\scriptsize\par}

{\scriptsize 03 \hspace{0.5cm}}{\scriptsize\textbf{for}}{\scriptsize{}
$i=1,\ldots,N$ }{\scriptsize\textbf{do}}{\scriptsize\par}

{\scriptsize 04 \hspace{0.5cm}\hspace{0.5cm}$f_{i}=f\left(P_{i}\right)$}{\scriptsize\par}

{\scriptsize 05 \hspace{0.5cm}\hspace{0.5cm}Pick a random number
$r\in[0,1]$.}{\scriptsize\par}

{\scriptsize 06 \hspace{0.5cm}\hspace{0.5cm}}{\scriptsize\textbf{If}}{\scriptsize{}
$r\le p_{l}$ }{\scriptsize\textbf{then}}{\scriptsize{} $f_{i}=\mbox{LS}(P_{i})$,
Where $\mbox{LS}(x)$ is a local optimization procedure. }{\scriptsize\par}

{\scriptsize 07 \hspace{0.5cm}}{\scriptsize\textbf{endfor}}{\scriptsize\par}

{\scriptsize 08\hspace{0.5cm}}{\scriptsize\textbf{ Obtain}}{\scriptsize{}
the current best $\left(x_{\mbox{best}},f_{\mbox{best}}\right)$ from
$P$}{\scriptsize\par}

{\scriptsize 09 \hspace{0.5cm}sort $P$ according to the fitness values.}{\scriptsize\par}

{\scriptsize 10 \hspace{0.5cm}}{\scriptsize\textbf{for}}{\scriptsize{}
$t=1,\ldots,T$}{\scriptsize\textbf{ do}}{\scriptsize\par}

{\scriptsize 11 \hspace{0.5cm}\hspace{0.5cm}}{\scriptsize\textbf{for}}{\scriptsize{}
$i=1,\ldots,N$ }{\scriptsize\textbf{do}}{\scriptsize\par}

{\scriptsize 12 \hspace{0.5cm}\hspace{0.5cm}\hspace{0.5cm}// Phase
A: move using better “seabird” or best, depending on strategyMode}{\scriptsize\par}

{\scriptsize 13 \hspace{0.5cm}\hspace{0.5cm}\hspace{0.5cm}}{\scriptsize\textbf{Set
}}{\scriptsize$C=\left\{ P_{k}:k\ne i\ \mbox{and}\ f\left(P_{k}\right)<f\left(P_{i}\right)\right\} $ }{\scriptsize\par}

{\scriptsize 14 \hspace{0.5cm}\hspace{0.5cm}\hspace{0.5cm}}{\scriptsize\textbf{Pick}}{\scriptsize{}
$SS$ a random element of $C$}{\scriptsize\par}

{\scriptsize 15 \hspace{0.5cm}\hspace{0.5cm}\hspace{0.5cm}}{\scriptsize\textbf{if}}{\scriptsize{}
$\mbox{strategyMode}=\mbox{aggressive}$ }{\scriptsize\textbf{then}}{\scriptsize\par}

{\scriptsize 16 \hspace{0.5cm}\hspace{0.5cm}\hspace{0.5cm}\hspace{0.5cm}}{\scriptsize\textbf{for}}{\scriptsize{}
$j=1,\ldots,n$ }{\scriptsize\textbf{do}}{\scriptsize\par}

{\scriptsize 17 \hspace{0.5cm}\hspace{0.5cm}\hspace{0.5cm}\hspace{0.5cm}\hspace{0.5cm}}{\scriptsize\textbf{Pick}}{\scriptsize{}
a random number $r\in[0,1]$.}{\scriptsize\par}

{\scriptsize 18 \hspace{0.5cm}\hspace{0.5cm}\hspace{0.5cm}\hspace{0.5cm}\hspace{0.5cm}}{\scriptsize\textbf{Set}}{\scriptsize{}
$I\in\left\{ 1,2\right\} $ a random integer value.}{\scriptsize\par}

{\scriptsize 19 \hspace{0.5cm}\hspace{0.5cm}\hspace{0.5cm}\hspace{0.5cm}\hspace{0.5cm}}{\scriptsize\textbf{Set
}}{\scriptsize$y_{j}=P_{i,j}+2(r-0.5)\left(SS_{j}-IP_{i,j}\right)$}{\scriptsize\par}

{\scriptsize 20 \hspace{0.5cm}\hspace{0.5cm}\hspace{0.5cm}\hspace{0.5cm}}{\scriptsize\textbf{endfor}}{\scriptsize\par}

{\scriptsize 21 \hspace{0.5cm}\hspace{0.5cm}\hspace{0.5cm}}{\scriptsize\textbf{else
if}}{\scriptsize{} strategyMode = conservative }{\scriptsize\textbf{then}}{\scriptsize\par}

{\scriptsize 22 \hspace{0.5cm}\hspace{0.5cm}\hspace{0.5cm}\hspace{0.5cm}}{\scriptsize\textbf{for}}{\scriptsize{}
$j=1,\ldots,n$ }{\scriptsize\textbf{do}}{\scriptsize\par}

{\scriptsize 23 \hspace{0.5cm}\hspace{0.5cm}\hspace{0.5cm}\hspace{0.5cm}\hspace{0.5cm}}{\scriptsize\textbf{Set}}{\scriptsize{}
$r\in[0,1]$ a randomly selected number.}{\scriptsize\par}

{\scriptsize 24 \hspace{0.5cm}\hspace{0.5cm}\hspace{0.5cm}\hspace{0.5cm}\hspace{0.5cm}}{\scriptsize\textbf{Set
}}{\scriptsize$y_{j}=P_{i,j}+0.3\left(r-0.5\right)\left(x_{best,j}-P_{i,j}\right)$}{\scriptsize\par}

{\scriptsize 25 \hspace{0.5cm}\hspace{0.5cm}\hspace{0.5cm}\hspace{0.5cm}}{\scriptsize\textbf{endfor}}{\scriptsize\par}

{\scriptsize 26 \hspace{0.5cm}\hspace{0.5cm}\hspace{0.5cm}}{\scriptsize\textbf{else}}{\scriptsize{}
// mixed strategy}{\scriptsize\par}

{\scriptsize 27 \hspace{0.5cm}\hspace{0.5cm}\hspace{0.5cm}\hspace{0.5cm}}{\scriptsize\textbf{Select}}{\scriptsize{}
with probability 50\% the aggressive or the conservative strategy.}{\scriptsize\par}

{\scriptsize 28 \hspace{0.5cm}\hspace{0.5cm}\hspace{0.5cm}}{\scriptsize\textbf{endif}}{\scriptsize\par}

{\scriptsize 29 \hspace{0.5cm}\hspace{0.5cm}\hspace{0.5cm}}{\scriptsize\textbf{if}}{\scriptsize{}
$y\ \in\ S$ and $f(y)<f\left(P_{i}\right)$ }{\scriptsize\textbf{then}}{\scriptsize\par}

{\scriptsize 30 \hspace{0.5cm}\hspace{0.5cm}\hspace{0.5cm}\hspace{0.5cm}}{\scriptsize\textbf{Set
}}{\scriptsize$P_{i}=y$}{\scriptsize\par}

{\scriptsize 31 \hspace{0.5cm}\hspace{0.5cm}\hspace{0.5cm}}{\scriptsize\textbf{endif}}{\scriptsize\par}

{\scriptsize 32 \hspace{0.5cm}\hspace{0.5cm}}{\scriptsize\textbf{endif}}{\scriptsize\par}

{\scriptsize 33 \hspace{0.5cm}\hspace{0.5cm}// Phase B (dive): pull
towards current best solution}{\scriptsize\par}

{\scriptsize 34 \hspace{0.5cm}\hspace{0.5cm}}{\scriptsize\textbf{Set}}{\scriptsize{}
$b=\arg\min_{k}f\left(P_{k}\right)$ // bestSolution in population
after Phase A}{\scriptsize\par}

{\scriptsize 35 \hspace{0.5cm}\hspace{0.5cm}}{\scriptsize\textbf{for}}{\scriptsize{}
$j=1,\ldots,n$ }{\scriptsize\textbf{do}}{\scriptsize\par}

{\scriptsize 36 \hspace{0.5cm}\hspace{0.5cm}\hspace{0.5cm}}{\scriptsize\textbf{Set
$r\in[0,1]$ }}{\scriptsize a randomly selected number }{\scriptsize\par}

{\scriptsize 37 \hspace{0.5cm}\hspace{0.5cm}\hspace{0.5cm}Set $z_{j}=P_{i,j}+\left(1-2r\right)\left(b_{j}-\frac{P_{i,j}}{t}\right)$}{\scriptsize\par}

{\scriptsize 38 \hspace{0.5cm}\hspace{0.5cm}}{\scriptsize\textbf{if}}{\scriptsize{}
$z\in S$ and $f\left(z\right)<f_{i}$ }{\scriptsize\textbf{then set
$P_{i}=z$}}{\scriptsize\par}

{\scriptsize 39 \hspace{0.5cm}\hspace{0.5cm}}{\scriptsize\textbf{endfor}}{\scriptsize\par}

{\scriptsize 40 \hspace{0.5cm}}{\scriptsize\textbf{endfor}}{\scriptsize\par}

{\scriptsize 41 \hspace{0.5cm}}{\scriptsize\textbf{Set}}{\scriptsize{}
$f_{\mbox{best}}=\mbox{LS}\left(x_{\mbox{best}}\right)$}{\scriptsize\par}

{\scriptsize 42 }{\scriptsize\textbf{endwhile}}{\scriptsize\par}

{\scriptsize 43 }{\scriptsize\textbf{return}}{\scriptsize{} $\left(x_{\mbox{best}},f_{\mbox{best}}\right)$}{\scriptsize\par}
\end{algorithm}
 Furthermore, the steps of the proposed method are also graphically
shown in Figure \ref{fig:Algorithm}.

\begin{figure}
\includegraphics[scale=0.4]{mfo\lyxdot drawio}\caption{The steps of the proposed method \label{fig:Algorithm}}
\end{figure}


\subsection{The used termination rule\label{subsec:The-used-termination}}

The used termination rule is provided in the work of Charilogis and
Tsoulos \citep{stopping rule2} and it has the following steps:
\begin{itemize}
\item At every iteration $t$, the difference between the current best value
$f_{min}^{(t)}$ and the previous best value $f_{min}^{(t-1)}$ is
computed:
\end{itemize}
\begin{center}
\begin{equation}
\delta^{(t)}=\left|f_{\mbox{min}}^{(k)}-f_{\mbox{min}}^{(k-1)}\right|\label{eq:best}
\end{equation}
\par\end{center}
\begin{itemize}
\item The algorithm terminates when $\delta^{(t)}\le\epsilon$ for series
of predefined consecutive iterations $N_{k}$, where $\epsilon$ is
a small positive number, for example $10^{-6}$.
\end{itemize}

\subsection{Movement Strategies}

The proposed optimization algorithm employs three distinct movement
strategies that regulate the behavioral dynamics of seabirds during
the search for optimal solutions. Each strategy defines a different
balance between exploration of the search space and exploitation of
promising regions already identified. The aggressive strategy is characterized
by strong stochastic dynamics and a tendency toward rapid discovery
of new, potentially better regions in the search space. Individuals
are driven toward other, more successful members of the population,
performing large displacements that enhance diversity and dispersion.
This approach promotes exploration and enables the algorithm to escape
local optima. However, it also increases the risk of generating infeasible
or inefficient solutions if appropriate boundary control mechanisms
are not applied. The conservative strategy focuses on exploitation
of the best-known solution, promoting smoother and more stable movements.
Individuals move gradually toward the current global best position
with limited randomness, aiming to refine the accuracy of their position
over time. This strategy reduces population diversity and enhances
convergence. The mixed strategy acts as an adaptive balance mechanism
between the two aforementioned approaches. At each iteration, the
algorithm randomly decides whether to apply the aggressive or conservative
movement pattern, allowing dynamic transitions between exploration
and exploitation phases. This flexibility enables the search process
to adjust to the evolving characteristics of the optimization landscape,
preventing premature convergence and maintaining adaptive search capability.
The integration of these three strategies ensures that the algorithm
maintains both exploratory power, essential for discovering new regions,
and exploitative stability, crucial for fine-tuning solutions. In
conclusion, their combined action enhances the robustness and efficiency
of the method.

\subsection{Algorithmic architecture}

The strength of the proposed method does not lie in any single operator
but in the way its components work together within a coherent and
adaptive optimization framework. The three movement strategies, the
Dive Phase, the selectively applied BFGS refinement, and the dynamic
termination rule are not used as isolated mechanisms, instead, each
one contributes a specific functional role that the algorithm coordinates
through a structured mode-switching process. The aggressive strategy
promotes broad stochastic exploration and enables escape from deceptive
or highly multimodal regions, while the conservative strategy emphasizes
stable, directed exploitation toward high-quality areas of the landscape.
The mixed strategy provides an adaptive transition between these two
behaviors, allowing the search dynamics to adjust to the evolving
geometry and difficulty of the problem. The Dive Phase reinforces
intensification within promising basins, and the BFGS refinement increases
precision when convergence is approaching. The dynamic termination
rule further regulates computational effort by responding to the actual
rate of progress. The synergy among these elements forms an adaptive
architecture capable of shifting naturally from global exploration
to precise local refinement. This coordinated interaction rather than
any individual mechanism constitutes the principal innovation of the
method and explains its robustness, reduced number of function evaluations,
and improved convergence behavior across diverse optimization landscapes.


\section{Results\label{sec:Results} }

This section begins with a description of the functions that will
be used in the experiments and then presents in detail the experiments
that were performed, in which the parameters available in the proposed
algorithm were studied, in order to study its reliability and adequacy. 

\subsection{Test Functions}

A variety of test functions were used in the conducted experiments\citep{testfunctions1,testfunctions2,testfunctions3,testfunctions4,testfunctions5,gkls}.
These functions are used in a series of research papers. The description
of each used test function is provided below. In all cases, the constant
$n$ defines the dimension of the objective function\@.
\begin{center}
{\tiny{}
\begin{table}
\centering{}{\tiny{}%
\begin{tabular}{|c|c|c|c|}
\hline 
{\tiny\textbf{NAME}} & {\tiny\textbf{FORMULA}} & {\tiny\textbf{DIM}} & {\tiny$G_{min}$}\tabularnewline
\hline 
\hline 
{\tiny\textbf{ACKLEY}} & \begin{cellvarwidth}[t]
\centering
{\tiny$f(x)=-a\exp\left(-b\sqrt{\frac{1}{n}\sum_{i=1}^{n}x_{i}^{2}}\right)-\exp\left(\frac{1}{n}\sum_{i=1}^{n}\cos\left(cx_{i}\right)\right)+a+\exp(1)$}{\tiny\par}

{\tiny$a=20.0,b=0.2$}
\end{cellvarwidth} & {\tiny 2} & {\tiny 4.440892099e-16}\tabularnewline
\hline 
{\tiny\textbf{BF1}} & {\tiny$f(x)=x_{1}^{2}+2x_{2}^{2}-\frac{3}{10}\cos\left(3\pi x_{1}\right)-\frac{4}{10}\cos\left(4\pi x_{2}\right)+\frac{7}{10}$} & {\tiny 2} & {\tiny 0}\tabularnewline
\hline 
{\tiny\textbf{BF2}} & {\tiny$f(x)=x_{1}^{2}+2x_{2}^{2}-\frac{3}{10}\cos\left(3\pi x_{1}\right)\cos\left(4\pi x_{2}\right)+\frac{3}{10}$} & {\tiny 2} & {\tiny 0}\tabularnewline
\hline 
{\tiny\textbf{BF3}} & {\tiny$f(x)=x_{1}^{2}+2x_{2}^{2}-\frac{3}{10}\cos\left(3\pi x_{1}+4\pi x_{2}\right)+\frac{3}{10}$} & {\tiny 2} & {\tiny 0}\tabularnewline
\hline 
{\tiny\textbf{BRANIN}} & \begin{cellvarwidth}[t]
\centering
{\tiny$f(x)=\left(x_{2}-\frac{5.1}{4\pi^{2}}x_{1}^{2}+\frac{5}{\pi}x_{1}-6\right)^{2}+10\left(1-\frac{1}{8\pi}\right)\cos(x_{1})+10$}{\tiny\par}

{\tiny$-5\le x_{1}\le10,\ 0\le x_{2}\le15$}
\end{cellvarwidth} & {\tiny 2} & {\tiny 0.3978873577}\tabularnewline
\hline 
{\tiny\textbf{CAMEL}} & {\tiny$f(x)=4x_{1}^{2}-2.1x_{1}^{4}+\frac{1}{3}x_{1}^{6}+x_{1}x_{2}-4x_{2}^{2}+4x_{2}^{4},\quad x\in[-5,5]^{2}$} & {\tiny 2} & {\tiny -1.031628453}\tabularnewline
\hline 
{\tiny\textbf{DIFFERENT POWERS}} & {\tiny$f(\mathbf{x})=\sqrt{\sum_{i=1}^{n}|x_{i}|^{2+4\frac{i-1}{n-1}}}$} & {\tiny n} & {\tiny 0}\tabularnewline
\hline 
{\tiny\textbf{DIFFPOWER}} & {\tiny$f(x)=\sum_{i=1}^{n}|x_{i}-y_{i}|^{p}$$\ \ n=2\ p=2,5,10$} & {\tiny n} & {\tiny 0}\tabularnewline
\hline 
{\tiny\textbf{DISCUS}} & {\tiny$f(x)=10^{6}x_{1}^{2}+\sum_{i=2}^{n}x_{i}^{2}$} & {\tiny n} & {\tiny 0}\tabularnewline
\hline 
{\tiny\textbf{EASOM}} & {\tiny$f(x)=-\cos\left(x_{1}\right)\cos\left(x_{2}\right)\exp\left(\left(x_{2}-\pi\right)^{2}-\left(x_{1}-\pi\right)^{2}\right)$} & {\tiny 2} & {\tiny -1}\tabularnewline
\hline 
{\tiny\textbf{ELLIPSOIDAL}} & {\tiny$f(x)=\sum_{i=1}^{n}\left(10^{6}\right)^{\frac{i-1}{n-1}}x_{i}^{2}$} & {\tiny n} & {\tiny 0}\tabularnewline
\hline 
{\tiny\textbf{EQUAL MAXIMA}} & {\tiny$f(x)=\sin^{6}(5\pi x)\cdot e^{-2\log(2)\cdot\left(\frac{x-0.1}{0.8}\right)^{2}}$} & {\tiny n} & {\tiny 0}\tabularnewline
\hline 
{\tiny\textbf{EXP}} & {\tiny$f(x)=-\exp\left(-0.5\sum_{i=1}^{n}x_{i}^{2}\right),\quad-1\le x_{i}\le1$} & {\tiny n} & {\tiny -1}\tabularnewline
\hline 
{\tiny\textbf{GKLS}} & {\tiny$f(x)=\mbox{Gkls}(x,n,w)$$\ \ n=2,3\ w=50,100$} & {\tiny n} & {\tiny -1}\tabularnewline
\hline 
{\tiny\textbf{GOLDSTAIN}} & \begin{cellvarwidth}[t]
\centering
{\tiny$f(x)=[1+(x_{1}+x_{2+1)})^{2}(19-14x_{1}+3x_{1}^{2}14x_{2}+6x_{1}x_{2}+3x_{2}^{2})]$}{\tiny\par}

{\tiny$[30+(2x_{1}-3x_{2})^{2}(18-32x_{1}+12x_{1}^{2}+48x_{2}-36x_{1}x_{2}+27x_{2}^{2})]$}
\end{cellvarwidth} & {\tiny 2} & {\tiny 3}\tabularnewline
\hline 
\begin{cellvarwidth}[t]
\centering
{\tiny\textbf{GRIEWANK}}{\tiny\par}

{\tiny\textbf{ROSENBROCK}}
\end{cellvarwidth} & {\tiny$f(\mathbf{x})=\underbrace{\left(\frac{\|\mathbf{x}\|^{2}}{4000}-\prod_{i=1}^{n}\cos\left(\frac{x_{i}}{\sqrt{i}}\right)+1\right)}_{\text{Griewank}}\cdot\underbrace{\left(\frac{1}{10}\sum_{i=1}^{n-1}\left[100(x_{i+1}-x_{i}^{2})^{2}+(1-x_{i})^{2}\right]\right)}_{\text{Rosenbrock}}$} & {\tiny n} & {\tiny 0}\tabularnewline
\hline 
{\tiny\textbf{GRIEWANK}} & {\tiny$f(x)=1+\frac{1}{200}\sum_{i=1}^{n}x_{i}^{2}-\prod_{i=1}^{n}\frac{\cos(x_{i})}{\sqrt{(i)}}$} & {\tiny n} & {\tiny 0}\tabularnewline
\hline 
{\tiny\textbf{HANSEN}} & {\tiny$f(x)=\sum_{i=1}^{5}i\cos\left[(i-1)x_{1}+i\right]\sum_{j=1}^{5}j\cos\left[(j+1)x_{2}+j\right]$} & {\tiny 2} & {\tiny -176.5417931}\tabularnewline
\hline 
{\tiny\textbf{HARTMAN3}} & {\tiny$f(x)=-\sum_{i=1}^{4}c_{i}\exp\left(-\sum_{j=1}^{3}a_{ij}\left(x_{j}-p_{ij}\right)^{2}\right)$} & {\tiny 3} & {\tiny -3.862782148}\tabularnewline
\hline 
{\tiny\textbf{HARTAMAN6}} & {\tiny$f(x)=-\sum_{i=1}^{4}c_{i}\exp\left(-\sum_{j=1}^{6}a_{ij}\left(x_{j}-p_{ij}\right)^{2}\right)$} & {\tiny 6} & {\tiny -3.22368011}\tabularnewline
\hline 
{\tiny\textbf{KATSUURA}} & {\tiny$f(\mathbf{x})=\frac{10}{n^{2}}\prod_{i=1}^{n}\left(1+i\sum_{k=1}^{32}\frac{|2^{k}x_{i}-\lfloor2^{k}x_{i}\rfloor|}{2^{k}}\right)^{\frac{10}{n^{1.2}}}-\frac{10}{n^{2}}+\frac{1}{Dn}\sum_{i=1}^{n}x_{i}^{2}$} & {\tiny n} & {\tiny 0}\tabularnewline
\hline 
{\tiny\textbf{MICHALEWICZ}} & {\tiny$f(\mathbf{x})=-\sum_{i=1}^{n}\sin(x_{i})\cdot\sin^{2m}\left(\frac{i\cdot x_{i}^{2}}{\pi}\right)$} & \begin{cellvarwidth}[t]
\centering
{\tiny 2,}{\tiny\par}

{\tiny 5,}{\tiny\par}

{\tiny 10}
\end{cellvarwidth} & \begin{cellvarwidth}[t]
\centering
{\tiny -1.8013}{\tiny\par}

{\tiny -4.6877}{\tiny\par}

{\tiny -9.6602}
\end{cellvarwidth}\tabularnewline
\hline 
{\tiny\textbf{POTENTIAL}} & {\tiny$V_{LJ}(r)=4\epsilon\left[\left(\frac{\sigma}{r}\right)^{12}-\left(\frac{\sigma}{r}\right)^{6}\right]$$\ \ n=9,15,21,30$} & \begin{cellvarwidth}[t]
\centering
{\tiny 5,}{\tiny\par}

{\tiny 6,}{\tiny\par}

{\tiny 10}
\end{cellvarwidth} & \begin{cellvarwidth}[t]
\centering
{\tiny -9.103852416}{\tiny\par}

{\tiny -12.71206226}{\tiny\par}

{\tiny -28.42253189}
\end{cellvarwidth}\tabularnewline
\hline 
{\tiny\textbf{RARSTIGIN2}} & {\tiny$f(x)=x_{1}^{2}+x_{2}^{2}-\cos(18x_{1})-\cos(18x_{2})$} & {\tiny 2} & {\tiny -2}\tabularnewline
\hline 
{\tiny\textbf{ROSENBROCK}} & {\tiny$f(x)=\sum_{i=1}^{n-1}\left(100\left(x_{i+1}-x_{i}^{2}\right)^{2}+\left(x_{i}-1\right)^{2}\right),\quad-30\le x_{i}\le30$} & {\tiny n} & {\tiny 0}\tabularnewline
\hline 
{\tiny\textbf{SHARP RIDGE}} & {\tiny$f(\mathbf{x})=x_{1}^{2}+\alpha\sum_{i=2}^{n}x_{i}^{2},\ a>1$} & {\tiny n} & {\tiny 0}\tabularnewline
\hline 
{\tiny\textbf{SHEKEL5}} & {\tiny$f(x)=-\sum_{i=1}^{5}\frac{1}{(x-a_{i})(x-a_{i})^{T}+c_{i}}$} & {\tiny 4} & {\tiny -10.10774912}\tabularnewline
\hline 
{\tiny\textbf{SHEKEL7}} & {\tiny$f(x)=-\sum_{i=1}^{7}\frac{1}{(x-a_{i})(x-a_{i})^{T}+c_{i}}$} & {\tiny 4} & {\tiny -10.342377774}\tabularnewline
\hline 
{\tiny\textbf{SHEKEL10}} & {\tiny$f(x)=-\sum_{i=1}^{10}\frac{1}{(x-a_{i})(x-a_{i})^{T}+c_{i}}$} & {\tiny 4} & {\tiny -10.53640982}\tabularnewline
\hline 
{\tiny\textbf{SINUSOIDAL}} & {\tiny$f(x)=-\left(2.5\prod_{i=1}^{n}\sin\left(x_{i}-z\right)+\prod_{i=1}^{n}\sin\left(5\left(x_{i}-z\right)\right)\right),\quad0\le x_{i}\le\pi$} & {\tiny n} & {\tiny -3.5}\tabularnewline
\hline 
{\tiny\textbf{SPHERE}} & {\tiny$f(\mathbf{x})=\sum_{i=1}^{n}x_{i}^{2}$} & {\tiny n} & {\tiny 0}\tabularnewline
\hline 
{\tiny\textbf{STEP ELLIPSOIDAL}} & {\tiny$f(\mathbf{x})=\sum_{i=1}^{n}\left\lfloor x_{i}+0.5\right\rfloor ^{2}+\alpha\sum_{i=1}^{n}\left(10^{6}\cdot\frac{i-1}{n-1}\right)x_{i}^{2},\ a=1$} & {\tiny n} & {\tiny 0}\tabularnewline
\hline 
{\tiny\textbf{TEST2N}} & {\tiny$f(x)=\frac{1}{2}\sum_{i=1}^{n}x_{i}^{4}-16x_{i}^{2}+5x_{i}$} & \begin{cellvarwidth}[t]
\centering
{\tiny 4,}{\tiny\par}

{\tiny 5}
\end{cellvarwidth} & \begin{cellvarwidth}[t]
\centering
{\tiny -156.6646628}{\tiny\par}

{\tiny -195.8308285}
\end{cellvarwidth}\tabularnewline
\hline 
{\tiny\textbf{TEST30N}} & \begin{cellvarwidth}[t]
\centering
{\tiny$f(x)=\frac{1}{10}\sin^{2}\left(3\pi x_{1}\right)\sum_{i=2}^{n-1}\left(\left(x_{i}-1\right)^{2}\left(1+\sin^{2}\left(3\pi x_{i+1}\right)\right)\right)+$}{\tiny\par}

{\tiny$+\left(x_{n}-1\right)^{2}\left(1+\sin^{2}\left(2\pi x_{n}\right)\right)$}
\end{cellvarwidth} & {\tiny n} & {\tiny 0}\tabularnewline
\hline 
{\tiny\textbf{ZAKHAROV}} & {\tiny$f(\mathbf{x})=\sum_{i=1}^{n}x_{i}^{2}+\left(\sum_{i=1}^{n}\frac{i}{2}x_{i}\right)^{2}+\left(\sum_{i=1}^{n}\frac{i}{2}x_{i}\right)^{4}$} & {\tiny n} & {\tiny 0}\tabularnewline
\hline 
\end{tabular}}
\end{table}
}{\tiny\par}
\par\end{center}

\subsection{Experimental results }

A series of experiments was carried out for the previously mentioned
functions and these experiments were executed on an AMD RYZEN 5950X
with 128GB RAM. The operating system of the running machine was Debian
Linux. Each experiment was conducted 30 times, with different random
numbers each time, and the averages were recorded. The software used
in the experiments was coded in ANSI C++ using the freely available
optimization environment of OPTIMUS \citep{optimus}, which can be
downloaded from\textbf{ }\url{https://github.com/itsoulos/OPTIMUS}.
The values for the experimental parameters used in the proposed method
are outlined in Table \ref{tab:expSettings}. 

\begin{table}[H]
\caption{The values of the parameters of the proposed method.\label{tab:expSettings}}

\centering{}%
\begin{tabular}{|c|c|c|}
\hline 
PARAMETER & MEANING & VALUE\tabularnewline
\hline 
\hline 
N & Number of agents & 200\tabularnewline
\hline 
$G_{max}$ & Maximum number of allowed iterations & 200\tabularnewline
\hline 
$p_{l}$ & Local search rate & 0.05\tabularnewline
\hline 
$T$ & Number of mfo iterations & 2\tabularnewline
\hline 
$F$ & Differential weight for DE & 0.8\tabularnewline
\hline 
$CR$ & Crossover probability & 0.9\tabularnewline
\hline 
$N_{I}$ & Number of iterations used in the termination rule & 8\tabularnewline
\hline 
- & Mutation rate for GA & 0.05 (5\%)\tabularnewline
\hline 
- & Selection Rate for GA & 0.05 (5\%)\tabularnewline
\hline 
- & Selection method for GA & Roulette\tabularnewline
\hline 
\end{tabular}
\end{table}
In the following tables that depict the experimental results, the
numbers in cells stand for the average function calls, as measured
on 30 independent runs. The numbers in parentheses denote the fraction
of the executions where the method discovered successfully the global
minimum. If this number is not present, then the method managed to
locate the global minimum in every run ( 100\% success).

\subsection{The proposed method in comparison with others}

The comparison of results for nine optimization methods (PROPOSED,
Genetic Algorithm (GA)\citep{genetic3}, Whale Optimization Algorithm(WOA)\citep{key-22},
MEWOA\citep{key-21}, Differential Evolution (DE)\citep{diffe1},
Particle Swarm Optimization (IPSO)\citep{key-23}, Arithmetic Optimization
Algorithm (AOA)\citep{key-4}, Smell agent Optimization (SAO)\citep{key-5},
Grey Wolf Optimizer (MYIGWO)\citep{key-20} and Ant Colony Optimization
(ACO)\citep{aco1}) is based on a diverse set of benchmark functions.
The evaluation focused on two key performance indicators: the total
number of objective function evaluations and the corresponding success
rates, as shown in Table 2. The analysis of the total function evaluations
reveals substantial differences among the tested algorithms. The GENETIC
algorithm required 316,189 evaluations, while WOA recorded the highest
computational cost with 923,113 evaluations. The MEWOA, DE, IPSO,
AOA, SAO, ACO and MYIGWO methods required 489,691, 326,083, 314,060,
540,139, 305,505, 260,753 and 39,018 evaluations, respectively. In
contrast, the PROPOSED algorithm achieved the lowest total number
of function evaluations among the tested methods, requiring only 204,488
calls overall, demonstrating exceptional computational efficiency.
This advantage was observed across almost all benchmark functions.
For example, on the Ackley function the PROPOSED method converged
in only 4,971 evaluations, whereas GENETIC and WOA required 5,811
and 24,766 evaluations, respectively. Similarly, for the Branin function,
the PROPOSED algorithm reached convergence in 2,397 evaluations, compared
with 2,648 for GENETIC and 4,777 for AOA. While the MYIGWO method
required even fewer evaluations on average (approximately 39,018 in
total), its success rate was substantially lower (around 69\%), indicating
reduced reliability in consistently reaching the optimal solution.
This highlights a trade-off between computational speed and robustness,
with the PROPOSED algorithm achieving both low computational cost
and high consistency. In addition to the reduced number of evaluations,
the PROPOSED algorithm maintained a 100\% success rate on most benchmark
functions, further supporting its reliability. Other methods such
as DE, IPSO, and SAO also produced satisfactory results but with noticeably
higher computational effort. By comparison, ACO exhibited lower consistency,
with success rates below unity (e.g., 0.57 for BF1 and 0.70 for BF2).
Overall, the results indicate that the PROPOSED algorithm combines
strong reliability with high computational efficiency, achieving performance
comparable to or better than well-known alternatives while requiring
substantially fewer evaluations.

{\tiny{}
\begin{table}
{\tiny\caption{Experimental results using different optimization methods. Numbers
in cells represent sum function calls.}
}{\tiny\par}
\raggedright{}{\tiny{}%
\begin{tabular}{ccccccccccc}
\hline 
{\tiny\textbf{FUNCTION}} & {\tiny\textbf{PROPOSED}} & {\tiny\textbf{GENETIC}} & {\tiny\textbf{WOA}} & \begin{cellvarwidth}[t]
\centering
{\tiny\textbf{ME}}\\
{\tiny\textbf{WOA}}
\end{cellvarwidth} & {\tiny\textbf{DE}} & {\tiny\textbf{IPSO}} & {\tiny\textbf{AOA}} & {\tiny\textbf{SAO}} & {\tiny\textbf{ACO}} & \begin{cellvarwidth}[t]
\centering
{\tiny MYI}{\tiny\par}

{\tiny GWO}
\end{cellvarwidth}\tabularnewline
\hline 
\hline 
{\tiny ACKLEY} & {\tiny 4971} & {\tiny 5811} & {\tiny 24766} & {\tiny 15257} & {\tiny 6425} & {\tiny 7355} & {\tiny 9590} & {\tiny 5904} & {\tiny 6112} & {\tiny 926(0.60)}\tabularnewline
\hline 
{\tiny BF1} & {\tiny 3449} & {\tiny 4279} & {\tiny 9924} & {\tiny 8423} & {\tiny 4272} & {\tiny 4121} & {\tiny 8356} & {\tiny 4694} & {\tiny 4120(0.57)} & {\tiny 846(0.30)}\tabularnewline
\hline 
{\tiny BF2} & {\tiny 3255} & {\tiny 3903} & {\tiny 9597} & {\tiny 7982} & {\tiny 3863} & {\tiny 3767} & {\tiny 6987} & {\tiny 4166} & {\tiny 3613(0.70)} & {\tiny 837(0.23)}\tabularnewline
\hline 
{\tiny BF3} & {\tiny 3028} & {\tiny 3494} & {\tiny 20117} & {\tiny 7087} & {\tiny 3475} & {\tiny 3305} & {\tiny 7029} & {\tiny 3706} & {\tiny 3563(0.77)} & {\tiny 832(0.30)}\tabularnewline
\hline 
{\tiny BRANIN} & {\tiny 2397} & {\tiny 2648} & {\tiny 5939} & {\tiny 4170} & {\tiny 2543} & {\tiny 2522} & {\tiny 4777} & {\tiny 2583} & {\tiny 2814} & {\tiny 818(0.97)}\tabularnewline
\hline 
{\tiny CAMEL} & {\tiny 2757} & {\tiny 3104} & {\tiny 5917} & {\tiny 5854} & {\tiny 3054} & {\tiny 2942} & {\tiny 5050} & {\tiny 3089} & {\tiny 2421} & {\tiny 825}\tabularnewline
\hline 
\begin{cellvarwidth}[t]
\centering
{\tiny DIFFERENT}\\
{\tiny POWERS10}
\end{cellvarwidth} & {\tiny 6651} & {\tiny 10954} & {\tiny 23650} & {\tiny 16588} & {\tiny 10542} & {\tiny 9807} & {\tiny 27533} & {\tiny 11234} & {\tiny 10471} & {\tiny 924}\tabularnewline
\hline 
\begin{cellvarwidth}[t]
\centering
{\tiny DIFF}\\
{\tiny POWER2}
\end{cellvarwidth} & {\tiny 5614} & {\tiny 9053} & {\tiny 13476} & {\tiny 14248} & {\tiny 8510} & {\tiny 8121} & {\tiny 14367} & {\tiny 9139} & {\tiny 10540} & {\tiny 881}\tabularnewline
\hline 
\begin{cellvarwidth}[t]
\centering
{\tiny DIFF}\\
{\tiny POWER5}
\end{cellvarwidth} & {\tiny 13836} & {\tiny 24466} & {\tiny 51816} & {\tiny 40313} & {\tiny 25515} & {\tiny 27348} & {\tiny 58890} & {\tiny 27918} & {\tiny 32583} & {\tiny 1228}\tabularnewline
\hline 
\begin{cellvarwidth}[t]
\centering
{\tiny DIFF}\\
{\tiny POWER10}
\end{cellvarwidth} & {\tiny 19486} & {\tiny 37664} & {\tiny 73782} & {\tiny 56980} & {\tiny 40402} & {\tiny 34923} & {\tiny 74297} & {\tiny 39429} & {\tiny 16664} & {\tiny 1342}\tabularnewline
\hline 
{\tiny DISCUS10} & {\tiny 2185} & {\tiny 2711} & {\tiny 6476} & {\tiny 4084} & {\tiny 2663} & {\tiny 2540} & {\tiny 6302} & {\tiny 2716} & {\tiny 2010} & {\tiny 821}\tabularnewline
\hline 
{\tiny EASOM} & {\tiny 2033} & {\tiny 2062} & {\tiny 3153} & {\tiny 3194} & {\tiny 1953} & {\tiny 1998} & {\tiny 2691} & {\tiny 2167} & {\tiny 1995} & {\tiny 808}\tabularnewline
\hline 
\begin{cellvarwidth}[t]
\centering
{\tiny ELL}\\
{\tiny IPSOIDAL10}
\end{cellvarwidth} & {\tiny 2479} & {\tiny 3309} & {\tiny 8979} & {\tiny 5057} & {\tiny 3339} & {\tiny 3540} & {\tiny 6308} & {\tiny 2507} & {\tiny 2146} & {\tiny 831}\tabularnewline
\hline 
\begin{cellvarwidth}[t]
\centering
{\tiny EQUAL}\\
{\tiny MAXIMA10}
\end{cellvarwidth} & {\tiny 8968} & {\tiny 16399} & {\tiny 63542} & {\tiny 26257} & {\tiny 17891} & {\tiny 17231} & {\tiny 20536} & {\tiny 17014} & {\tiny 18343} & {\tiny 1061}\tabularnewline
\hline 
\begin{cellvarwidth}[t]
\centering
{\tiny EXP}\\
{\tiny ONENTIAL10}
\end{cellvarwidth} & {\tiny 2768} & {\tiny 3854} & {\tiny 8408} & {\tiny 6224} & {\tiny 3928} & {\tiny 3298} & {\tiny 6216} & {\tiny 3052} & {\tiny 3941} & {\tiny 839}\tabularnewline
\hline 
{\tiny GKLS250} & {\tiny 2701} & {\tiny 2579} & {\tiny 4978} & {\tiny 4481} & {\tiny 2412} & {\tiny 2580} & {\tiny 3220} & {\tiny 1789} & {\tiny 2238} & {\tiny 822}\tabularnewline
\hline 
{\tiny GKLS350} & {\tiny 3054} & {\tiny 3076} & {\tiny 7226(0.96)} & {\tiny 4117} & {\tiny 2860} & {\tiny 3016} & {\tiny 3314} & {\tiny 1634(0.86)} & {\tiny 2110(0.90)} & {\tiny 824(0.70)}\tabularnewline
\hline 
{\tiny GOLDSTEIN} & {\tiny 3195} & {\tiny 3802} & {\tiny 9016} & {\tiny 6598} & {\tiny 3736} & {\tiny 3856} & {\tiny 6508} & {\tiny 3522} & {\tiny 4239} & {\tiny 843(0.97)}\tabularnewline
\hline 
{\tiny GRIEWANK2} & {\tiny 4289(0.80)} & {\tiny 4313(0.73)} & {\tiny 7948(0.87)} & {\tiny 6613(0.96)} & {\tiny 4689(0.80)} & {\tiny 4516} & {\tiny 6339(0.73)} & {\tiny 4881(0.73)} & {\tiny 3624(0.43)} & {\tiny 827(0.17)}\tabularnewline
\hline 
{\tiny GRIEWANK10} & {\tiny 5374} & {\tiny 8381} & {\tiny 51143} & {\tiny 14380} & {\tiny 8888} & {\tiny 8087} & {\tiny 16979} & {\tiny 8788} & {\tiny 5679} & {\tiny 912(0.17)}\tabularnewline
\hline 
\begin{cellvarwidth}[t]
\centering
{\tiny GRIEWANK}\\
{\tiny ROSENBROCK10}
\end{cellvarwidth} & {\tiny 5505} & {\tiny 9576} & {\tiny 16592} & {\tiny 13856} & {\tiny 9923} & {\tiny 7675} & {\tiny 17936} & {\tiny 9157} & {\tiny 3694} & {\tiny 927(0.97)}\tabularnewline
\hline 
{\tiny HANSEN} & {\tiny 3531} & {\tiny 3345} & {\tiny 10498} & {\tiny 5072} & {\tiny 3189} & {\tiny 3546} & {\tiny 4166} & {\tiny 3245} & {\tiny 3042(0.70)} & {\tiny 824(0.63)}\tabularnewline
\hline 
{\tiny HARTMAN3} & {\tiny 2633} & {\tiny 3142} & {\tiny 8440} & {\tiny 5095} & {\tiny 3177} & {\tiny 3103} & {\tiny 4034} & {\tiny 2539} & {\tiny 3297} & {\tiny 826}\tabularnewline
\hline 
{\tiny HARTMAN6} & {\tiny 2874} & {\tiny 3941} & {\tiny 22615} & {\tiny 6214} & {\tiny 3870} & {\tiny 3688} & {\tiny 5468} & {\tiny 3019} & {\tiny 3949(0.77)} & {\tiny 840(0.37)}\tabularnewline
\hline 
{\tiny KATSUURA10} & {\tiny 8648(0.03)} & {\tiny 19539(0.03)} & {\tiny 61156(0.33)} & {\tiny 25791(0.03)} & {\tiny 22199(0.03)} & {\tiny 16452(0.03)} & {\tiny 24172(0.03)} & {\tiny 18650(0.03)} & {\tiny 24341(0.03)} & {\tiny 1010(0.03)}\tabularnewline
\hline 
\begin{cellvarwidth}[t]
\centering
{\tiny MIC}\\
{\tiny HALEWICZ4}
\end{cellvarwidth} & {\tiny 3757(0.63)} & {\tiny 5906(0.90)} & {\tiny 14045(0.87)} & {\tiny 7419(0.97)} & {\tiny 5186(0.97)} & {\tiny 4583(0.93)} & {\tiny 5746(0.93} & {\tiny 4176(0.80)} & {\tiny 4213(0.17)} & {\tiny 836(0.17)}\tabularnewline
\hline 
{\tiny POTENTIAL5} & {\tiny 4944} & {\tiny 8166} & {\tiny 39455} & {\tiny 12218} & {\tiny 8525} & {\tiny 10128} & {\tiny 10883} & {\tiny 7879} & {\tiny 7456} & {\tiny 905}\tabularnewline
\hline 
{\tiny POTENTIAL6} & {\tiny 5887(0.37)} & {\tiny 12034(0.80)} & {\tiny 46466(0.77)} & {\tiny 13728(0.77)} & {\tiny 12073(0.73)} & {\tiny 13873(0.47)} & {\tiny 11948(0.67)} & {\tiny 9564(0.70)} & {\tiny 8212(0.13)} & {\tiny 919}\tabularnewline
\hline 
{\tiny POTENTIAL10} & {\tiny 8599(0.93)} & {\tiny 15417} & {\tiny 59487} & {\tiny 23210} & {\tiny 16382} & {\tiny 21690(0.97)} & {\tiny 18199(0.96)} & {\tiny 14463} & {\tiny 9235(0.27)} & {\tiny 980(0.03)}\tabularnewline
\hline 
{\tiny RASTRIGIN} & {\tiny 3639} & {\tiny 3840(0.93)} & {\tiny 8116} & {\tiny 6127} & {\tiny 4287} & {\tiny 3956} & {\tiny 5052} & {\tiny 2813(0.93)} & {\tiny 2676(0.33)} & {\tiny 802}\tabularnewline
\hline 
\begin{cellvarwidth}[t]
\centering
{\tiny ROS}\\
{\tiny ENBROCK8}
\end{cellvarwidth} & {\tiny 4964} & {\tiny 8085} & {\tiny 14094} & {\tiny 12282} & {\tiny 8245} & {\tiny 7843} & {\tiny 16423} & {\tiny 8448} & {\tiny 2893} & {\tiny 925}\tabularnewline
\hline 
\begin{cellvarwidth}[t]
\centering
{\tiny ROS}\\
{\tiny ENBROCK16}
\end{cellvarwidth} & {\tiny 6467} & {\tiny 11469} & {\tiny 23544} & {\tiny 17851} & {\tiny 11999} & {\tiny 11450} & {\tiny 25092} & {\tiny 12016} & {\tiny 3604} & {\tiny 970}\tabularnewline
\hline 
\begin{cellvarwidth}[t]
\centering
{\tiny SHARP}\\
{\tiny RIDGE10}
\end{cellvarwidth} & {\tiny 6006} & {\tiny 10297} & {\tiny 18391} & {\tiny 15280} & {\tiny 10924} & {\tiny 10481} & {\tiny 22452} & {\tiny 10013} & {\tiny 3594} & {\tiny 947}\tabularnewline
\hline 
{\tiny SHEKEL5} & {\tiny 3230} & {\tiny 4540} & {\tiny 17010} & {\tiny 6551} & {\tiny 4361} & {\tiny 3986} & {\tiny 7615} & {\tiny 4271} & {\tiny 4492(0.63)} & {\tiny 834(0.23)}\tabularnewline
\hline 
{\tiny SHEKEL7} & {\tiny 3212} & {\tiny 4517} & {\tiny 17873} & {\tiny 6985} & {\tiny 4310} & {\tiny 4009} & {\tiny 7491} & {\tiny 4245} & {\tiny 4719(0.67)} & {\tiny 836(0.26)}\tabularnewline
\hline 
{\tiny SHEKEL10} & {\tiny 3168} & {\tiny 4266} & {\tiny 15524} & {\tiny 6750} & {\tiny 4217} & {\tiny 4010} & {\tiny 7074} & {\tiny 4111} & {\tiny 4442(0.47)} & {\tiny 832(0.13)}\tabularnewline
\hline 
{\tiny SPHERE10} & {\tiny 1660} & {\tiny 1706} & {\tiny 6389} & {\tiny 2927} & {\tiny 1653} & {\tiny 1636} & {\tiny 5128} & {\tiny 906} & {\tiny 1843} & {\tiny 805}\tabularnewline
\hline 
\begin{cellvarwidth}[t]
\centering
{\tiny STEPE}\\
{\tiny LLIPSOIDAL4}
\end{cellvarwidth} & {\tiny 3018} & {\tiny 2842(0.83)} & {\tiny 2129} & {\tiny 2237(0.93)} & {\tiny 2385(0.73)} & {\tiny 1879} & {\tiny 2701(0.43)} & {\tiny 1262(0.07)} & {\tiny 1583(0.10)} & {\tiny 803(0.30)}\tabularnewline
\hline 
\begin{cellvarwidth}[t]
\centering
{\tiny SIN}\\
{\tiny USOIDAL8}
\end{cellvarwidth} & {\tiny 3272} & {\tiny 4572} & {\tiny 20782} & {\tiny 7032} & {\tiny 4519} & {\tiny 3995} & {\tiny 7249} & {\tiny 3901} & {\tiny 4023(0.83)} & {\tiny 845(0.83)}\tabularnewline
\hline 
\begin{cellvarwidth}[t]
\centering
{\tiny SIN}\\
{\tiny USOIDAL16}
\end{cellvarwidth} & {\tiny 4454(0.93)} & {\tiny 6588} & {\tiny 34409} & {\tiny 9368} & {\tiny 7349} & {\tiny 4697} & {\tiny 10405} & {\tiny 6491} & {\tiny 4669(0.63)} & {\tiny 863(0.60)}\tabularnewline
\hline 
{\tiny TEST2N4} & {\tiny 3236} & {\tiny 3878} & {\tiny 12279} & {\tiny 6019} & {\tiny 3699} & {\tiny 3611} & {\tiny 5765} & {\tiny 3251} & {\tiny 3504(0.56)} & {\tiny 831(0.43)}\tabularnewline
\hline 
{\tiny TEST2N5} & {\tiny 3639(0.97)} & {\tiny 4109(0.86)} & {\tiny 15351(0.83)} & {\tiny 6178} & {\tiny 4248(0.96)} & {\tiny 3977(0.93)} & {\tiny 6647(0.93)} & {\tiny 3569(0.97)} & {\tiny 3718(0.30)} & {\tiny 834(0.17)}\tabularnewline
\hline 
{\tiny TEST30N10} & {\tiny 3134} & {\tiny 5238} & {\tiny 16970} & {\tiny 8809} & {\tiny 5206} & {\tiny 6055} & {\tiny 7915} & {\tiny 4915} & {\tiny 6203} & {\tiny 956(0.90)}\tabularnewline
\hline 
\begin{cellvarwidth}[t]
\centering
{\tiny ZAK}\\
{\tiny HAROV10}
\end{cellvarwidth} & {\tiny 2521} & {\tiny 3314} & {\tiny 11645} & {\tiny 4785} & {\tiny 3197} & {\tiny 2864} & {\tiny 5289} & {\tiny 2669} & {\tiny 2125} & {\tiny 821}\tabularnewline
\hline 
{\tiny\textbf{SUM}} & {\tiny\textbf{204488(0.95)}} & {\tiny\textbf{316189(0.96)}} & {\tiny\textbf{923113(0.97)}} & {\tiny\textbf{489691(0.97)}} & {\tiny\textbf{326083(0.96)}} & {\tiny\textbf{314060(0.96)}} & {\tiny\textbf{540139(0.95)}} & {\tiny\textbf{305505(0.93)}} & {\tiny\textbf{260753(0.77)}} & {\tiny 39018(0.69)}\tabularnewline
\hline 
\end{tabular}}{\tiny\par}
\end{table}
}{\tiny\par}

\begin{figure}
\includegraphics[scale=0.5]{fig2}

\caption{A statistical comparison of the proposed with other optimization methods}
\end{figure}

Figure 2 presents the pairwise statistical significance between the
PROPOSED optimizer and all baseline algorithms. All pairwise comparisons
were conducted using the paired Wilcoxon signed-rank test, and the
resulting p-values were corrected using the Holm adjustment to account
for multiple testing. Following the conventional star notation (ns:
p \textgreater{} 0.05, {*}: p \textless{} 0.05, {*}{*}: p \textless{}
0.01, {*}{*}{*}: p \textless{} 0.001, {*}{*}{*}{*}: p \textless{}
0.0001), all comparisons except ACO reach {*}{*}{*}{*}, indicating
very-extremely significant differences. Specifically, PROPOSED differs
from GENETIC, WOA, MEWOA, DE, IPSO, AOA, MYIGWO and SAO at p \textless{}
0.0001, whereas the comparison with ACO is not statistically significant
(p \textgreater{} 0.05). In conjunction with the reported performance
metrics, these results indicate that the advantage of PROPOSED over
most baselines is highly unlikely to be due to random variation.

\subsection{Runtime comparison of the proposed method with others }

From the table of execution times across all benchmark functions and
algorithms, we observe that the proposed algorithm consistently maintains
low values and completes most tests with a relatively small computational
cost. For the simpler functions, its times remain very low, and even
for the more demanding cases, its performance stays competitive. This
suggests that the method reaches solutions without requiring an excessive
number of evaluations. Compared with the other techniques in the table,
the proposed algorithm generally shows lower execution times than
most methods. Many of the remaining algorithms exhibit higher values,
especially in more complex functions, indicating a greater computational
load. Overall, the results show that the proposed algorithm performs
well in terms of execution time and achieves a balance between speed
and effectiveness. This stable performance makes it a reliable option
among the methods that were examined.

{\tiny{}
\begin{table}
{\tiny\caption{Runtime comparison of the proposed method with others.}
}{\tiny\par}
\raggedright{}{\tiny{}%
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|}
\hline 
{\tiny\textbf{FUNCTION}} & \begin{cellvarwidth}[t]
\centering
{\tiny\textbf{PROPOSED}}{\tiny\par}

{\tiny\textbf{(TIME)}}
\end{cellvarwidth} & \begin{cellvarwidth}[t]
\centering
{\tiny\textbf{GENETIC}}{\tiny\par}

{\tiny\textbf{(TIME)}}
\end{cellvarwidth} & \begin{cellvarwidth}[t]
\centering
{\tiny\textbf{WOA}}{\tiny\par}

{\tiny\textbf{(TIME)}}
\end{cellvarwidth} & \begin{cellvarwidth}[t]
\centering
{\tiny\textbf{MEWOA}}{\tiny\par}

{\tiny\textbf{(TIME)}}
\end{cellvarwidth} & \begin{cellvarwidth}[t]
\centering
{\tiny\textbf{DE}}{\tiny\par}

{\tiny\textbf{(TIME)}}
\end{cellvarwidth} & \begin{cellvarwidth}[t]
\centering
{\tiny\textbf{IPSO}}{\tiny\par}

{\tiny\textbf{(TIME)}}
\end{cellvarwidth} & \begin{cellvarwidth}[t]
\centering
{\tiny\textbf{AOA}}{\tiny\par}

{\tiny\textbf{(TIME)}}
\end{cellvarwidth} & \begin{cellvarwidth}[t]
\centering
{\tiny\textbf{SAO}}{\tiny\par}

{\tiny\textbf{(TIME)}}
\end{cellvarwidth} & \begin{cellvarwidth}[t]
\centering
{\tiny\textbf{ACO}}{\tiny\par}

{\tiny\textbf{(TIME)}}
\end{cellvarwidth} & \begin{cellvarwidth}[t]
\centering
{\tiny\textbf{MYIGWO}}{\tiny\par}

{\tiny\textbf{(TIME)}}
\end{cellvarwidth}\tabularnewline
\hline 
\hline 
{\tiny ACKLEY} & {\tiny 0.204} & {\tiny 0.505} & {\tiny 0.331} & {\tiny 0.225} & {\tiny 0.29} & {\tiny 0.131} & {\tiny 0.163} & {\tiny 0.118} & {\tiny 1.441} & {\tiny 0.043}\tabularnewline
\hline 
{\tiny BF1} & {\tiny 0.166} & {\tiny 0.446} & {\tiny 0.162} & {\tiny 0.144} & {\tiny 0.252} & {\tiny 0.093} & {\tiny 0.137} & {\tiny 0.092} & {\tiny 1.227} & {\tiny 0.041}\tabularnewline
\hline 
{\tiny BF2} & {\tiny 0.167} & {\tiny 0.462} & {\tiny 0.168} & {\tiny 0.154} & {\tiny 0.25} & {\tiny 0.093} & {\tiny 0.123} & {\tiny 0.09} & {\tiny 1.271} & {\tiny 0.037}\tabularnewline
\hline 
{\tiny BF3} & {\tiny 0.16} & {\tiny 0.44} & {\tiny 0.362} & {\tiny 0.137} & {\tiny 0.241} & {\tiny 0.083} & {\tiny 0.125} & {\tiny 0.076} & {\tiny 1.351} & {\tiny 0.04}\tabularnewline
\hline 
{\tiny BRANIN} & {\tiny 0.158} & {\tiny 0.435} & {\tiny 0.145} & {\tiny 0.1} & {\tiny 0.226} & {\tiny 0.084} & {\tiny 0.11} & {\tiny 0.081} & {\tiny 1.399} & {\tiny 0.037}\tabularnewline
\hline 
{\tiny CAMEL} & {\tiny 0.152} & {\tiny 0.432} & {\tiny 0.121} & {\tiny 0.123} & {\tiny 0.241} & {\tiny 0.079} & {\tiny 0.096} & {\tiny 0.072} & {\tiny 0.996} & {\tiny 0.038}\tabularnewline
\hline 
\begin{cellvarwidth}[t]
\centering
{\tiny DIFFERENT}\\
{\tiny POWERS10}
\end{cellvarwidth} & {\tiny 0.371} & {\tiny 0.863} & {\tiny 0.947} & {\tiny 0.696} & {\tiny 0.697} & {\tiny 0.445} & {\tiny 1.159} & {\tiny 0.521} & {\tiny 6.77} & {\tiny 0.062}\tabularnewline
\hline 
{\tiny DIFFPOWER2} & {\tiny 0.203} & {\tiny 0.522} & {\tiny 0.226} & {\tiny 0.245} & {\tiny 0.318} & {\tiny 0.158} & {\tiny 0.242} & {\tiny 0.171} & {\tiny 1.447} & {\tiny 0.04}\tabularnewline
\hline 
{\tiny DIFFPOWER5} & {\tiny 0.794} & {\tiny 1.598} & {\tiny 2.525} & {\tiny 1.974} & {\tiny 1.549} & {\tiny 1.374} & {\tiny 2.825} & {\tiny 1.429} & {\tiny 4.826} & {\tiny 0.066}\tabularnewline
\hline 
{\tiny DIFFPOWER10} & {\tiny 3.245} & {\tiny 6.764} & {\tiny 12.213} & {\tiny 9.653} & {\tiny 7.182} & {\tiny 5.971} & {\tiny 12.431} & {\tiny 6.809} & {\tiny 7.626} & {\tiny 0.149}\tabularnewline
\hline 
{\tiny DISCUS10} & {\tiny 0.171} & {\tiny 0.471} & {\tiny 0.21} & {\tiny 0.134} & {\tiny 0.331} & {\tiny 0.114} & {\tiny 0.226} & {\tiny 0.108} & {\tiny 5.06} & {\tiny 0.05}\tabularnewline
\hline 
{\tiny EASOM} & {\tiny 0.154} & {\tiny 0.432} & {\tiny 0.101} & {\tiny 0.096} & {\tiny 0.23} & {\tiny 0.074} & {\tiny 0.078} & {\tiny 0.077} & {\tiny 0.787} & {\tiny 0.037}\tabularnewline
\hline 
{\tiny ELLIPSOIDAL10} & {\tiny 0.206} & {\tiny 0.536} & {\tiny 0.366} & {\tiny 0.211} & {\tiny 0.402} & {\tiny 0.187} & {\tiny 0.281} & {\tiny 0.152} & {\tiny 5.114} & {\tiny 0,.056}\tabularnewline
\hline 
{\tiny EQUALMAXIMA10} & {\tiny 0.705} & {\tiny 1.486} & {\tiny 3.929} & {\tiny 1.703} & {\tiny 1.413} & {\tiny 1.167} & {\tiny 1.328} & {\tiny 1.143} & {\tiny 11.159} & {\tiny 0.083}\tabularnewline
\hline 
{\tiny EXPONENTIAL10} & {\tiny 0.179} & {\tiny 0.475} & {\tiny 0.205} & {\tiny 0.149} & {\tiny 0.331} & {\tiny 0.111} & {\tiny 0.156} & {\tiny 0.112} & {\tiny 4.119} & {\tiny 0.053}\tabularnewline
\hline 
{\tiny\textbf{SUM(TIME)}} & {\tiny 7.035} & {\tiny 15.867} & {\tiny 22.011} & {\tiny 15.744} & {\tiny 13.953} & {\tiny 10.164} & {\tiny 19.48} & {\tiny 11.051} & {\tiny 54.593} & {\tiny 0.832}\tabularnewline
\hline 
\end{tabular}}{\tiny\par}
\end{table}
}{\tiny\par}

\subsection{Comparative analysis of different strategic variations}

To further evaluate the internal adaptability of the proposed optimization
framework, three distinct strategic configurations: Aggressive, Conservative,
and Mixed were examined across a comprehensive set of benchmark functions.
The total number of objective function evaluations recorded for each
configuration was 204,488 for the aggressive strategy, 212,680 for
the conservative strategy, and 209,767 for the mixed strategy. Among
these, the aggressive configuration demonstrated the lowest computational
demand, indicating faster convergence behavior. For example, the aggressive
strategy in the Easom benchmark function required 2033 evaluations
and Potential5 required 4944 evaluations. While in the conservative
strategy Easom required 2178 evaluations and Potential5 required 5,108
evaluations. On the other hand, in the mixed strategy it required
2100 and 4914 evaluations in the corresponding functions. In terms
of reliability, all three configurations demonstrated excellent success
rates, maintaining near-perfect consistency across most benchmark
functions. Specifically, the success rates were 95\% for the aggressive
strategy, 93\% for the conservative strategy, and 94\% for the mixed
configuration. The algorithm’s stable performance highlights its robustness,
as it stays reliable under different search strategies. Minor differences
mainly affect speed and computational effort.

Overall, the results show that the proposed framework is flexible
and adaptable, allowing its settings to adjust to different problems.
The Conservative version offers a good balance between accuracy and
speed, the Aggressive one works best on complex problems by avoiding
early convergence, and the Mixed approach provides a stable middle
ground. This adaptability makes the framework suitable for many optimization
tasks.

\begin{table}
\caption{Experiments using different strategy for the proposed method.}

\centering{}%
\begin{tabular}{|c|c|c|c|}
\hline 
\textbf{FUNCTION} & \textbf{AGGRESSIVE} & \textbf{CONSERVATIVE} & \textbf{MIXED}\tabularnewline
\hline 
\hline 
ACKLEY & 4971 & \textbf{4749} & 4881\tabularnewline
\hline 
BF1 & \textbf{3449} & 3541 & 3520\tabularnewline
\hline 
BF2 & \textbf{3255} & 3355 & 3291\tabularnewline
\hline 
BF3 & \textbf{3028} & 3090 & 3076\tabularnewline
\hline 
BRANIN & \textbf{2397} & 2548 & 2460\tabularnewline
\hline 
CAMEL & \textbf{2757} & 2854 & 2815\tabularnewline
\hline 
DIFFERENTPOWERS10 & \textbf{6651} & 6978 & 6803\tabularnewline
\hline 
DIFFPOWER2 & 5614 & \textbf{5451} & 5792\tabularnewline
\hline 
DIFFPOWER5 & 13836 & \textbf{13301} & 14807\tabularnewline
\hline 
DIFFPOWER10 & 19486 & 19332 & \textbf{19118}\tabularnewline
\hline 
DISCUS10 & \textbf{2185} & 2424 & 2274\tabularnewline
\hline 
EASOM & \textbf{2033} & 2178 & 2100\tabularnewline
\hline 
ELLIPSOIDAL10 & \textbf{2479} & 2717 & 2566\tabularnewline
\hline 
EQUALMAXIMA10 & 8968 & 9069 & \textbf{8717}\tabularnewline
\hline 
EXPONENTIAL10 & \textbf{2768} & 3065 & 2930\tabularnewline
\hline 
GKLS250 & \textbf{2701} & 2833 & 2769\tabularnewline
\hline 
GKLS350 & \textbf{3054} & 3583 & 3260\tabularnewline
\hline 
GOLDSTEIN & \textbf{3195} & 3243 & 3244\tabularnewline
\hline 
GRIEWANK2 & \textbf{4289(0.80)} & 4319(0.67) & 4407(0.73)\tabularnewline
\hline 
GRIEWANK10 & 5374 & 5546 & \textbf{5249(0.97)}\tabularnewline
\hline 
GRIEWANKROSENBROCK10 & \textbf{5505} & 5957 & 5806\tabularnewline
\hline 
HANSEN & 3531 & 3557(0.80) & \textbf{3288(0.93)}\tabularnewline
\hline 
HARTMAN3 & \textbf{2633} & 2850 & 2755\tabularnewline
\hline 
HARTMAN6 & \textbf{2874} & 3182 & 2971\tabularnewline
\hline 
KATSUURA10 & \textbf{8648(0.03)} & 9366(0.03) & 9109(0.03)\tabularnewline
\hline 
MICHALEWICZ4 & \textbf{3757(0.63)} & 4099(0.60) & 4404(0.60)\tabularnewline
\hline 
POTENTIAL5 & 4944 & 5108 & \textbf{4914}\tabularnewline
\hline 
POTENTIAL6 & \textbf{5887(0.37)} & 6215(0.50) & 6451(0.56)\tabularnewline
\hline 
POTENTIAL10 & 8599(0.93) & 8745(0.93) & \textbf{8305(0.93)}\tabularnewline
\hline 
RASTRIGIN & \textbf{3639} & 3660(0.93) & 3848\tabularnewline
\hline 
ROSENBROCK8 & \textbf{4964} & 5256 & 5137\tabularnewline
\hline 
ROSENBROCK16 & \textbf{6467} & 6929 & 6837\tabularnewline
\hline 
SHARPRIDGE10 & \textbf{6006} & 6287 & 6024\tabularnewline
\hline 
SHEKEL5 & \textbf{3230} & 3436 & 3317\tabularnewline
\hline 
SHEKEL7 & \textbf{3212} & 3405 & 3288\tabularnewline
\hline 
SHEKEL10 & \textbf{3168} & 3360 & 3270\tabularnewline
\hline 
SPHERE10 & \textbf{1660} & 1927 & 1791\tabularnewline
\hline 
STEPELLIPSOIDAL4 & \textbf{3018} & 3435(0.93) & 3245\tabularnewline
\hline 
SINUSOIDAL8 & \textbf{3272} & 3665 & 3499\tabularnewline
\hline 
SINUSOIDAL16 & \textbf{4454(0.93)} & 4561(0.93) & 4576(0.93)\tabularnewline
\hline 
TEST2N4 & \textbf{3236} & 3434(0.93) & 3371(0.97)\tabularnewline
\hline 
TEST2N5 & \textbf{3639(0.97)} & 3731(0.80) & 3511(0.77)\tabularnewline
\hline 
TEST30N10 & \textbf{3134} & 3590 & 3380\tabularnewline
\hline 
ZAKHAROV10 & \textbf{2521} & 2749 & 2591\tabularnewline
\hline 
\textbf{SUM} & \textbf{204488(0.95)} & \textbf{212680(0.93)} & \textbf{209767(0.94)}\tabularnewline
\hline 
\end{tabular}
\end{table}

\begin{figure}
\includegraphics[scale=0.5]{fig3}

\caption{A statistical comparison of the proposed method with different movement
strategies}

\end{figure}

Figure 3 presents the pairwise statistical comparisons between the
three exploration--exploitation control modes of the proposed optimizer.
As in the previous analysis, all comparisons were performed using
the paired Wilcoxon signed-rank test, with Holm-adjusted p-values
to account for multiple testing. Following the conventional star notation.
AGGRESSIVE vs CONSERVATIVE reaches {*}{*}{*}{*} (p \textless{} 0.0001),
indicating a very-extremely significant difference. CONSERVATIVE vs
MIXED is {*}{*} (p \textless{} 0.01), showing a highly significant
separation. MIXED vs AGGRESSIVE is {*}{*}{*} (p \textless{} 0.001),
reflecting an extremely significant but comparatively smaller gap.
Overall, the regimes induce materially different behaviors, with MIXED
lying closer to AGGRESSIVE than to CONSERVATIVE.

\subsection{Impact of Population Size on Algorithmic Performance}

To assess the influence of population size on convergence behavior
and computational efficiency, both the proposed optimization algorithm
and the Genetic Algorithm (GA) were evaluated under four different
population configurations: 50, 100, 200, and 500. The experiments
were conducted using a representative suite of benchmark functions.
In a population size of 50 individuals, both algorithms demonstrated
rapid convergence with low computational cost. The proposed algorithm
required only 1983 evaluations for the Ackley function, while BF1
required 1075 and BF2 required 983. The Genetic Algorithm converged
after 2148 evaluations for Ackley, while 1258 for BF1 and 1105 for
BF2. Despite their computational efficiency, both methods showed a
slight decline in success rate in certain benchmark functions due
to the limited population diversity. For example, the proposed algorithm
achieved success rates around 0.90 for BF2 and BF3, while the GA achieved
97\% for Ackley and 30\% for Griewank2. When the population size was
increased to 100, both algorithms reached their most balanced performance.
The proposed algorithm achieved a 100\% success rate on almost all
test functions with 3053 evaluations for Ackley, 1862 for BF1, 1734
evaluations for BF2 and 1211 for Branin. The Genetic Algorithm achieved
comparable results, also maintaining a 100\% success rate, with 3331
evaluations for Ackley, 2287 for BF1, and 1353 for Branin. This configuration
provided sufficient population diversity for effective exploration
without incurring significant computational overhead. As a result,
a medium-sized population offered the optimal trade-off between exploration
and exploitation, ensuring both robustness and computational efficiency.
Further increasing the population to 200 led to a significant increase
in computational cost for both algorithms. The proposed algorithm
required 4971 evaluations for Ackley while 3449 evaluations for BF1
and 3255 for BF2. The GA reached 5811 evaluations for Ackley and 3494
for BF3. Although the success rate slightly improved, the additional
computational effort did not yield proportional benefits in optimization
performance. When the population size was further increased to 500,
this pattern became even more apparent. The proposed algorithm required
up to 10,690 evaluations for Ackley while 8346 for the BF1 function
and 7825 for the BF2 function. The GA needed 13,664 evaluations for
Ackley, 10,866 for BF1, and 6716 for Branin. While both algorithms
maintained high success rates, the computational effort increased
drastically, indicating a prolonged and redundant exploratory phase
that hindered convergence speed and overall efficiency. The comparative
analysis reveals a consistent trend for both algorithms: population
size has a direct and significant impact on convergence dynamics and
computational efficiency. Small populations favor faster convergence
but risk premature stagnation due to limited diversity. Very large
populations improve reliability but impose excessive computational
cost. Medium-sized populations achieve the best balance between speed,
accuracy, and robustness.

Overall, the proposed algorithm consistently outperformed the Genetic
Algorithm at all population levels, requiring fewer evaluations to
reach convergence while maintaining equal or higher success rates.
This improvement is attributed to its adaptive parameter control and
dynamic balance between local and global search, which enhance convergence
efficiency without compromising precision. In summary, population
size plays a crucial role in determining the optimization efficiency
of both algorithms. For practical applications, a moderate population
configuration ensures optimal performance, achieving rapid, accurate,
and stable convergence with minimal computational overhead.
\begin{center}
{\tiny{}
\begin{table}
\begin{raggedright}
{\tiny{}%
\begin{tabular}{|c|c|c|c|c|c|c|c|c|}
\hline 
{\tiny\textbf{FUNCTION}} & {\tiny\textbf{N=50}} & {\tiny\textbf{GA(N=50)}} & {\tiny\textbf{N=100}} & {\tiny\textbf{GA(N=100)}} & {\tiny\textbf{N=200}} & {\tiny\textbf{GA(N=200)}} & {\tiny\textbf{N=500}} & {\tiny\textbf{GA(N=500)}}\tabularnewline
\hline 
\hline 
{\tiny ACKLEY} & {\tiny 1983} & {\tiny 2148(0.97)} & {\tiny 3053} & {\tiny 3331} & {\tiny 4971} & {\tiny 5811} & {\tiny 10690} & {\tiny 13664}\tabularnewline
\hline 
{\tiny BF1} & {\tiny 1075(0.97)} & {\tiny 1258} & {\tiny 1862} & {\tiny 2287} & {\tiny 3449} & {\tiny 4279} & {\tiny 8346} & {\tiny 10866}\tabularnewline
\hline 
{\tiny BF2} & {\tiny 983(0.90)} & {\tiny 1105} & {\tiny 1734} & {\tiny 2037} & {\tiny 3255} & {\tiny 3903} & {\tiny 7825} & {\tiny 9791}\tabularnewline
\hline 
{\tiny BF3} & {\tiny 885(0.90)} & {\tiny 1015} & {\tiny 1612} & {\tiny 1827} & {\tiny 3028} & {\tiny 3494} & {\tiny 7280} & {\tiny 8797}\tabularnewline
\hline 
{\tiny BRANIN} & {\tiny 627} & {\tiny 689} & {\tiny 1211} & {\tiny 1353} & {\tiny 2397} & {\tiny 2648} & {\tiny 5953} & {\tiny 6716}\tabularnewline
\hline 
{\tiny CAMEL} & {\tiny 813} & {\tiny 863} & {\tiny 1445} & {\tiny 1638} & {\tiny 2757} & {\tiny 3104} & {\tiny 6820} & {\tiny 7910}\tabularnewline
\hline 
\begin{cellvarwidth}[t]
\centering
{\tiny DIFFERENT}{\tiny\par}

{\tiny POWERS10}
\end{cellvarwidth} & {\tiny 2108} & {\tiny 2711} & {\tiny 3534} & {\tiny 5334} & {\tiny 6651} & {\tiny 10954} & {\tiny 15520} & {\tiny 25428}\tabularnewline
\hline 
{\tiny DIFFPOWER2} & {\tiny 1519} & {\tiny 2427} & {\tiny 2888} & {\tiny 4700} & {\tiny 5614} & {\tiny 9053} & {\tiny 13599} & {\tiny 22630}\tabularnewline
\hline 
{\tiny DIFFPOWER5} & {\tiny 3679} & {\tiny 6279} & {\tiny 6454} & {\tiny 12695} & {\tiny 13836} & {\tiny 24466} & {\tiny 39269} & {\tiny 66396}\tabularnewline
\hline 
{\tiny DIFFPOWER10} & {\tiny 4908} & {\tiny 9505} & {\tiny 9758} & {\tiny 19182} & {\tiny 19486} & {\tiny 37664} & {\tiny 47689} & {\tiny 91211}\tabularnewline
\hline 
{\tiny DISCUS10} & {\tiny 562} & {\tiny 675} & {\tiny 1089} & {\tiny 1364} & {\tiny 2185} & {\tiny 2711} & {\tiny 5435} & {\tiny 6632}\tabularnewline
\hline 
{\tiny EASOM} & {\tiny 533} & {\tiny 528} & {\tiny 1029} & {\tiny 1041} & {\tiny 2033} & {\tiny 2062} & {\tiny 5084} & {\tiny 5188}\tabularnewline
\hline 
{\tiny ELLIPSOIDAL10} & {\tiny 641} & {\tiny 820} & {\tiny 1239} & {\tiny 1662} & {\tiny 2479} & {\tiny 3309} & {\tiny 6170} & {\tiny 8099}\tabularnewline
\hline 
{\tiny EQUALMAXIMA10} & {\tiny 2272} & {\tiny 4025} & {\tiny 4373} & {\tiny 8329} & {\tiny 8968} & {\tiny 16399} & {\tiny 22053} & {\tiny 39624}\tabularnewline
\hline 
{\tiny EXPONENTIAL10} & {\tiny 768} & {\tiny 958} & {\tiny 1399} & {\tiny 1939} & {\tiny 2768} & {\tiny 3854} & {\tiny 6832} & {\tiny 9420}\tabularnewline
\hline 
{\tiny GKLS250} & {\tiny 861} & {\tiny 751} & {\tiny 1410} & {\tiny 1328} & {\tiny 2701} & {\tiny 2579} & {\tiny 5999} & {\tiny 5995}\tabularnewline
\hline 
{\tiny GKLS350} & {\tiny 1068(0.97)} & {\tiny 888(0.97)} & {\tiny 1836} & {\tiny 1663} & {\tiny 3054} & {\tiny 3076} & {\tiny 6478} & {\tiny 5938}\tabularnewline
\hline 
{\tiny GOLDSTEIN} & {\tiny 932} & {\tiny 1086} & {\tiny 1623} & {\tiny 1979} & {\tiny 3195} & {\tiny 3802} & {\tiny 7762} & {\tiny 9688}\tabularnewline
\hline 
{\tiny GRIEWANK2} & {\tiny 1509(0.67)} & {\tiny 1378(0.30)} & {\tiny 3092(0.73)} & {\tiny 2557(0.57)} & {\tiny 4289(0.80)} & {\tiny 4313(0.73)} & {\tiny 10040(0.90)} & {\tiny 10882(0.93)}\tabularnewline
\hline 
{\tiny GRIEWANK10} & {\tiny 1806(0.83)} & {\tiny 2610} & {\tiny 3139(0.97)} & {\tiny 4729} & {\tiny 5374} & {\tiny 8381} & {\tiny 12575} & {\tiny 19950}\tabularnewline
\hline 
\begin{cellvarwidth}[t]
\centering
{\tiny GRIEWANK}{\tiny\par}

{\tiny ROSENBROCK10}
\end{cellvarwidth} & {\tiny 1467} & {\tiny 2334} & {\tiny 2730} & {\tiny 4745} & {\tiny 5505} & {\tiny 9576} & {\tiny 13676} & {\tiny 23160}\tabularnewline
\hline 
{\tiny HANSEN} & {\tiny 1048(0.87)} & {\tiny 1156(0.83)} & {\tiny 1876(0.93)} & {\tiny 2007(0.90)} & {\tiny 3531} & {\tiny 3345} & {\tiny 7224} & {\tiny 8015}\tabularnewline
\hline 
{\tiny HARTMAN3} & {\tiny 753} & {\tiny 838} & {\tiny 1327} & {\tiny 1652} & {\tiny 2633} & {\tiny 3142} & {\tiny 6598} & {\tiny 8008}\tabularnewline
\hline 
{\tiny HARTMAN6} & {\tiny 790} & {\tiny 1012} & {\tiny 1512} & {\tiny 2025} & {\tiny 2874} & {\tiny 3941} & {\tiny 7010} & {\tiny 9779}\tabularnewline
\hline 
{\tiny KATSUURA10} & {\tiny 2591(0.03)} & {\tiny 5212(0.03)} & {\tiny 4882(0.03)} & {\tiny 10239(0.03)} & {\tiny 8648(0.03)} & {\tiny 19539(0.03)} & {\tiny 26799(0.03)} & {\tiny 51916(0.03)}\tabularnewline
\hline 
{\tiny MICHALEWICZ4} & {\tiny 1230(0.37)} & {\tiny 1482(0.40)} & {\tiny 2142(0.53)} & {\tiny 3154(0.80)} & {\tiny 3757(0.63)} & {\tiny 5906(0.90)} & {\tiny 9094(0.93)} & {\tiny 12347(0.90)}\tabularnewline
\hline 
{\tiny POTENTIAL5} & {\tiny 1205} & {\tiny 1984} & {\tiny 2523} & {\tiny 4283} & {\tiny 4944} & {\tiny 8166} & {\tiny 11784} & {\tiny 20239}\tabularnewline
\hline 
{\tiny POTENTIAL6} & {\tiny 1514(0.13)} & {\tiny 2732(0.27)} & {\tiny 2881(0.23)} & {\tiny 5617(0.50)} & {\tiny 5887(0.37)} & {\tiny 12034(0.80)} & {\tiny 15758(0.83)} & {\tiny 27537(0.97)}\tabularnewline
\hline 
{\tiny POTENTIAL10} & {\tiny 2527(0.47)} & {\tiny 4956(0.57)} & {\tiny 4958(0.77)} & {\tiny 8409(0.93)} & {\tiny 8599(0.93)} & {\tiny 15417} & {\tiny 19233} & {\tiny 34576}\tabularnewline
\hline 
{\tiny RASTRIGIN} & {\tiny 1390(0.73)} & {\tiny 1260(0.90)} & {\tiny 2499(0.93)} & {\tiny 2323(0.87)} & {\tiny 3639} & {\tiny 3840(0.93)} & {\tiny 7904} & {\tiny 8761(0.93)}\tabularnewline
\hline 
{\tiny ROSENBROCK8} & {\tiny 1396} & {\tiny 2122} & {\tiny 2484} & {\tiny 4164} & {\tiny 4964} & {\tiny 8085} & {\tiny 12581} & {\tiny 20176}\tabularnewline
\hline 
{\tiny ROSENBROCK16} & {\tiny 1861} & {\tiny 3236} & {\tiny 3316} & {\tiny 6133} & {\tiny 6467} & {\tiny 11469} & {\tiny 16465} & {\tiny 29665}\tabularnewline
\hline 
{\tiny SHARPRIDGE10} & {\tiny 1601} & {\tiny 2580} & {\tiny 2940} & {\tiny 5194} & {\tiny 6006} & {\tiny 10297} & {\tiny 15017} & {\tiny 25314}\tabularnewline
\hline 
{\tiny SHEKEL5} & {\tiny 884} & {\tiny 1100} & {\tiny 1611} & {\tiny 2261} & {\tiny 3230} & {\tiny 4540} & {\tiny 8010} & {\tiny 10847}\tabularnewline
\hline 
{\tiny SHEKEL7} & {\tiny 902} & {\tiny 1110} & {\tiny 1688} & {\tiny 2279} & {\tiny 3212} & {\tiny 4517} & {\tiny 8052} & {\tiny 10817}\tabularnewline
\hline 
{\tiny SHEKEL10} & {\tiny 953} & {\tiny 1115} & {\tiny 1607} & {\tiny 2217} & {\tiny 3168} & {\tiny 4266} & {\tiny 7895} & {\tiny 10506}\tabularnewline
\hline 
{\tiny SPHERE10} & {\tiny 434} & {\tiny 430} & {\tiny 831} & {\tiny 855} & {\tiny 1660} & {\tiny 1706} & {\tiny 4132} & {\tiny 4238}\tabularnewline
\hline 
\begin{cellvarwidth}[t]
\centering
{\tiny STEP}\\
{\tiny ELLIPSOIDAL4}
\end{cellvarwidth} & {\tiny 922(0.97)} & {\tiny 629(0.13)} & {\tiny 1700} & {\tiny 1392(0.50)} & {\tiny 3018} & {\tiny 2842(0.83)} & {\tiny 6528} & {\tiny 6095(0.80)}\tabularnewline
\hline 
{\tiny SINUSOIDAL8} & {\tiny 1002(0.93)} & {\tiny 1300} & {\tiny 1877} & {\tiny 2443} & {\tiny 3272} & {\tiny 4572} & {\tiny 7623} & {\tiny 10946}\tabularnewline
\hline 
{\tiny SINUSOIDAL16} & {\tiny 1432(0.63)} & {\tiny 2225(0.83)} & {\tiny 2498(0.80)} & {\tiny 4038(0.97)} & {\tiny 4454(0.93)} & {\tiny 6588} & {\tiny 10649} & {\tiny 16327}\tabularnewline
\hline 
{\tiny TEST2N4} & {\tiny 959(0.73)} & {\tiny 1033(0.63)} & {\tiny 1557(0.87)} & {\tiny 1969(0.93)} & {\tiny 3236} & {\tiny 3878} & {\tiny 6856} & {\tiny 8420}\tabularnewline
\hline 
{\tiny TEST2N5} & {\tiny 932(0.40)} & {\tiny 1105(0.53)} & {\tiny 1651(0.63)} & {\tiny 2196(0.77)} & {\tiny 3639(0.97)} & {\tiny 4109(0.86)} & {\tiny 7659} & {\tiny 9566(0.86)}\tabularnewline
\hline 
{\tiny TEST30N10} & {\tiny 935} & {\tiny 1214} & {\tiny 1552} & {\tiny 2818} & {\tiny 3134} & {\tiny 5238} & {\tiny 8436} & {\tiny 13206}\tabularnewline
\hline 
{\tiny ZAKHAROV10} & {\tiny 645} & {\tiny 811} & {\tiny 1250} & {\tiny 1670} & {\tiny 2521} & {\tiny 3314} & {\tiny 6234} & {\tiny 8102}\tabularnewline
\hline 
{\tiny\textbf{SUM}} & {\tiny\textbf{58905(0.88)}} & {\tiny\textbf{84695(0.87)}} & {\tiny\textbf{107672(0.92)}} & {\tiny\textbf{165058(0.93)}} & {\tiny\textbf{204488(0.95)}} & {\tiny\textbf{316189(0.96)}} & {\tiny\textbf{502636(0.97)}} & {\tiny\textbf{773388(0.96)}}\tabularnewline
\hline 
\end{tabular}}{\tiny\par}
\par\end{raggedright}
{\tiny\caption{Experiment different number of population (proposed method and Genetic
Algorithm)}
}{\tiny\par}
\end{table}
}{\tiny\par}
\par\end{center}

\subsection{Convergence Analysis}

The convergence analysis of the MFO algorithm on the Ackley and Katsuura
functions shows how differently the method behaves across two distinct
optimization landscapes. For the Ackley function, the algorithm starts
with a G\_best of 2.650 in the first generation. From the second generation
onward, the value drops straight to zero and stays there. This means
MFO quickly finds an excellent solution and remains stable without
any fluctuations afterward, showing fast convergence and consistent
performance.

\begin{figure}
\includegraphics[scale=0.15]{Ackley}

\caption{Comparing G\_best by Generation (Ackley)}

\end{figure}

In contrast, the Katsuura function presents a tougher, highly irregular
landscape. The first generation shows a high G\_best value, which
makes sense given the complexity of the terrain. Between the first
and second generation, the algorithm makes a sharp improvement, escaping
poor initial regions. From the third generation onward, the convergence
stabilizes around 0.016451 with no noticeable oscillations.

\begin{figure}

\includegraphics[scale=0.15]{Katsuura}\caption{Comparing G\_best by Generation (Katsuura)}

\end{figure}

This stability indicates that MFO identifies a promising region and
focuses on it without unnecessary over-exploration. Overall, in both
benchmarks, MFO demonstrates strong early exploration, rapid improvement,
and stable late-stage convergence. The comparison highlights the algorithm’s
flexibility and robustness across both smooth and highly rugged multimodal
search spaces, confirming its reliability in challenging global optimization
tasks.

\subsection{Friedman Test }

The results of the Friedman test clearly show that there are significant
differences between the optimization algorithms we compared. The p-value
was tiny, which means that these differences are not due to chance
but reflect real performance variations. The post-hoc analysis helped
identify which pairs of algorithms differ significantly. In most cases,
the differences were statistically significant, while only a few algorithm
pairs showed no meaningful difference. Overall, the proposed algorithm
performed better than the others and consistently achieved the strongest
results. Based on these findings, the statistical analysis confirms
the effectiveness of the proposed method and shows that it consistently
outperforms several well-known optimization techniques.

\begin{figure}
\includegraphics[scale=0.5]{Fridman}

\caption{A statistical comparison of the proposed with other optimization methods}

\end{figure}


\subsection{Performance Evaluation on Practical Problems}

To further examine the practical efficiency and scalability of the
proposed optimization algorithm, two real-world engineering design
problems were investigated: the GasCycle\citep{key-8} and the Tandem
Queueing System\citep{key-6}. These problems were selected because
they differ significantly in mathematical formulation and computational
complexity, providing a comprehensive framework for evaluating the
algorithm’s performance under diverse and realistic conditions.

Each problem was tested across multiple dimensional configurations,
ranging from 40 to 400 variables, in order to assess how the algorithm
behaves as the search space becomes more complex. For every configuration,
the execution time in seconds was recorded as the main performance
indicator. This experimental setup enables a direct comparison of
how computational efficiency changes with increasing dimensionality.
\begin{itemize}
\item \textbf{GasCycle Thermal Cycle}

Vars: $\bm{x}=[T_{1},T_{3},P_{1},P_{3}]^{\top}$. \quad{}$r=P_{3}/P_{1},\ \gamma=1.4.$
\[
\eta(\bm{x})=1-r^{-(\gamma-1)/\gamma}\,\frac{T_{1}}{T_{3}},\qquad\min_{\bm{x}}f(\bm{x})=-\eta(\bm{x}).
\]
Bounds: $300\!\le\!T_{1}\!\le\!1500,\;1200\!\le\!T_{3}\!\le\!2000,\;1\!\le\!P_{1},P_{3}\!\le\!20.$

Penalty: infeasible $\Rightarrow f=10^{20}$.
\end{itemize}
The GasCycle problem represents a thermodynamic optimization task
involving nonlinear relationships and interdependent design parameters.
As the dimensionality of the problem increases, the algorithm exhibits
a smooth and predictable increase in computational effort. Specifically,
the execution time increases from 0.449 s (dimension 40) to 7.511
s (dimension 400).

\begin{figure}
\includegraphics[scale=0.15]{GasCycle}

\caption{Comparing times with different dimensions( GasCycle)}
\end{figure}

The graph illustrates a clear, nearly linear increase in execution
time as the problem’s dimensionality grows from 40 to 400 variables.
This trend indicates that the algorithm scales smoothly, maintaining
computational stability while handling larger search spaces. The consistent
growth in time reflects the efficiency of the adaptive mechanism,
which balances the computational effort across dimensions without
abrupt fluctuations.
\begin{itemize}
\item \textbf{Tandem Space Trajectory} (MGA-1DSM, EVEEJ + 2$\times$Saturn)

Vars\textbf{ ($D{=}18$):} $\bm{x}=[t_{0},T_{1},T_{2},T_{3},T_{4},T_{5A},T_{5B},s_{1},s_{2},s_{3},s_{4},s_{5A},s_{5B},r_{p},k_{A1},k_{A2},k_{B1},k_{B2}]^{\top}$.
\[
\begin{aligned} & 7000\le t_{0}\le10000,\\
 & 30\le T_{1}\le500,\;30\le T_{2}\le600,\;30\le T_{3}\le1200,\\
 & 30\le T_{4}\le1600,\;30\le T_{5A},T_{5B}\le2000,\\
 & 0\le s_{1..4},s_{5A},s_{5B},r_{p},k_{A1},k_{A2},k_{B1},k_{B2}\le1.
\end{aligned}
\]

Objective: 
\[
\min_{\bm{x}}\ \Delta V_{\text{tot}}=\Delta V_{\text{launch}}(T_{1})+\Delta V_{\text{legs}}(T_{1}{:}T_{4})+\Delta V_{A}+\Delta V_{B}+\Delta V_{\text{DSM}}(\bm{s},r_{p})-G_{\text{GA}}-G_{J}+P_{\text{hard}}+P_{\text{soft}},
\]
\medskip{}
 
\[
P_{\text{soft}}=\beta\max\!\Big\{0,\ (T_{1}{+}\cdots{+}T_{4}+\tfrac{1}{2}(T_{5A}{+}T_{5B}))-3500\Big\}.
\]
Notes: $\Delta V_{\text{launch}}$ decreases (log-like) in $T_{1}$
($\ge6$ km/s floor), leg/branch costs decrease with TOF.

\medskip{}

\end{itemize}
The Tandem Queueing System is a more demanding case, characterized
by strong interdependencies among variables and a denser set of constraints.
As the dimensionality of the problem increases, the algorithm exhibits
a smooth and predictable increase in computational effort. Specifically,
the execution time increases from 1.746 s (dimension 40) to 54.123
s (dimension 400).

\begin{figure}
\includegraphics[scale=0.15]{Tandem}

\caption{Comparing times with different dimensions( Tandem)}
\end{figure}

The graph shows a steeper increase in execution time compared to the
GasCycle case, reflecting the higher complexity of the Tandem problem.
Despite this, the algorithm maintains stable and efficient performance.

The results from both test problems show that the proposed algorithm
maintains a good balance between accuracy and computational cost.
The execution time remains within reasonable limits even as the problem
size increases. Overall, the algorithm proves to be adaptable and
scalable, demonstrating consistent performance across different problems.

\section{Discussion\label{sec:Discussion}}

The experimental analysis conducted in this study offers a comprehensive
evaluation of the proposed optimization framework across a wide range
of benchmark functions and parameter configurations. The results provide
clear evidence of the algorithm’s strong convergence behavior, computational
efficiency, and robustness when compared with several state-of-the-art
metaheuristics. The performance was assessed on both unimodal and
multimodal landscapes, ensuring that the conclusions drawn reflect
diverse and challenging optimization scenarios. Across all benchmark
functions, the proposed method demonstrated consistently stable and
competitive convergence dynamics. In comparison with well-established
algorithms such as Genetic Algorithm, WOA, MEWOA, Differential Evolution,
iPSO, AOA, SAO, and ACO, the proposed variant achieved the lowest
number of function evaluations in nearly every test case, while maintaining
a 100\% success rate in most functions. These findings underscore
the algorithm’s capacity to reach high-quality solutions efficiently
and reliably, reflecting its well-balanced interplay between global
exploration and local exploitation.

The statistical analysis further reinforces these conclusions. Pairwise
hypothesis testing revealed that the differences between the proposed
method and nearly all baseline algorithms were highly significant,
with the majority of comparisons yielding p-values below 0.0001. This
confirms that the performance improvements observed are not attributable
to stochastic variability. Additionally, comparisons among the three
exploration--exploitation control modes revealed statistically significant
behavioral differences, validating the distinct roles and contributions
of each strategy within the overall search dynamic.

A deeper examination of the internal movement strategies \textbf{Aggressive,
Conservative, and Mixed }provides insight into the adaptability of
the proposed framework. The Conservative mode offered an efficient
compromise between computational effort and solution precision, while
the Aggressive mode performed particularly well in highly multimodal
landscapes by enabling rapid escape from deceptive regions. The Mixed
strategy, which probabilistically alternates between aggressive and
conservative updates, produced consistently reliable results across
a wide variety of problems. These observations highlight the algorithm’s
ability to adjust its search behavior according to landscape characteristics.
The effectiveness of each strategy is closely linked to the way it
interacts with the underlying fitness landscape. The Aggressive strategy
benefits problems with many local minima by promoting large stochastic
jumps and reducing stagnation, whereas the Conservative strategy is
more suitable for unimodal or smoothly varying functions, where smaller
directed updates accelerate convergence. The Mixed strategy is advantageous
in complex or hybrid landscapes, where a dynamic balance between exploration
and exploitation is essential. Overall, the reduction in function
evaluations arises from the synergy among these strategies: aggressive
exploration enables rapid identification of promising regions, conservative
refinement improves them efficiently, and the mixed behavior prevents
premature convergence. As a result, computational effort is concentrated
where it is most impactful, enhancing the overall convergence speed.
The differences among the three strategies become more evident when
examining the mathematical update rules that govern their behavior.
The \textbf{Aggressive strategy }is defined by: $[y_{j}=P_{i,j}+2(r-0.5)\left(SS_{j}-IP_{i,j}\right),]$
with $(r\in[0,1])$. The scaling term $2(r-0.5)$ allows large bidirectional
steps along the vector $((SS_{j}-IP_{i,j}))$, thereby enhancing global
exploration and facilitating escape from deceptive or narrow local
minima. The \textbf{Conservative strategy} uses the update:$[y_{j}=P_{i,j}+0.3(r-0.5)\left(x_{best,j}-P_{i,j}\right),]$
where the reduced coefficient $0.3(r-0.5)$constrains the movement
to small, stable steps directed toward the current best solution.
This promotes exploitation and ensures efficient local refinement
in relatively smooth or unimodal landscapes. The Mixed strategy alternates
between the aggressive and conservative updates with equal probability,
effectively combining large exploratory jumps with fine-grained exploitation.
This probabilistic mechanism prevents stagnation while maintaining
convergence pressure when promising regions are detected. Together,
these updated equations explain the behavioral distinctions of the
three strategies: the aggressive formulation increases variance and
encourages broader exploration, the conservative one focuses the search
around high-quality regions, and the mixed approach dynamically balances
both tendencies. This mathematical perspective clarifies how the structure
of each movement rule contributes to the overall adaptability and
robustness of the proposed algorithm.

An additional avenue for future research involves introducing stochastic
selection of movement strategies either on a per-individual or per-iteration
basis. Random assignment of Aggressive, Conservative, or Mixed behavior
across individuals may increase behavioral diversity and strengthen
global exploration, whereas stochastic switching at each iteration
could create a time-varying exploration--exploitation balance. Although
such mechanisms were not explored in the present study, they represent
promising directions for enhancing adaptability and reducing stagnation
in highly deceptive or heterogeneous landscapes.

The impact of population size was also investigated for both the proposed
method and the standard Genetic Algorithm. In both cases, population
size played a critical role in shaping convergence speed and computational
cost. Smaller populations converged rapidly but occasionally suffered
from reduced diversity, while excessively large populations introduced
redundant evaluations. Optimal performance was observed for medium-sized
populations, where a well-balanced exploration--exploitation trade-off
was maintained. When directly compared under identical conditions,
the proposed algorithm consistently outperformed the Genetic Algorithm,
requiring fewer evaluations to achieve convergence without sacrificing
success rate.

In summary, the experimental results confirm that the proposed optimization
framework achieves an effective balance between computational efficiency,
convergence accuracy, and robustness. It adapts well to diverse problem
structures and parameter settings, consistently outperforming classical
evolutionary and swarm-based approaches. Its combination of fast convergence,
stable behavior, and high reliability underscores its potential as
a powerful, scalable, and versatile tool for real-world optimization
applications.

\section{Conclusions\label{sec:Conclusions}}

This study introduced a novel optimization framework that integrates
an adaptive mechanism to maintain an effective balance between exploration
and exploitation throughout the search process. The experimental evaluation,
conducted across a comprehensive suite of benchmark functions, demonstrated
that the proposed algorithm consistently outperforms several well-established
metaheuristics in terms of convergence accuracy, computational speed,
and overall efficiency. Furthermore, statistical analyses confirmed
that these improvements are statistically significant and not attributable
to random variability. The investigation of the internal search strategies
Aggressive, Conservative, and Mixed also revealed the algorithm’s
capacity to adjust its search dynamics in accordance with the structural
characteristics and difficulty of the optimization problem. This adaptability
promotes a well-calibrated interaction between global exploration
and local exploitation, ultimately contributing to stable and efficient
convergence.

Despite its strong empirical performance, the enhanced MFO variant
presents certain limitations that merit consideration. As a population-based
stochastic optimizer may incur substantial computational cost in high-dimensional
or noisy scenarios, where maintaining population diversity becomes
increasingly challenging and the integrated local search component
adds to the per-iteration overhead. Moreover, the three movement strategies
exhibit landscape-dependent behavior: aggressive steps may overshoot
narrow basins of attraction, whereas conservative steps may lead to
premature convergence in highly multimodal terrains. These observations
highlight several promising research directions, including adaptive
regime selection, dimensionality-reduction mechanisms, surrogate-assisted
extensions, and more rigorous theoretical convergence studies.

A promising direction for future research is to explore how the proposed
framework could be combined with other well-established metaheuristic
algorithms, such as Differential Evolution or Particle Swarm Optimization.
Such hybridization could leverage the strengths of different search
strategies and potentially lead to more effective optimization performance.
In addition, applying the improved MFO to large-scale and high-dimensional
optimization problems would provide a more complete assessment of
its scalability, robustness, and practical usefulness in real-world
scenarios where computational demands are considerably higher.

Overall, the proposed algorithm constitutes a flexible and powerful
optimization approach capable of addressing both simple and complex
real-world problems. Its consistently superior performance, supported
by statistical validation, underscores its potential for applications
requiring high precision, adaptability, and computational efficiency.\\


\authorcontributions{\textbf{Author Contributions:} G.K., V.C. and I.G.T. conceived of
the idea and the methodology, and G.K. and V.C. implemented the corresponding
software. G.K. conducted the experiments, employing objective functions
as test cases, and provided the comparative experiments. I.G.T. performed
the necessary statistical tests. All authors have read and agreed
to the published version of the manuscript.}


\funding{This research has been financed by the European Union: Next Generation
EU through the Program Greece 2.0 National Recovery and Resilience
Plan, under the call RESEARCH--CREATE--INNOVATE, project name “iCREW:
Intelligent small craft simulator for advanced crew training using
Virtual Reality techniques” (project code: TAEDK-06195).}

\institutionalreview{Not Applicable.}

\informedconsent{Not Applicable.}

\dataavailability{Not Applicable.}

\conflictsofinterest{The authors declare no conflicts of interest.}

\appendix

\begin{adjustwidth}{-\extralength}{0cm}{}


\reftitle{References}
\begin{thebibliography}{999}
\bibitem[(1999)]{go1}A. Törn, M.M. Ali, S. Viitanen, Stochastic global
optimization: Problem classes and solution techniques. Journal of
Global Optimization \textbf{14}, pp. 437-447, 1999.

\bibitem[(1999)]{go2}Floudas, C. A., \& Pardalos, P. M. (Eds.). (2013).
State of the art in global optimization: computational methods and
applications

\bibitem[(1999)]{go3}Horst, R., \& Pardalos, P. M. (Eds.). (2013).
Handbook of global optimization (Vol. 2). Springer Science \& Business
Media.

\bibitem{Powell}Powell, M. J. D. (1989). A tolerant algorithm for
linearly constrained optimization calculations. Mathematical Programming,
45(1), 547-566.

\bibitem{stopping rule2}Charilogis, V., \& Tsoulos, I. G. (2022).
Toward an ideal particle swarm optimizer for multidimensional functions.
Information, 13(5), 217.

\bibitem[Author1(year)]{go_math1} ACarrizosa, E., Molero-Río, C.,
\& Romero Morales, D. (2021). Mathematical optimization in classification
and regression trees. Top, 29(1), 5-33.

\bibitem{go_math3}Legat, B., Dowson, O., Garcia, J. D., \& Lubin,
M. (2022). MathOptInterface: a data structure for mathematical optimization
problems. INFORMS Journal on Computing, 34(2), 672-689.

\bibitem{go_physics2}Su, H., Zhao, D., Heidari, A. A., Liu, L., Zhang,
X., Mafarja, M., \& Chen, H. (2023). RIME: A physics-based optimization.
Neurocomputing, 532, 183-214.

\bibitem{go_physics3}Stilck França, D., \& Garcia-Patron, R. (2021).
Limitations of optimization algorithms on noisy quantum devices. Nature
Physics, 17(11), 1221-1227.

\bibitem{go_chem1}Zhang, J., \& Glezakou, V. A. (2021). Global optimization
of chemical cluster structures: Methods, applications, and challenges.
International Journal of Quantum Chemistry, 121(7), e26553.

\bibitem{go_chem2}Hu, Y., Zang, Z., Chen, D., Ma, X., Liang, Y.,
You, W., \& Zhang, Z. (2022). Optimization and evaluation of SO2 emissions
based on WRF-Chem and 3DVAR data assimilation. Remote Sensing, 14(1),
220.

\bibitem{go_med2}Kaur, P., \& Singh, R. K. (2023). A review on optimization
techniques for medical image analysis. Concurrency and Computation:
Practice and Experience, 35(1), e7443.

\bibitem{medicine}Houssein, E. H., Hosney, M. E., Mohamed, W. M.,
Ali, A. A., \& Younis, E. M. (2023). Fuzzy-based hunger games search
algorithm for global optimization and feature selection using medical
data. Neural Computing and Applications, 35(7), 5251-5275.

\bibitem{key-1}Wang, Y., Ma, Y., Song, F., Ma, Y., Qi, C., Huang,
F., ... \& Zhang, F. (2020). Economic and efficient multi-objective
operation optimization of integrated energy system considering electro-thermal
demand response. Energy, 205, 118022.

\bibitem{go_econ2}Alirahmi, S. M., Mousavi, S. B., Razmi, A. R.,
\& Ahmadi, P. (2021). A comprehensive techno-economic analysis and
multi-criteria optimization of a compressed air energy storage (CAES)
hybridized with solar and desalination units. Energy Conversion and
Management, 236, 114053.

\bibitem{key-2}Shezan, S. A., Ishraque, M. F., Shafiullah, G. M.,
Kamwa, I., Paul, L. C., Muyeen, S. M., ... \& Kumar, P. P. (2023).
Optimization and control of solar-wind islanded hybrid microgrid by
using heuristic and deterministic optimization algorithms and fuzzy
logic controller. Energy reports, 10, 3272-3288.

\bibitem{go_determ3}Xu, Z., Zhao, Z., \& Liu, J. (2024). Deterministic
Multi-Objective Optimization of Analog Circuits. Electronics, 13(13),
2510.

\bibitem{stohastic}Hsieh, Y. P., Karimi Jaghargh, M. R., Krause,
A., \& Mertikopoulos, P. (2024). Riemannian stochastic optimization
methods avoid strict saddle points. Advances in Neural Information
Processing Systems, 36.

\bibitem{stohastic1}Tyurin, A., \& Richtárik, P. (2024). Optimal
time complexities of parallel stochastic optimization methods under
a fixed computation model. Advances in Neural Information Processing
Systems, 36.

\bibitem{crs1}Price, W. (1983). Global optimization by controlled
random search. Journal of optimization theory and applications, 40(3),
333-348.

\bibitem{crs2}Křivý, I., \& Tvrdik, J. (1995). The controlled random
search algorithm in optimizing regression models. Computational statistics
\& data analysis, 20(2), 229-234.

\bibitem{simann_major}Kirkpatrick, S., Gelatt Jr, C. D., \& Vecchi,
M. P. (1983). Optimization by simulated annealing. science, 220(4598),
671-680.

\bibitem{simann1}Ingber, L. (1989). Very fast simulated re-annealing.
Mathematical and computer modelling, 12(8), 967-973.

\bibitem{mult_minfinder}Tsoulos, I. G., \& Lagaris, I. E. (2006).
MinFinder: Locating all the local minima of a function. Computer Physics
Communications, 174(2), 166-179.

\bibitem{mult_cfo}Liu, Y., \& Tian, P. (2015). A multi-start central
force optimization for global optimization. Applied Soft Computing,
27, 92-98.

\bibitem{key-3}Sohail, A. (2023). Genetic algorithms in the fields
of artificial intelligence and data sciences. Annals of Data Science,
10(4), 1007-1018.

\bibitem{genetic3}Charilogis, V., Tsoulos, I. G., \& Stavrou, V.
N. (2023). An Intelligent Technique for Initial Distribution of Genetic
Algorithms. Axioms, 12(10), 980.

\bibitem{key-9}Yuan, F., Huang, X., Zheng, L., Wang, L., Wang, Y.,
Yan, X., ... \& Peng, Y. (2025). The evolution and optimization strategies
of a PBFT consensus algorithm for consortium blockchains. Information,
16(4), 268.

\bibitem{key-10}Shir, O. M., \& Emmerich, M. (2025, July). Correlated
geometric mutations for integer evolution strategies. In Proceedings
of the Genetic and Evolutionary Computation Conference Companion (pp.
1825-1832).

\bibitem{key-11}Vanneschi, L., Farinati, D., Rasteiro, D., Rosenfeld,
L., Pietropolli, G., \& Silva, S. (2025). Exploring non-bloating geometric
semantic genetic programming. Genetic Programming Theory and Practice
XXI, 237-258.

\bibitem{key-12}Maurya, P., Kushwaha, A., \& Prakash, O. (2025).
Medical Data Classification Using Genetic Programming: A Systematic
Literature Review. Expert Systems, 42(3), e70007.

\bibitem{diffe1}Deng, W., Shang, S., Cai, X., Zhao, H., Song, Y.,
\& Xu, J. (2021). An improved differential evolution algorithm and
its application in optimization problem. Soft Computing, 25, 5277-5298.

\bibitem{diffe2}Pant, M., Zaheer, H., Garcia-Hernandez, L., \& Abraham,
A. (2020). Differential Evolution: A review of more than two decades
of research. Engineering Applications of Artificial Intelligence,
90, 103479.

\bibitem{pso_major}Shami, T. M., El-Saleh, A. A., Alswaitti, M.,
Al-Tashi, Q., Summakieh, M. A., \& Mirjalili, S. (2022). Particle
swarm optimization: A comprehensive survey. Ieee Access, 10, 10031-10061.

\bibitem{pso1}Gad, A. G. (2022). Particle swarm optimization algorithm
and its applications: a systematic review. Archives of computational
methods in engineering, 29(5), 2531-2561. 

\bibitem{aco1}Rokbani, N., Kumar, R., Abraham, A., Alimi, A. M.,
Long, H. V., Priyadarshini, I., \& Son, L. H. (2021). Bi-heuristic
ant colony optimization-based approaches for traveling salesman problem.
Soft Computing, 25, 3775-3794.

\bibitem{aco2}Wu, L., Huang, X., Cui, J., Liu, C., \& Xiao, W. (2023).
Modified adaptive ant colony optimization algorithm and its application
for solving path planning of mobile robot. Expert Systems with Applications,
215, 119410.

\bibitem{key-16}Hamadneh, T., Kaabneh, K., AbuFalahah, I., Bektemyssova,
G., Shaikemelev, G., Umutkulov, D., ... \& Dehghani, M. (2024). Magnificent
frigatebird optimization: a new bio-inspired metaheuristic approach
for solving optimization problems. Comput. Mater. Contin, 80, 2721-2741.

\bibitem{key-25}Makhadmeh, S. N., Al-Betar, M. A., Doush, I. A.,
Awadallah, M. A., Kassaymeh, S., Mirjalili, S., \& Zitar, R. A. (2023).
Recent advances in Grey Wolf Optimizer, its versions and applications.
Ieee Access, 12, 22991-23028.

\bibitem{key-22}Alyasseri, Z. A. A., Ali, N. S., Al-Betar, M. A.,
Makhadmeh, S. N., Jamil, N., Awadallah, M. A., ... \& Mirjalili, S.
(2024). Recent advances of whale optimization algorithm, its versions
and applications. Handbook of whale optimization algorithm, 9-31.

\bibitem{testfunctions1}Ali, M. M., \& Kaelo, P. (2008). Improved
particle swarm algorithms for global optimization. Applied mathematics
and computation, 196(2), 578-593.

\bibitem{testfunctions2}Koyuncu, H., \& Ceylan, R. (2019). A PSO
based approach: Scout particle swarm algorithm for continuous global
optimization problems. Journal of Computational Design and Engineering,
6(2), 129-142.

\bibitem{testfunctions3}Siarry, P., Berthiau, G., Durdin, F., \&
Haussy, J. (1997). Enhanced simulated annealing for globally minimizing
functions of many-continuous variables. ACM Transactions on Mathematical
Software (TOMS), 23(2), 209-228.

\bibitem{testfunctions4}Tsoulos, I. G., \& Lagaris, I. E. (2008).
GenMin: An enhanced genetic algorithm for global optimization. Computer
Physics Communications, 178(11), 843-851.

\bibitem{testfunctions5}LaTorre, A., Molina, D., Osaba, E., Poyatos,
J., Del Ser, J., \& Herrera, F. (2021). A prescription of methodological
guidelines for comparing bio-inspired optimization algorithms. Swarm
and Evolutionary Computation, 67, 100973.

\bibitem{gkls}Gaviano, M., Kvasov, D. E., Lera, D., \& Sergeyev,
Y. D. (2003). Algorithm 829: Software for generation of classes of
test functions with known local and global minima for global optimization.
ACM Transactions on Mathematical Software (TOMS), 29(4), 469-480.

\bibitem[(2018)]{optimus}Tsoulos, I.G.; Charilogis, V.; Kyrou, G.;
Stavrou, V.N.; Tzallas, A. OPTIMUS: A Multidimensional Global Optimization
Package. J. Open Source Softw. 2025, 10, 7584.

\bibitem{key-21}Zangmo, R., Sudabattula, S. K., Dharavat, N., Mishra,
S., Basha, C. H., \& Irfan, M. M. (2025). Techno-economic analysis
of distribution system at various load models using MEWOA algorithm. Scientific
Reports, 15(1), 8273.

\bibitem{key-23}Masoudi, M. R., Haghighi, M., \& Sezavar, H. R. (2025,
February). Optimization of Energy Hub Performance Using the Improved
Particle Swarm Optimization (IPSO) Algorithm for Integrated Energy
Systems. In 2025 12th Iranian Conference on Renewable Energies and
Distributed Generation (ICREDG) (pp. 1-6). IEEE.

\bibitem{key-4}hang, R., Mao, S., Zhao, S., \& Liu, C. (2024). An
arithmetic optimization algorithm with balanced diversity and convergence
for multimodal multiobjective optimization. Swarm and Evolutionary
Computation, 91, 101724.

\bibitem{key-5}Sulaiman, A. T., Bello-Salau, H., Onumanyi, A. J.,
Mu’azu, M. B., Adedokun, E. A., Salawudeen, A. T., \& Adekale, A.
D. (2024). A particle swarm and smell agent-based hybrid algorithm
for enhanced optimization. Algorithms, 17(2), 53.

\bibitem{key-20}Zhao, H., Zhang, K., Li, X., \& Jin, J. (2025). MYIGWO:
A grey wolf optimizer with dual mutation and chaotic adaptive neighborhood
for engineering problems and path planning. Advances in Engineering
Software, 211, 104044.

\bibitem{key-8}Luo, B., Su, X., Zhang, S., Yan, P., Liu, J., \& Li,
R. (2025). Analysis of a novel gas cycle cooler with large temperature
glide for space cooling. Energy, 326, 136294.

\bibitem{key-6}Keerthika, R., Niranjan, S. P., \& Komala Durga, B.
(2025). A Survey on the tandem queueing models. Scope, 14, 134-148.

\end{thebibliography}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% for journal Sci
%\reviewreports{\\
%Reviewer 1 comments and authors' response\\
%Reviewer 2 comments and authors' response\\
%Reviewer 3 comments and authors' response
%}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\PublishersNote{}

\end{adjustwidth}{}
\end{document}
