%% LyX 2.3.7 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[journal,article,submit,pdftex,moreauthors]{Definitions/mdpi}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\setcounter{secnumdepth}{1}
\setcounter{tocdepth}{1}
\usepackage{float}
\usepackage{url}
\usepackage{graphicx}

\makeatletter

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% LyX specific LaTeX commands.

\Title{An Innovative Hybrid Approach to Global Optimization}

\TitleCitation{An Innovative Hybrid Approach to Global Optimization}

\Author{Vasileios Charilogis$^{1}$,Glykeria Kyrou$^{2}$, Ioannis G. Tsoulos$^{3,*}$,
Anna Maria Gianni$^{4}$}

\AuthorNames{V. Charilogis, Glykeria Kyrou, I.G. Tsoulos, A.M. Gianni}

\AuthorCitation{Charilogis V.; Kyrou, G.;Tsoulos I.G.; Gianni A.M; }


\address{$^{1}$\quad{}Department of Informatics and Telecommunications,
University of Ioannina, Greece; v.charilog@uoi.gr\\
$^{2}$\quad{}Department of Informatics and Telecommunications, University
of Ioannina, Greece; g.kyrou@uoi.gr\\
$^{3}$\quad{}Department of Informatics and Telecommunications, University
of Ioannina, Greece; itsoulos@uoi.gr\\
$^{4}$\quad{}Department of Informatics and Telecommunications, University
of Ioannina, Greece; am.gianni@uoi.gr}


\corres{Correspondence: itsoulos@uoi.gr;}


\abstract{Global optimization is critical in engineering, computer science,
and various industrial applications, as it aims to find optimal solutions
for complex problems. The development of efficient algorithms has
emerged from the need for optimization, with each algorithm offering
specific advantages and disadvantages. An effective approach to solving
complex problems is the hybrid method, which combines established
global optimization algorithms. This paper presents a hybrid global
optimization method, which produces trial solutions of the objective
problem through vector operations derived from methods, such as Genetic
Algorithms, Line Search and Differential Evolution. These operations
are based on samples derived either from internal line searches or
genetically modified samples in specific subsets of Euclidean space.
Additionally, other relevant approaches are explored to enhance the
method's efficiency. The new method was applied on a wide series of
benchmark problems from the recent literature and comparison was made
against other established methods of Global Optimization. }


\keyword{Optimization; Differential evolution; Genetic algorithm; Line search;
Evolutionary techniques; Stochastic methods; Hybrid methods.}

%% Because html converters don't know tabularnewline
\providecommand{\tabularnewline}{\\}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
%  LaTeX support: latex@mdpi.com 
%  For support, please attach all files needed for compiling as well as the log file, and specify your operating system, LaTeX version, and LaTeX editor.

%=================================================================


% For posting an early version of this manuscript as a preprint, you may use "preprints" as the journal and change "submit" to "accept". The document class line would be, e.g., \documentclass[preprints,article,accept,moreauthors,pdftex]{mdpi}. This is especially recommended for submission to arXiv, where line numbers should be removed before posting. For preprints.org, the editorial staff will make this change immediately prior to posting.

%--------------------
% Class Options:
%--------------------
%----------
% journal
%----------
% Choose between the following MDPI journals:
% acoustics, actuators, addictions, admsci, adolescents, aerospace, agriculture, agriengineering, agronomy, ai, algorithms, allergies, alloys, analytica, animals, antibiotics, antibodies, antioxidants, applbiosci, appliedchem, appliedmath, applmech, applmicrobiol, applnano, applsci, aquacj, architecture, arts, asc, asi, astronomy, atmosphere, atoms, audiolres, automation, axioms, bacteria, batteries, bdcc, behavsci, beverages, biochem, bioengineering, biologics, biology, biomass, biomechanics, biomed, biomedicines, biomedinformatics, biomimetics, biomolecules, biophysica, biosensors, biotech, birds, bloods, blsf, brainsci, breath, buildings, businesses, cancers, carbon, cardiogenetics, catalysts, cells, ceramics, challenges, chemengineering, chemistry, chemosensors, chemproc, children, chips, cimb, civileng, cleantechnol, climate, clinpract, clockssleep, cmd, coasts, coatings, colloids, colorants, commodities, compounds, computation, computers, condensedmatter, conservation, constrmater, cosmetics, covid, crops, cryptography, crystals, csmf, ctn, curroncol, currophthalmol, cyber, dairy, data, dentistry, dermato, dermatopathology, designs, diabetology, diagnostics, dietetics, digital, disabilities, diseases, diversity, dna, drones, dynamics, earth, ebj, ecologies, econometrics, economies, education, ejihpe, electricity, electrochem, electronicmat, electronics, encyclopedia, endocrines, energies, eng, engproc, ent, entomology, entropy, environments, environsciproc, epidemiologia, epigenomes, est, fermentation, fibers, fintech, fire, fishes, fluids, foods, forecasting, forensicsci, forests, foundations, fractalfract, fuels, futureinternet, futureparasites, futurepharmacol, futurephys, futuretransp, galaxies, games, gases, gastroent, gastrointestdisord, gels, genealogy, genes, geographies, geohazards, geomatics, geosciences, geotechnics, geriatrics, hazardousmatters, healthcare, hearts, hemato, heritage, highthroughput, histories, horticulturae, humanities, humans, hydrobiology, hydrogen, hydrology, hygiene, idr, ijerph, ijfs, ijgi, ijms, ijns, ijtm, ijtpp, immuno, informatics, information, infrastructures, inorganics, insects, instruments, inventions, iot, j, jal, jcdd, jcm, jcp, jcs, jdb, jeta, jfb, jfmk, jimaging, jintelligence, jlpea, jmmp, jmp, jmse, jne, jnt, jof, joitmc, jor, journalmedia, jox, jpm, jrfm, jsan, jtaer, jzbg, kidney, kidneydial, knowledge, land, languages, laws, life, liquids, literature, livers, logics, logistics, lubricants, lymphatics, machines, macromol, magnetism, magnetochemistry, make, marinedrugs, materials, materproc, mathematics, mca, measurements, medicina, medicines, medsci, membranes, merits, metabolites, metals, meteorology, methane, metrology, micro, microarrays, microbiolres, micromachines, microorganisms, microplastics, minerals, mining, modelling, molbank, molecules, mps, msf, mti, muscles, nanoenergyadv, nanomanufacturing, nanomaterials, ncrna, network, neuroglia, neurolint, neurosci, nitrogen, notspecified, nri, nursrep, nutraceuticals, nutrients, obesities, oceans, ohbm, onco, oncopathology, optics, oral, organics, organoids, osteology, oxygen, parasites, parasitologia, particles, pathogens, pathophysiology, pediatrrep, pharmaceuticals, pharmaceutics, pharmacoepidemiology, pharmacy, philosophies, photochem, photonics, phycology, physchem, physics, physiologia, plants, plasma, pollutants, polymers, polysaccharides, poultry, powders, preprints, proceedings, processes, prosthesis, proteomes, psf, psych, psychiatryint, psychoactives, publications, quantumrep, quaternary, qubs, radiation, reactions, recycling, regeneration, religions, remotesensing, reports, reprodmed, resources, rheumato, risks, robotics, ruminants, safety, sci, scipharm, seeds, sensors, separations, sexes, signals, sinusitis, skins, smartcities, sna, societies, socsci, software, soilsystems, solar, solids, sports, standards, stats, stresses, surfaces, surgeries, suschem, sustainability, symmetry, synbio, systems, taxonomy, technologies, telecom, test, textiles, thalassrep, thermo, tomography, tourismhosp, toxics, toxins, transplantology, transportation, traumacare, traumas, tropicalmed, universe, urbansci, uro, vaccines, vehicles, venereology, vetsci, vibration, viruses, vision, waste, water, wem, wevj, wind, women, world, youth, zoonoticdis 

%---------
% article
%---------
% The default type of manuscript is "article", but can be replaced by: 
% abstract, addendum, article, book, bookreview, briefreport, casereport, comment, commentary, communication, conferenceproceedings, correction, conferencereport, entry, expressionofconcern, extendedabstract, datadescriptor, editorial, essay, erratum, hypothesis, interestingimage, obituary, opinion, projectreport, reply, retraction, review, perspective, protocol, shortnote, studyprotocol, systematicreview, supfile, technicalnote, viewpoint, guidelines, registeredreport, tutorial
% supfile = supplementary materials

%----------
% submit
%----------
% The class option "submit" will be changed to "accept" by the Editorial Office when the paper is accepted. This will only make changes to the frontpage (e.g., the logo of the journal will get visible), the headings, and the copyright information. Also, line numbering will be removed. Journal info and pagination for accepted papers will also be assigned by the Editorial Office.

%------------------
% moreauthors
%------------------
% If there is only one author the class option oneauthor should be used. Otherwise use the class option moreauthors.

%---------
% pdftex
%---------
% The option pdftex is for use with pdfLaTeX. If eps figures are used, remove the option pdftex and use LaTeX and dvi2pdf.

%=================================================================
% MDPI internal commands - do not modify
\firstpage{1} 
 
\setcounter{page}{\@firstpage} 

\pubvolume{1}
\issuenum{1}
\articlenumber{0}
\pubyear{2023}
\copyrightyear{2023}
%\externaleditor{Academic Editor: Firstname Lastname} % For journal Automation, please change Academic Editor to "Communicated by"
\datereceived{}
\daterevised{ } % Comment out if no revised date
\dateaccepted{}
\datepublished{}
%\datecorrected{} % Corrected papers include a "Corrected: XXX" date in the original paper.
%\dateretracted{} % Corrected papers include a "Retracted: XXX" date in the original paper.
\hreflink{https://doi.org/} % If needed use \linebreak
%\doinum{}
%------------------------------------------------------------------
% The following line should be uncommented if the LaTeX file is uploaded to arXiv.org
%\pdfoutput=1

%=================================================================
% Add packages and commands here. The following packages are loaded in our class file: fontenc, inputenc, calc, indentfirst, fancyhdr, graphicx, epstopdf, lastpage, ifthen, lineno, float, amsmath, setspace, enumitem, mathpazo, booktabs, titlesec, etoolbox, tabto, xcolor, soul, multirow, microtype, tikz, totcount, changepage, attrib, upgreek, cleveref, amsthm, hyphenat, natbib, hyperref, footmisc, url, geometry, newfloat, caption

%=================================================================
%% Please use the following mathematics environments: Theorem, Lemma, Corollary, Proposition, Characterization, Property, Problem, Example, ExamplesandDefinitions, Hypothesis, Remark, Definition, Notation, Assumption
%% For proofs, please use the proof environment (the amsthm package is loaded by the MDPI class).

%=================================================================
% The fields PACS, MSC, and JEL may be left empty or commented out if not applicable
%\PACS{J0101}
%\MSC{}
%\JEL{}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Only for the journal Diversity
%\LSID{\url{http://}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Only for the journal Applied Sciences:
%\featuredapplication{Authors are encouraged to provide a concise description of the specific application or a potential application of the work. This section is not mandatory.}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Only for the journal Data:
%\dataset{DOI number or link to the deposited data set in cases where the data set is published or set to be published separately. If the data set is submitted and will be published as a supplement to this paper in the journal Data, this field will be filled by the editors of the journal. In this case, please make sure to submit the data set as a supplement when entering your manuscript into our manuscript editorial system.}

%\datasetlicense{license under which the data set is made available (CC0, CC-BY, CC-BY-SA, CC-BY-NC, etc.)}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Only for the journal Toxins
%\keycontribution{The breakthroughs or highlights of the manuscript. Authors can write one or two sentences to describe the most important part of the paper.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Only for the journal Encyclopedia
%\encyclopediadef{Instead of the abstract}
%\entrylink{The Link to this entry published on the encyclopedia platform.}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Only for the journal Advances in Respiratory Medicine
%\addhighlights{yes}
%\renewcommand{\addhighlights}{%

%\noindent This is an obligatory section in â€œAdvances in Respiratory Medicineâ€, whose goal is to increase the discoverability and readability of the article via search engines and other scholars. Highlights should not be a copy of the abstract, but a simple text allowing the reader to quickly and simplified find out what the article is about and what can be cited from it. Each of these parts should be devoted up to 2~bullet points.\vspace{3pt}\\
%\textbf{What are the main findings?}
% \begin{itemize}[labelsep=2.5mm,topsep=-3pt]
% \item First bullet.
% \item Second bullet.
% \end{itemize}\vspace{3pt}
%\textbf{What is the implication of the main finding?}
% \begin{itemize}[labelsep=2.5mm,topsep=-3pt]
% \item First bullet.
% \item Second bullet.
% \end{itemize}
%}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\@ifundefined{showcaptionsetup}{}{%
 \PassOptionsToPackage{caption=false}{subfig}}
\usepackage{subfig}
\makeatother

\begin{document}
\maketitle

\section{Introduction}

The basic goal of global optimization is to find the global minimum
by searching for the appropriate range of the underlying objective
problem. The global optimization method aims to find the global minimum
of a continuous multidimensional function and is defined as

\begin{equation}
x^{*}=\mbox{arg}\min_{x\in S}f(x)\label{eq:eq1}
\end{equation}
where the set $S$ is defined as follows: 
\[
S=\left[a_{1},b_{1}\right]\times\left[a_{2},b_{2}\right]\times\ldots\left[a_{n},b_{n}\right]
\]

Global Optimization refers to algorithms whose main objective is to
find the global optimum of a problem. According to literature research
there are a variety of real-world problems that can be formulated
as global optimization problems, such as problems in mathematics \citep{go_math1,go_math2,go_math3},
physics \citep{go_physics1,go_physics2,go_physics3}, chemistry \citep{go_chem1,go_chem2,go_chem3},
and medicine \citep{go_med1,go_med2,medicine}, biology \citep{go_bio1,go_bio2},
agriculture \citep{go_agri1,go_agri2} and economics \citep{go_econ1,go_econ2}.
Optimization methods can be categorized into deterministic \citep{go_determ1,go_determ2,go_determ3}
and stochastic \citep{stohastic,stohastic1,stohastic2} based on how
they approach solving the problem. The techniques used for deterministic
are mainly interval methods \citep{interval1,interval2}. In interval
methods, the set S is divided into smaller regions that may contain
the global minimum using certain criteria. On the other hand stochastic
methods use randomness to explore the solution space. Recently, Sergeyev
et al. \citep{Sergeyev} proposed a comparison between deterministic
and stochastic methods.

A series of stochastic optimization methods are the so - called evolutionary
methods, which attempt to mimic a series of natural processes. Such
methods include the Genetic algorithms \citep{genetic1,genetic2},
the Differential Evolution method \citep{diffe1,diffe2}, Particle
Swarm Optimization (PSO) methods \citep{pso_major,pso1,pso2}, Ant
Colony optimization methods \citep{aco1,aco2}, the Fish Swarm Algorithm
\citep{fish}, the Dolphin Swarm Algorithm \citep{dolphin}, the Whale
Optimization Algorithm (WOA) algorithm \citep{WOA,WOA1,WOA2} etc.
Also, due to the wide spread of parallel computing units, a variety
of research papers related to evolutionary techniques appeared that
use such processing units \citep{gpu1,gpu2,gpu3}. In the current
work, two evolutionary methods were incorporated in the final algorithm:
Genetic Algorithms and the Differential Evolution method was combined
into a hybrid optimization method.

Genetic algorithms were formulated by John Holland \citep{holland}
and his team and they initially generated randomly candidate solutions
to an optimization problem. These solutions were modified through
a series of operators that mimic natural processes, such as mutation,
selection and crossover. Genetic algorithms have been used widely
in areas such as networking \citep{ga_problem1}, robotics \citep{ga_problem2,ga_problem3},
energy topics \citep{ga_problem4,ga_problem5} etc.\textbf{ }They
can be combined with machine learning to solve complex problems, such
as neural network training \citep{ga_nn1,ga_nn2}.

Furthermore, differential evolution (DE) is used in symmetric optimization
problems \citep{de_symmetry1,de_symmetry2} and in problems that are
discontinuous and noisy and change over time. After studies, it was
observed that differential evolution can be successfully combined
with other techniques for machine learning applications, such as classification
\citep{de_problem1,de_problem2}, feature selection \citep{de_problem3,de_problem4},
deep learning \citep{de_deep1,de_deep2} etc.

Hybrid methods \citep{go_local2,go_local3} in global optimization
refer to techniques that combine multiple optimization strategies
to solve complex problems. These methods aim to take advantage of
different approaches to find the global optimum in a more efficient
way, particularly when dealing with large-scale problems or strongly
nonlinear optimization landscapes. A typical example of a hybrid method
is the work of Shutao Li et al. who proposed a new hybrid PSO-BFGS
strategy for the global optimization of multimodal functions \citep{hybrid1}.
To make the combination more efficient, they proposed an LDI to dynamically
start the local search and a repositioning technique to maintain the
particle diversity, which can effectively avoid the premature convergence
problem. Another innovative hybrid method is the work of M. Andalib
Sahnehsaraei et al. where a hybrid algorithm using GA operators and
PSO formula is proposed was presented through the use of efficient
operators, for example, traditional and multiple crossovers, mutation
and PSO formula \citep{hybrid2}.

The current work produces through a series of steps trial solutions
using the genetic operators of the Genetic Algorithm as well as solutions
identified by a line search procedure. In current work, the Armijo
line search is a method is used. This method is incorporated to estimate
an appropriate step when updating the trial points, and it was introduced
in the work of Armijo\citep{armijo}. The solutions produced in the
previous step are used to formulate new trial solutions using a process
derived from Differential Evolution. 

The remainder of this paper is divided into the following sections:
in section \ref{sec:overallAlgorithm}, the proposed method is described,
in section \ref{sec:Experiments} the experimental results and statistical
comparisons are presented, and finally in section \ref{sec:Conclusions}
some conclusions and guidelines for future improvements are discussed.

\section{The overall algorithm\label{sec:overallAlgorithm}}

The proposed method combines some aspects from different optimization
algorithms and the main steps are subsequently:
\begin{enumerate}
\item \textbf{Initialization step.}
\begin{enumerate}
\item \textbf{Set} the population size $N\ge$4.
\item \textbf{Set} $n$ the dimension of the benchmark function.
\item \textbf{Initialize }the samples\textbf{ $x_{i},i=1,\ldots,N$ }using
uniform distribution.
\end{enumerate}
\item \textbf{Calculation step.\label{enu:Calculation-step.}}
\begin{enumerate}
\item \textbf{For }$i=1\ldots\mbox{N}\ $\textbf{do}
\begin{enumerate}
\item \textbf{Obtain }sample $x_{i}$.
\item \textbf{Find nearest sample $c_{\iota}$ from $x_{\iota}$:
\begin{equation}
d(x,c)=\sqrt{\sum_{i=1}^{n}\left(x_{i}-c_{i}\right)^{2}}\label{eq:nearestSamply}
\end{equation}
}where $d(x,c)$ is the Euclidean Distance.
\item \textbf{Set} direction vectors: $p_{1}=-\nabla f(x_{i})$ end $p_{2}=-\nabla f(c_{i})$
\item \textbf{Set} initial step size for Armijo $a=a_{0}$
\item \textbf{Compute }with line search Armijo the sample:
\begin{itemize}
\item \textbf{Find }new points using line search $\mbox{minLS}(x,c)$: $x_{i}^{new}=x_{i}+ap_{1}$
and $c_{i}^{new}=c_{i}+ap_{2}$
\item \textbf{Adjust} step size $a$ until Armijo condition is met:
\begin{equation}
f\left(x_{i}^{new},c_{i}{}^{new}\right)\le f\left(x_{i},c_{i}\right)+c_{1}a\nabla f\left(x_{i},c_{i}\right)^{T}\left(p_{1},p_{2}\right)\label{eq:findSample}
\end{equation}
\end{itemize}
\item \textbf{Make} sample-child with crossover with random number $g_{k}\in[0.0,1.0]$:
\begin{equation}
\mbox{child}\left(x,x^{best}\right)=g_{k}x_{k}+\left(1-g_{k}\right)x_{k}^{best}\label{eq:makeChild}
\end{equation}
\item \textbf{For} $j=1,\ldots,n$ \textbf{do}
\begin{itemize}
\item \textbf{Set} trial vector:
\begin{equation}
y_{j}=x_{j}+F\times\left(\mbox{minLS}(x_{\iota},c_{\iota})_{j}-\mbox{child}(x_{i},c_{i})_{j}\right)\label{eq:trialSample}
\end{equation}
where $F$ is the so - called Differential Weight of Differential
Evolution algorithm.
\item \textbf{If $y_{j}\notin\left[a_{j},b_{j}\right]$, }then $y_{j}=x_{i,j}$
\end{itemize}
\item \textbf{EndFor}
\end{enumerate}
\begin{itemize}
\item \textbf{Set} $r\in[0,1]$ a random number. If $r\le p_{m}$ then $x_{i}=\mbox{LS}\left(x_{i}\right)$,
where $\mbox{LS}(x)$ is a local search procedure like the BFGS procedure\citep{bfgs}.
\item \textbf{If} $f\left(y\right)\le f\left(x\right)$ then $x=y$, $x^{best}=y$.
\end{itemize}
\item \textbf{EndFor}
\end{enumerate}
\item \textbf{Check the termination rule stated in }\citep{charilogis},
which means that the method checks the difference between the current
optimal solution $f_{min}^{(t)}$ and the previous $f_{min}^{(t-1)}$
one. The algorithm terminates when the following:
\begin{equation}
\left|f_{\mbox{min}}^{(k)}-f_{\mbox{min}}^{(k-1)}\right|\le\epsilon\label{eq:term}
\end{equation}
holds for $N_{t}$ iterations. The value $\epsilon$ is a small positive
value. In the conducted experiments the value $\epsilon=10^{-5}$
was used. If the termination rule of equation \ref{eq:term} does
not hold, then the algorithm continues from Step \ref{enu:Calculation-step.}.
\item \textbf{Return} the sample $x^{best}$ in the population with the
lower function value $f\left(x^{best}\right)$.
\end{enumerate}
Initially, the algorithm initializes the trial solutions as well as
the necessary parameters. At every iteration, the method constructs
trial solutions using genetic algorithms operations as well as the
Armijo line search method and these solutions are combined using a
process from Differential Evolution, using the following steps:
\begin{itemize}
\item Identification of the nearest point \textbf{$c_{\iota}$ }for each
sample\textbf{ $x_{\iota}$}.
\item Calculation of a sample $\mbox{minLS}(x,c)$ through Armijo line search,
between the sample \textbf{$x_{i}$ }and the sample$c_{i}$.
\item Generation of the sample using the crossover process of the Genetic
Algorithm, between the sample \textbf{$x_{\iota}$ }and the best sample$x_{i}^{best}$
.
\item Computation of the trial point $y_{i}$ using a process derived from
Differential Evolution.
\end{itemize}

\section{Experiments\label{sec:Experiments}}

\subsection{Settings and benchmark functions}

To ensure the reliability of the experimental results, the experiments
were repeated 30 times using different seeds for the random number
generator. The experiments were conducted on an AMD Ryzen 5950X processor
with 128 GB of RAM and the used operating system was Debian Linux.
The software as well as the objective function was written entirely
in ANSI-C++ with the assistance of the open - source optimization
environment OPTIMUS, that is available from \url{http://www.github.com/itsoulos/GlobalOptimus}
(accessed on 17 September 2024). The test functions used in the experiments
are presented in Table \ref{tab:benchmarkFunctions}. 
\begin{table}[H]
\caption{The benchmark functions used in the conducted experiments.\label{tab:benchmarkFunctions}}

\centering{}%
\begin{tabular}{|c|c|c|}
\hline 
{\footnotesize{}NAME} & {\scriptsize{}FORMULA} & {\scriptsize{}DIMENSION}\tabularnewline
\hline 
\hline 
{\footnotesize{}BF1} & {\scriptsize{}$f(x)=x_{1}^{2}+2x_{2}^{2}-\frac{3}{10}\cos\left(3\pi x_{1}\right)-\frac{4}{10}\cos\left(4\pi x_{2}\right)+\frac{7}{10}$} & {\scriptsize{}2}\tabularnewline
\hline 
{\footnotesize{}BF2} & {\scriptsize{}$f(x)=x_{1}^{2}+2x_{2}^{2}-\frac{3}{10}\cos\left(3\pi x_{1}\right)\cos\left(4\pi x_{2}\right)+\frac{3}{10}$} & {\scriptsize{}2}\tabularnewline
\hline 
{\footnotesize{}BF3} & {\scriptsize{}$f(x)=x_{1}^{2}+2x_{2}^{2}-\frac{3}{10}\cos\left(3\pi x_{1}\right)\cos\left(4\pi x_{2}\right)+\frac{3}{10}$} & {\scriptsize{}2}\tabularnewline
\hline 
{\footnotesize{}BRANIN} & {\scriptsize{}$f(x)=\left(x_{2}-\frac{5.1}{4\pi^{2}}x_{1}^{2}+\frac{5}{\pi}x_{1}-6\right)^{2}+10\left(1-\frac{1}{8\pi}\right)\cos(x_{1})+10$} & {\scriptsize{}2}\tabularnewline
\hline 
{\footnotesize{}CAMEL} & {\scriptsize{}$f(x)=4x_{1}^{2}-2.1x_{1}^{4}+\frac{1}{3}x_{1}^{6}+x_{1}x_{2}-4x_{2}^{2}+4x_{2}^{4},\quad x\in[-5,5]^{2}$} & {\scriptsize{}2}\tabularnewline
\hline 
{\footnotesize{}Easom} & {\scriptsize{}$f(x)=-\cos\left(x_{1}\right)\cos\left(x_{2}\right)\exp\left(\left(x_{2}-\pi\right)^{2}-\left(x_{1}-\pi\right)^{2}\right)$} & {\scriptsize{}2}\tabularnewline
\hline 
{\footnotesize{}ELP} & {\scriptsize{}$f(x)=\sum_{i=1}^{n}\left(10^{6}\right)^{\frac{i-1}{n-1}}x_{i}^{2}$} & {\scriptsize{}$n=10,20,30$}\tabularnewline
\hline 
{\footnotesize{}Exp} & {\scriptsize{}$f(x)=-\exp\left(-0.5\sum_{i=1}^{n}x_{i}^{2}\right),\quad-1\le x_{i}\le1$} & {\scriptsize{}$n=4,8,16,32$}\tabularnewline
\hline 
{\footnotesize{}Gkls\citep{Gaviano}} & {\scriptsize{}$f(x)=\mbox{Gkls}(x,n,w)$} & {\scriptsize{}$n=2,3\ w=50,100$}\tabularnewline
\hline 
{\footnotesize{}Griewank2} & {\scriptsize{}$f(x)=1+\frac{1}{200}\sum_{i=1}^{2}x_{i}^{2}-\prod_{i=1}^{2}\frac{\cos(x_{i})}{\sqrt{(i)}}$} & {\scriptsize{}2}\tabularnewline
\hline 
{\footnotesize{}Griewank10} & {\scriptsize{}f$(x)=1+\frac{1}{200}\sum_{i=1}^{10}x_{i}^{2}-\prod_{i=1}^{10}\frac{\cos(x_{i})}{\sqrt{(i)}}$} & {\scriptsize{}10}\tabularnewline
\hline 
{\footnotesize{}Hansen} & {\scriptsize{}$f(x)=\sum_{i=1}^{5}i\cos\left[(i-1)x_{1}+i\right]\sum_{j=1}^{5}j\cos\left[(j+1)x_{2}+j\right]$} & {\scriptsize{}2}\tabularnewline
\hline 
{\footnotesize{}Hartman3} & {\scriptsize{}$f(x)=-\sum_{i=1}^{4}c_{i}\exp\left(-\sum_{j=1}^{3}a_{ij}\left(x_{j}-p_{ij}\right)^{2}\right)$} & {\scriptsize{}3}\tabularnewline
\hline 
{\footnotesize{}Hartman6} & {\scriptsize{}$f(x)=-\sum_{i=1}^{4}c_{i}\exp\left(-\sum_{j=1}^{6}a_{ij}\left(x_{j}-p_{ij}\right)^{2}\right)$} & {\scriptsize{}6}\tabularnewline
\hline 
{\footnotesize{}Potential\citep{Lennard}} & {\scriptsize{}$V_{LJ}(r)=4\epsilon\left[\left(\frac{\sigma}{r}\right)^{12}-\left(\frac{\sigma}{r}\right)^{6}\right]$} & {\scriptsize{}$n=9,15,21,30$}\tabularnewline
\hline 
{\footnotesize{}Rastrigin} & {\scriptsize{}$f(x)=x_{1}^{2}+x_{2}^{2}-\cos(18x_{1})-\cos(18x_{2})$} & {\scriptsize{}2}\tabularnewline
\hline 
{\footnotesize{}Rosenbrcok} & {\tiny{}$f(x)=\sum_{i=1}^{n-1}\left(100\left(x_{i+1}-x_{i}^{2}\right)^{2}+\left(x_{i}-1\right)^{2}\right),\quad-30\le x_{i}\le30$} & {\scriptsize{}$n=4,8,16$}\tabularnewline
\hline 
{\footnotesize{}Shekel5} & {\scriptsize{}$f(x)=-\sum_{i=1}^{5}\frac{1}{(x-a_{i})(x-a_{i})^{T}+c_{i}}$} & {\scriptsize{}4}\tabularnewline
\hline 
{\footnotesize{}Shekel7} & {\scriptsize{}$f(x)=-\sum_{i=1}^{7}\frac{1}{(x-a_{i})(x-a_{i})^{T}+c_{i}}$} & {\scriptsize{}4}\tabularnewline
\hline 
{\footnotesize{}Shekel10} & {\scriptsize{}$f(x)=-\sum_{i=1}^{10}\frac{1}{(x-a_{i})(x-a_{i})^{T}+c_{i}}$} & {\scriptsize{}4}\tabularnewline
\hline 
{\footnotesize{}Sinusoidal\citep{Zabinsky}} & {\scriptsize{}$f(x)=-\left(2.5\prod_{i=1}^{n}\sin\left(x_{i}-z\right)+\prod_{i=1}^{n}\sin\left(5\left(x_{i}-z\right)\right)\right),\quad0\le x_{i}\le\pi$} & {\scriptsize{}$n=4,8$}\tabularnewline
\hline 
{\footnotesize{}Test2N} & {\scriptsize{}$f(x)=\frac{1}{2}\sum_{i=1}^{n}x_{i}^{4}-16x_{i}^{2}+5x_{i}$} & {\scriptsize{}$n=4,5,7$}\tabularnewline
\hline 
{\footnotesize{}Test30N} & {\scriptsize{}$\frac{1}{10}\sin^{2}\left(3\pi x_{1}\right)\sum_{i=2}^{n-1}\left(\left(x_{i}-1\right)^{2}\left(1+\sin^{2}\left(3\pi x_{i+1}\right)\right)\right)+\left(x_{n}-1\right)^{2}\left(1+\sin^{2}\left(2\pi x_{n}\right)\right)$} & {\scriptsize{}$n=3,4$}\tabularnewline
\hline 
\end{tabular}
\end{table}
 The functions used in the conducted experiments have been proposed
by various researchers \citep{Ali1,Floudas1} in the relevant literature.
For a more accurate comparison of the methods, efforts were made to
maintain certain parameter values at equal or similar levels. The
values for the parameters of the algorithm are presented in Table\ref{tab:settings},
along with some explanation of each parameter.
\begin{center}
\begin{table}[H]
\caption{Parameters of optimization methods settings\label{tab:settings}}

\centering{}%
\begin{tabular}{|c|c|c|}
\hline 
PARAMETER & VALUE & EXPLANATION\tabularnewline
\hline 
\hline 
$N$ & $200$ & Number of samples for all methods\tabularnewline
\hline 
$N_{k}$ & $200$ & Maximum number of iterations for all methods\tabularnewline
\hline 
$SR$ & $\left|f_{\mbox{min}}^{(k)}-f_{\mbox{min}}^{(k-1)}\right|$ & Best fitness: Stopping rule for all methods\tabularnewline
\hline 
$N_{t}$ & $12$ & Similarity max count for all methods\tabularnewline
\hline 
$F$ & $0.8$ & Differential weight for Differential Evolution\tabularnewline
\hline 
$CR$ & $0.9$ & Crossover Probability for Differential Evolution\tabularnewline
\hline 
$C_{1},C_{2}$ & $0.5$ & Parameters of PSO\tabularnewline
\hline 
$G_{c}$ & $0.1$\ $(10\%)$ & Crossover rate for Genetic Algorithm\tabularnewline
\hline 
$G_{m}$ & $0.05$\ $(5\%)$ & Mutation rate for Genetic Algorithm\tabularnewline
\hline 
\end{tabular}
\end{table}
\par\end{center}

\subsection{Experimental results}

In Table \ref{tab:mainTable}, the average function calls for each
method and every objective function is presented. All experiments
were executed 30 times with different initialization for the random
generator each time and the average number of function calls was depicted.
The following notation was used in this table:
\begin{enumerate}
\item The column FUNCTION represents the name of the used objective problem.
\item The column DE represents the average function calls for the Differential
Evolution optimization technique.
\item the column IPSO stands for the application of the Improved PSO algorithm
as suggested by Charilogis and Tsoulos \citep{ipso} to the objective
problems.
\item The column GENETIC represents the application of the Genetic Algorithm
to the objective problem.
\item At the end of the table, the total sum of calls for each method is
listed. 
\item The number in parentheses indicates the percentage of executions in
which the global optimum was successfully found. The absence of this
number signifies that the global minimum was computed in every independent
run with 100\% success. 
\end{enumerate}
\begin{table}[H]
\centering{}\caption{Comparison of average function calls of proposed method against others\label{tab:mainTable}}
\begin{tabular}{|c|c|c|c|c|}
\hline 
FUNCTION & DE & IPSO & GENETIC & PROPOSED\tabularnewline
\hline 
\hline 
BF1 & 8268 & 4113 & 4007 & 3951\tabularnewline
\hline 
BF2 & 7913 & 3747 & 3793 & 3382\tabularnewline
\hline 
BF3 & 6327 & 3305 & 3479 & 2736\tabularnewline
\hline 
BRANIN & 4101 & 2522 & 2376 & 1622\tabularnewline
\hline 
CAMEL & 5609 & 2908 & 2869 & 2027\tabularnewline
\hline 
EASOM & 2978 & 1998 & 1958 & 969\tabularnewline
\hline 
ELP10 & 6288 & 4397 & 3131 & 2820\tabularnewline
\hline 
ELP20 & 10794 & 6883 & 6160 & 5337\tabularnewline
\hline 
ELP30 & 14172 & 9438 & 9576 & 7070\tabularnewline
\hline 
EXP4 & 5166 & 3177 & 2946 & 2370\tabularnewline
\hline 
EXP16 & 6498 & 3477 & 3250 & 2654\tabularnewline
\hline 
EXP32 & 7606 & 3728 & 3561 & 2652\tabularnewline
\hline 
GKLS250 & 3834 & 2495 & 2280 & 1115\tabularnewline
\hline 
GKLS350 & 3919 & 2658 & 2612 & 945(93)\tabularnewline
\hline 
GOLDSTEIN & 6781 & 3856 & 3687 & 2676\tabularnewline
\hline 
GRIEWANK2 & 7429(96) & 3168 & 4500(96) & 2453(50)\tabularnewline
\hline 
GRIEWANK10 & 18490 & 7942 & 6409 & 6351\tabularnewline
\hline 
HANSEN & 4185 & 2892 & 3209 & 2525\tabularnewline
\hline 
HARTMAN3 & 5190 & 3103 & 2751 & 1945\tabularnewline
\hline 
HARTMAN6 & 5968 & 3688 & 3219 & 2832\tabularnewline
\hline 
POTENTIAL3 & 6218 & 5154 & 4351 & 3537\tabularnewline
\hline 
POTENTIAL5 & 9119 & 10128 & 7704 & 6735\tabularnewline
\hline 
POTENTIAL6 & 10509(76) & 11780(46) & 10177(70) & 7706(76)\tabularnewline
\hline 
POTENTIAL10 & 12721(96) & 16550(86) & 13357 & 11517\tabularnewline
\hline 
RASTRIGIN & 6216 & 3539 & 4106 & 2125(86)\tabularnewline
\hline 
ROSENBROCK4 & 8452 & 5858 & 3679 & 4442\tabularnewline
\hline 
ROSENBROCK8 & 11530 & 7843 & 5269 & 6726\tabularnewline
\hline 
ROSENBROCK16 & 17432 & 11450 & 8509 & 7310\tabularnewline
\hline 
SHEKEL5 & 6662 & 3886 & 3325 & 3722\tabularnewline
\hline 
SHEKEL7 & 6967 & 4009 & 3360 & 3817\tabularnewline
\hline 
SHEKEL10 & 6757 & 3985 & 2488 & 3317\tabularnewline
\hline 
SINU4 & 5953 & 3409 & 2990 & 2192\tabularnewline
\hline 
SINU8 & 6973 & 3995 & 3441 & 3171\tabularnewline
\hline 
SINU16 & 6979 & 4680 & 4320 & 5250\tabularnewline
\hline 
TEST2N4 & 6396 & 3390 & 3330 & 2235\tabularnewline
\hline 
TEST2N5 & 6271 & 3604 & 4000(96) & 2530(93)\tabularnewline
\hline 
TEST2N7 & 7074 & 4020(96) & 4775(73) & 2939(50)\tabularnewline
\hline 
TEST30N3 & 6178 & 4018 & 3210 & 2728\tabularnewline
\hline 
TEST30N4 & 7006 & 4504 & 3678 & 3593\tabularnewline
\hline 
TOTAL SUM & 296929 & 195297 & 171842 & 144024\tabularnewline
\hline 
\end{tabular}
\end{table}
 At the end of every global optimization procedure, a BFGS variant
of Powell \citep{powell} is utilized to improve the discovered solution
and identify with certainty a local minimum.  Additionally, Figure
\ref{fig:proposedVSOthers} presents a statistical comparison between
the used optimization methods and the data provided from Table \ref{tab:mainTable}.
\begin{figure}[H]
\centering{}\includegraphics[scale=0.75]{img1}\caption{Comparison of function calls of proposed method against others\label{fig:proposedVSOthers}}
\end{figure}
 The proposed method seems to significantly reduce the required number
of functional calls compared to other techniques, and in many functions
this reduction can exceed 50\%. This trend is also confirmed by Figure
\ref{fig:StatProposedVSOthers}, where a statistical comparison is
presented between universal optimization methods used in the experiments.
\begin{figure}[H]
\centering{}\includegraphics[scale=0.5]{MethodsStat}\caption{Statistical comparison of proposed method against others\label{fig:StatProposedVSOthers}}
\end{figure}
 From this figure, it is evident that the proposed method demonstrates
significantly better performance compared to DE, with a substantial
reduction in cost (144,024 versus 296,929). The t-test confirmed the
statistically significant difference (p-value < 0.05). Compared to
IPSO, the proposed method also outperforms, with lower values in almost
all functions (144,024 versus 195,297). The Kruskal-Wallis test further
confirmed the statistical significance of this difference (p-value
< 0.05). Regarding GENETIC, the proposed method proves to be more
efficient, with a lower total cost (144,024 versus 171,842), and the
t-test indicates a statistically significant superiority.

In conclusion, the proposed method is more efficient compared to DE,
IPSO, and GENETIC. The differences in objective function costs are
statistically significant, as confirmed by both parametric (t-test)
and non-parametric (Kruskal-Wallis) analysis, demonstrating its superiority
in solving the specific functions.

An additional experiment was executed for the High Elliptic function,
which is defined as:
\[
f(x)=\sum_{i=1}^{n}\left(10^{6}\right)^{\frac{i-1}{n-1}}x_{i}^{2}
\]
where the parameter $n$ defines the dimension of the function. In
this experiment, the proposed method was applied to this function
as the dimension $n$ changes from 10 to 100. Figure \ref{fig:ELPcalls}
presents the performance of the proposed method for this function
in terms of function calls across various dimensions. It is evident
that as the dimensionality increases, the number of function calls
required to solve the problem also rises. Specifically, for a dimension
of 10, the algorithm requires 2,820 function calls, whereas for a
dimension of 100, it demands 23,062 function calls, which is approximately
eight times higher. This increase in function calls as the problem's
dimensionality grows suggests that the complexity of the ELP problem
scales significantly with dimensionality. The relationship between
dimensions and function calls appears to be nonlinear, indicating
that higher dimensions introduce additional computational challenges.
For instance, moving from 10 to 20 dimensions more than doubles the
required function calls, from 2,820 to 5,337, while the increase between
90 and 100 dimensions, although significant, shows a slightly smaller
relative increase (19,598 to 23,062). Figure \ref{fig:ELPTimes} shows
the response times of the ELP problem across various dimensions, from
10 to 100, measured in seconds. As the dimensionality increases, the
execution time also rises significantly. For instance, with 10 dimensions,
the execution time is only 0.224 seconds, while for 100 dimensions,
it escalates to 32.537 seconds, marking a more than 145-fold increase.
This substantial growth in execution time as the problem's dimensionality
increases highlights the complexity and computational demands of higher-dimensional
problems. The relationship between dimensions and time appears to
be nonlinear, as seen in the sharp rise in time as dimensions grow.
For example, moving from 10 to 20 dimensions results in more than
doubling the time, from 0.224 to 0.577 seconds, while increasing from
90 to 100 dimensions causes a larger jump, from 21.897 to 32.537 seconds.
\begin{center}
\begin{figure}[H]
\begin{centering}
\subfloat[ELP: Function calls\label{fig:ELPcalls}]{\begin{centering}
\includegraphics[scale=0.4]{img3}
\par\end{centering}
}\subfloat[ELP: Times\label{fig:ELPTimes}]{
\begin{centering}
\includegraphics[scale=0.4]{img4}
\par\end{centering}
}\caption{Different variations of the ELP problem\label{fig:ELP}}
\par\end{centering}
\end{figure}
\par\end{center}

In each iteration of the algorithm, a trial point is calculated through
vector operations, similar to the process of optimization using differential
evolution. The main difference between the proposed method compared
to differential evolution lies in the fact that the samples for calculating
the trial point are selected from nearby regions of the initial distribution,
rather than being chosen randomly. However, the performance of the
two methods differs in their ability to find optimal solutions, as
shown in Figure\ref{fig:proposedVSDE}. The extraneous samples of
Figure \ref{fig:proposedVSDE} have been removed.
\begin{center}
\begin{figure}[H]
\centering{}\includegraphics[scale=0.5]{img2}\caption{Comparison of proposed method against DE\label{fig:proposedVSDE}}
\end{figure}
\par\end{center}

\section{Conclusions\label{sec:Conclusions}}

An innovative global optimization method was proposed in this research
paper that utilizes techniques based on established optimization techniques.
More specifically, the new method utilizes the genetic operators from
the Genetic Algorithms as well as the Linear Search method in order
to create candidate solutions for the objective functions. Subsequently,
the candidate solutions are combined to produce new solutions using
techniques based on the Differential Evolution method. In order to
establish the effectiveness of the new method, a series of extensive
experiments were carried out on problems from the relevant literature
and numerical comparisons with established global optimization techniques.
The proposed optimization method demonstrates significantly better
performance compared to the other methods in terms of objective function
calls, with fewer calls indicating better efficiency. This suggests
that the proposed method is more efficient, achieving optimal solutions
with fewer objective function evaluations, which is crucial in problems
where each call is computationally expensive. Statistical tests, including
both the t-test and Kruskal-Wallis, confirm that the differences in
the number of calls between the proposed method and the others are
statistically significant (p-value < 0.05). This indicates that the
proposed method not only uses fewer resources but also achieves reliable
results with improved efficiency. In summary, the proposed method
clearly excels in efficiency compared with other methods, significantly
reducing the number of objective function calls and optimizing computational
cost.

Potential improvements to the algorithm could involve identifying
samples that contribute more effectively toward finding the optimal
solution. Additionally, since this is a novel optimization method,
exploring alternative termination criteria or different initial sample
distributions may lead to enhanced performance.

\vspace{6pt}


\authorcontributions{V.C., I.G.T. and V.S. conceived the idea and methodology and supervised
the technical part regarding the software. V.C. and A.M.G. conducted
the experiments, employing several datasets, and provided the comparative
experiments. I.G.T. performed the statistical analysis. V.S. and all
other authors prepared the manuscript. All authors have read and agreed
to the published version of the manuscript.}

\funding{This research received no external funding.}

\institutionalreview{Not applicable.}

\informedconsent{Not applicable. }

\institutionalreview{Not applicable.}

\acknowledgments{The experiments of this research work were performed at the high
performance computing system established at Knowledge and Intelligent
Computing Laboratory, Department of Informatics and Telecommunications,
University of Ioannina, acquired with the project “Educational Laboratory
equipment of TEI of Epirus” with MIS 5007094 funded by the Operational
Programme “Epirus” 2014--2020, by ERDF and national funds.}

\conflictsofinterest{The authors declare no conflict of interest.}

\appendixtitles{no}

\reftitle{References}
\begin{thebibliography}{999}
\bibitem{go_math1}Intriligator, M. D. (2002). Mathematical optimization
and economic theory. Society for Industrial and Applied Mathematics.

\bibitem{go_math2}Cánovas, M. J., Kruger, A., Phu, H. X., \& Théra,
M. (2020). Marco A. López, a Pioneer of Continuous Optimization in
Spain. Vietnam Journal of Mathematics, 48, 211-219.

\bibitem{go_math3}Mahmoodabadi, M. J., \& Nemati, A. R. (2016). A
novel adaptive genetic algorithm for global optimization of mathematical
test functions and real-world problems. Engineering Science and Technology,
an International Journal, 19(4), 2002-2021.

\bibitem{go_physics1}E. Iuliano, Global optimization of benchmark
aerodynamic cases using physics-based surrogate models, Aerospace
Science and Technology \textbf{67}, pp.273-286, 2017.

\bibitem{go_physics2}Q. Duan, S. Sorooshian, V. Gupta, Effective
and efficient global optimization for conceptual rainfall-runoff models,
Water Resources Research \textbf{28}, pp. 1015-1031 , 1992.

\bibitem{go_physics3}L. Yang, D. Robin, F. Sannibale, C. Steier,
W. Wan, Global optimization of an accelerator lattice using multiobjective
genetic algorithms, Nuclear Instruments and Methods in Physics Research
Section A: Accelerators, Spectrometers, Detectors and Associated Equipment
\textbf{609}, pp. 50-57, 2009.

\bibitem{go_chem1}S. Heiles, R. L. Johnston, Global optimization
of clusters using electronic structure methods, Int. J. Quantum Chem.
\textbf{113}, pp. 2091-- 2109, 2013.

\bibitem{go_chem2}W.H. Shin, J.K. Kim, D.S. Kim, C. Seok, GalaxyDock2:
Protein--ligand docking using beta-complex and global optimization,
J. Comput. Chem. \textbf{34}, pp. 2647-- 2656, 2013.

\bibitem{go_chem3}A. Liwo, J. Lee, D.R. Ripoll, J. Pillardy, H. A.
Scheraga, Protein structure prediction by global optimization of a
potential energy function, Biophysics \textbf{96}, pp. 5482-5485,
1999.

\bibitem{go_med1}Eva K. Lee, Large-Scale Optimization-Based Classification
Models in Medicine and Biology, Annals of Biomedical Engineering \textbf{35},
pp 1095-1109, 2007.

\bibitem{go_med2}Y. Cherruault, Global optimization in biology and
medicine, Mathematical and Computer Modelling \textbf{20}, pp. 119-132,
1994.

\bibitem{medicine}Houssein, E. H., Hosney, M. E., Mohamed, W. M.,
Ali, A. A., \& Younis, E. M. (2023). Fuzzy-based hunger games search
algorithm for global optimization and feature selection using medical
data. Neural Computing and Applications, 35(7), 5251-5275.

\bibitem{go_bio1}Banga, J.R. Optimization in computational systems
biology. BMC Syst. Biol. 2008, 2, 47. 

\bibitem{go_bio2}Beites, T.; Mendes, M.V. Chassis optimization as
a cornerstone for the application of synthetic biology based strategies
in microbial secondary metabolism. Front. Microbiol. 2015, 6, 159095. 

\bibitem{go_agri1}Filip, M.; Zoubek, T.; Bumbalek, R.; Cerny, P.;
Batista, C.E.; Olsan, P.; Bartos, P.; Kriz, P.; Xiao, M.; Dolan, A.;
et al. Advanced computational methods for agriculture machinery movement
optimization with applications in sugarcane production. Agriculture
2020, 10, 434

\bibitem{go_agri2}Zhang, D.; Guo, P. Integrated agriculture water
management optimization model for water saving potential analysis.
Agric. Water Manag. 2016, 170, 5--19.

\bibitem{go_econ1}Intriligator, M.D. Mathematical Optimization and
Economic Theory; SIAM: Philadelphia, PA, USA, 2002

\bibitem{go_econ2}Dixit, A.K. Optimization in Economic Theory; Oxford
University Press: Oxford, MA, USA, 1990.

\bibitem{go_determ1}Ion, I. G., Bontinck, Z., Loukrezis, D., Römer,
U., Lass, O., Ulbrich, S., ... \& De Gersem, H. (2018). Robust shape
optimization of electric devices based on deterministic optimization
methods and finite-element analysis with affine parametrization and
design elements. Electrical Engineering, 100(4), 2635-2647.

\bibitem{go_determ2}Cuevas-Velásquez, V., Sordo-Ward, A., García-Palacios,
J. H., Bianucci, P., \& Garrote, L. (2020). Probabilistic model for
real-time flood operation of a dam based on a deterministic optimization
model. Water, 12(11), 3206.

\bibitem{go_determ3}Pereyra, M., Schniter, P., Chouzenoux, E., Pesquet,
J. C., Tourneret, J. Y., Hero, A. O., \& McLaughlin, S. (2015). A
survey of stochastic simulation and optimization methods in signal
processing. IEEE Journal of Selected Topics in Signal Processing,
10(2), 224-241.

\bibitem{stohastic}Hannah, L. A. (2015). Stochastic optimization.
International Encyclopedia of the Social \& Behavioral Sciences, 2,
473-481.

\bibitem{stohastic1}Kizielewicz, B., \& Sałabun, W. (2020). A new
approach to identifying a multi-criteria decision model based on stochastic
optimization techniques. Symmetry, 12(9), 1551.

\bibitem{stohastic2}Chen, T., Sun, Y., \& Yin, W. (2021). Solving
stochastic compositional optimization is nearly as easy as solving
stochastic optimization. IEEE Transactions on Signal Processing, 69,
4937-4948.

\bibitem{interval1}M.A. Wolfe, Interval methods for global optimization,
Applied Mathematics and Computation \textbf{75}, pp. 179-206, 1996.

\bibitem{interval2}T. Csendes and D. Ratz, Subdivision Direction
Selection in Interval Methods for Global Optimization, SIAM J. Numer.
Anal. \textbf{34}, pp. 922--938, 1997. 

\bibitem{Sergeyev}Y.D. Sergeyev, D.E. Kvasov, M.S. Mukhametzhanov,
On the efficiency of nature-inspired metaheuristics in expensive global
optimization with limited budget. Sci Rep \textbf{8}, 453, 2018.

\bibitem{genetic1}D. Goldberg, Genetic Algorithms in Search, Optimization
and Machine Learning, Addison-Wesley Publishing Company, Reading,
Massachussets, 1989.

\bibitem{genetic2}Z. Michaelewicz, Genetic Algorithms + Data Structures
= Evolution Programs. Springer - Verlag, Berlin, 1996.

\bibitem{genetic3}Charilogis, V., Tsoulos, I. G., \& Stavrou, V.
N. (2023). An Intelligent Technique for Initial Distribution of Genetic
Algorithms. Axioms, 12(10), 980.

\bibitem{diffe1}R. Storn, K. Price, Differential Evolution - A Simple
and Efficient Heuristic for Global Optimization over Continuous Spaces,
Journal of Global Optimization \textbf{11}, pp. 341-359, 1997.

\bibitem{diffe2}J. Liu, J. Lampinen, A Fuzzy Adaptive Differential
Evolution Algorithm. Soft Comput \textbf{9}, pp.448--462, 2005.

\bibitem{pso_major}J. Kennedy and R. Eberhart, \textquotedbl Particle
swarm optimization,\textquotedbl{} Proceedings of ICNN'95 - International
Conference on Neural Networks, 1995, pp. 1942-1948 vol.4, doi: 10.1109/ICNN.1995.488968.

\bibitem{pso1}Riccardo Poli, James Kennedy kennedy, Tim Blackwell,
Particle swarm optimization An Overview, Swarm Intelligence \textbf{1},
pp 33-57, 2007. 

\bibitem{pso2}Ioan Cristian Trelea, The particle swarm optimization
algorithm: convergence analysis and parameter selection, Information
Processing Letters \textbf{85}, pp. 317-325, 2003.

\bibitem{aco1}M. Dorigo, M. Birattari and T. Stutzle, Ant colony
optimization, IEEE Computational Intelligence Magazine \textbf{1},
pp. 28-39, 2006.

\bibitem{aco2}K. Socha, M. Dorigo, Ant colony optimization for continuous
domains, European Journal of Operational Research 185, pp. 1155-1173,
2008.

\bibitem{fish}M. Neshat, G. Sepidnam, M. Sargolzaei, et al, Artificial
fish swarm algorithm: a survey of the state-of-the-art, hybridization,
combinatorial and indicative applications, Artif Intell Rev \textbf{42},
pp. 965--997, 2014.

\bibitem{dolphin}Tq. Wu, M. Yao, Jh. Yang, Dolphin swarm algorithm,
Frontiers Inf Technol Electronic Eng \textbf{17}, pp. 717--729, 2016. 

\bibitem{WOA}Mirjalili, S., Lewis, A.: The whale optimization algorithm.
Adv. Eng. Softw. 95, 51--67 (2016)

\bibitem{WOA1}Nasiri, J., \& Khiyabani, F. M. (2018). A whale optimization
algorithm (WOA) approach for clustering. Cogent Mathematics \& Statistics,
5(1), 1483565.

\bibitem{WOA2}Gharehchopogh, F. S., \& Gholizadeh, H. (2019). A comprehensive
survey: Whale Optimization Algorithm and its applications. Swarm and
Evolutionary Computation, 48, 1-24.

\bibitem{gpu1}Y. Zhou and Y. Tan, \textquotedbl GPU-based parallel
particle swarm optimization,\textquotedbl{} 2009 IEEE Congress on
Evolutionary Computation, pp. 1493-1500, 2009.

\bibitem{gpu2}L. Dawson and I. Stewart, \textquotedbl Improving
Ant Colony Optimization performance on the GPU using CUDA,\textquotedbl{}
2013 IEEE Congress on Evolutionary Computation, 2013, pp. 1901-1908,
doi: 10.1109/CEC.2013.6557791.

\bibitem{gpu3}Barkalov, K., Gergel, V. Parallel global optimization
on GPU. J Glob Optim 66, 3--20 (2016). 

\bibitem{holland}Holland, J.H. Genetic algorithms. Sci. Am. 1992,
267, 66--73.

\bibitem{ga_problem1}Y.H. Santana, R.M. Alonso, G.G. Nieto, L. Martens,
W. Joseph, D. Plets, Indoor genetic algorithm-based 5G network planning
using a machine learning model for path loss estimation, Appl. Sci.
\textbf{12}, 3923. 2022.

\bibitem{ga_problem2}X. Liu, D. Jiang, B. Tao, G. Jiang, Y. Sun,
J. Kong, B. Chen, Genetic algorithm-based trajectory optimization
for digital twin robots, Front. Bioeng. Biotechnol \textbf{9}, 793782,
2022.

\bibitem{ga_problem3}K. Nonoyama, Z.Liu, T. Fujiwara, M.M. Alam,
T. Nishi, Energy-efficient robot configuration and motion planning
using genetic algorithm and particle swarm optimization, Energies
\textbf{15}, 2074, 2022.

\bibitem{ga_problem4}K. Liu, B. Deng, Q. Shen, J. Yang, Y. Li, Optimization
based on genetic algorithms on energy conservation potential of a
high speed SI engine fueled with butanol--gasoline blends, Energy
Rep. \textbf{8}, pp. 69--80, 2022.

\bibitem{ga_problem5}G. Zhou, S. Zhu, S. Luo, Location optimization
of electric vehicle charging stations: Based on cost model and genetic
algorithm, Energy \textbf{247}, 123437, 2022.

\bibitem{ga_nn1}J. Arifovic, R. Gençay, Using genetic algorithms
to select architecture of a feedforward artificial neural network,
Physica A: Statistical Mechanics and its Applications \textbf{289},
pp. 574-594, 2001. 

\bibitem{ga_nn2}F. H. F. Leung, H. K. Lam, S. H. Ling, P. K. S. Tam,
Tuning of the structure and parameters of a neural network using an
improved genetic algorithm, IEEE Transactions on Neural Networks \textbf{14},
pp. 79-88, 2003.

\bibitem{de_symmetry1}Y.H. Li, J.Q. Wang, X.J. Wang, Y.L. Zhao, X.H.
Lu, D.L. Liu, Community Detection Based on Differential Evolution
Using Social Spider Optimization, Symmetry \textbf{9}, 2017.

\bibitem{de_symmetry2}W. Yang, E.M. Dilanga Siriwardane, R. Dong,
Y. Li, J. Hu, Crystal structure prediction of materials with high
symmetry using differential evolution, J. Phys.: Condens. Matter \textbf{33}
455902, 2021.

\bibitem{de_problem1}Maulik, U.; Saha, I. Automatic Fuzzy Clustering
Using Modified Differential Evolution for Image Classification. IEEE
Trans. Geosci. Remote Sens. 2010, 48, 3503--3510.

\bibitem{de_problem2}Zhang, Y.; Zhang, H.; Cai, J.; Yang, B. A Weighted
Voting Classifier Based on Differential Evolution. Abstr. Appl. Anal.
2014, 2014, 376950.

\bibitem{de_problem3}Hancer, E. Differential evolution for feature
selection: A fuzzy wrapper--filter approach. Soft Comput. 2019, 23,
5233--5248.

\bibitem{de_problem4}Vivekanandan, T.; Iyengar, N.C.S.N. Optimal
feature selection using a modified differential evolution algorithm
and its effectiveness for prediction of heart disease. Comput. Biol.
Med. 2017, 90, 125--136.

\bibitem{de_deep1}Deng, W.; Liu, H.; Xu, J.; Zhao, H.; Song, Y. An
Improved Quantum-Inspired Differential Evolution Algorithm for Deep
Belief Network. IEEE Trans. Instrum. Meas. 2020, 69, 7319--7327.

\bibitem{de_deep2}Wu, T.; Li, X.; Zhou, D.; Li, N.; Shi, J. Differential
Evolution Based Layer-Wise Weight Pruning for Compressing Deep Neural
Networks. Sensors 2021, 21, 880.

\bibitem{go_local2}H. Badem, A. Basturk, A. Caliskan, M.E. Yuksel,
A new hybrid optimization method combining artificial bee colony and
limited-memory BFGS algorithms for efficient numerical optimization,
Applied Soft Computing \textbf{70}, pp. 826-844, 2018.

\bibitem{go_local3}A.A. Nagra, F. Han, Q.H. Ling, An improved hybrid
self-inertia weight adaptive particle swarm optimization algorithm
with local search, Engineering Optimization \textbf{51}, pp. 1115-1132,
2018.

\bibitem{hybrid1}Li, S., Tan, M., Tsang, I. W., \& Kwok, J. T. Y.
(2011). A hybrid PSO-BFGS strategy for global optimization of multimodal
functions. IEEE Transactions on Systems, Man, and Cybernetics, Part
B (Cybernetics), 41(4), 1003-1014.

\bibitem{hybrid2}Andalib Sahnehsaraei, M., Mahmoodabadi, M. J., Taherkhorsandi,
M., Castillo-Villar, K. K., \& Mortazavi Yazdi, S. M. (2015). A hybrid
global optimization algorithm: particle swarm optimization in association
with a genetic algorithm. Complex System Modelling and Control Through
Intelligent Soft Computations, 45-86.

\bibitem{armijo}Armijo, L. (1966). Minimization of functions having
Lipschitz continuous first partial derivatives. Pacific Journal of
mathematics, 16(1), 1-3.

\bibitem[(2002)]{bfgs}, Convergence properties of the BFGS algoritm.
SIAM Journal on Optimization \textbf{13}, pp. 693-701, 2002.

\bibitem{charilogis}Charilogis, V.; Tsoulos, I.G. Toward an Ideal
Particle Swarm Optimizer for Multidimensional Functions. Information
2022, 13, 217. 

\bibitem{Gaviano} M. Gaviano, D.E. Ksasov, D. Lera, Y.D. Sergeyev,
Software for generation of classes of test functions with known local
and global minima for global optimization, ACM Trans. Math. Softw.
\textbf{29}, pp. 469-480, 2003.

\bibitem{Lennard} J.E. Lennard-Jones, On the Determination of Molecular
Fields, Proc. R. Soc. Lond. A \textbf{ 106}, pp. 463--477, 1924.

\bibitem{Zabinsky} Z.B. Zabinsky, D.L. Graesser, M.E. Tuttle, G.I.
Kim, Global optimization of composite laminates using improving hit
and run, In: Recent advances in global optimization, pp. 343-368,
1992.

\bibitem{Ali1}M. Montaz Ali, Charoenchai Khompatraporn, Zelda B.
Zabinsky, A Numerical Evaluation of Several Stochastic Algorithms
on Selected Continuous Global Optimization Test Problems, Journal
of Global Optimization \textbf{31}, pp 635-672, 2005. 

\bibitem{Floudas1}C.A. Floudas, P.M. Pardalos, C. Adjiman, W. Esposoto,
Z. G$\ddot{\mbox{u}}$m$\ddot{\mbox{u}}$s, S. Harding, J. Klepeis,
C. Meyer, C. Schweiger, Handbook of Test Problems in Local and Global
Optimization, Kluwer Academic Publishers, Dordrecht, 1999.

\bibitem{ipso}V. Charilogis, I.G. Tsoulos, Toward an Ideal Particle
Swarm Optimizer for Multidimensional Functions , Information \textbf{13},
217, 2022.

\bibitem{powell}Powell, M.J.D. A Tolerant Algorithm for Linearly
Constrained Optimization Calculations. Math. Program. 1989, 45, 547--566.

\end{thebibliography}

\end{document}
